---
title: "Machine Learning"
author: "Jenny Shyu"
date: today
---




## 1a. K-Means




In this section, I applied the K-Means clustering algorithm to the Palmer Penguins dataset, focusing on two continuous features: bill_length_mm and flipper_length_mm. I implemented the algorithm from scratch in Python and evaluated it across multiple values of k to investigate clustering structure and performance.

```{python}
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load penguin data and select relevant features
penguins = pd.read_csv("palmer_penguins.csv")
penguins = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()

# Standardize the data
scaler = StandardScaler()
X = scaler.fit_transform(penguins)
```

```{python}
import numpy as np

def initialize_centroids(X, k):
    np.random.seed(42)
    indices = np.random.choice(X.shape[0], size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans_custom(X, k, max_iters=100):
    centroids = initialize_centroids(X, k)
    for _ in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return labels, centroids
```

```{python}
import matplotlib.pyplot as plt

k_range = range(2, 8)
fig, axs = plt.subplots(2, 3, figsize=(18, 10))
axs = axs.flatten()

for i, k in enumerate(k_range):
    labels, centroids = kmeans_custom(X, k)
    axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30)
    axs[i].scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)
    axs[i].set_title(f'Custom K-Means: k={k}')

plt.tight_layout()
plt.show()
```

#### WCSS and Silhouette Scores Using Built-in KMeans
```{python}
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

wcss = []
sil_scores = []

for k in k_range:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X)
    wcss.append(model.inertia_)
    sil_scores.append(silhouette_score(X, model.labels_))
```

```{python}
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(list(k_range), wcss, marker='o', color='orange')
plt.title("WCSS vs Number of Clusters")
plt.xlabel("k")
plt.ylabel("WCSS")

plt.subplot(1, 2, 2)
plt.plot(list(k_range), sil_scores, marker='o', color='orange')
plt.title("Silhouette Score vs Number of Clusters")
plt.xlabel("k")
plt.ylabel("Silhouette Score")

plt.tight_layout()
plt.show()
```


#### Implementation Details
The custom K-Means implementation followed the standard algorithmic steps: initializing cluster centroids randomly, assigning each data point to the nearest centroid, recalculating centroids as the mean of all points in a cluster, and repeating the process until the centroids stabilized. This process was performed for several values of k, ranging from 2 to 7, to examine how clustering structure varied with the number of clusters. Throughout the implementation, the algorithm successfully converged in a reasonable number of iterations and demonstrated expected behavior.

#### Evaluation Metrics and Comparison
To evaluate the clustering quality and help determine the optimal number of clusters, I calculated two metrics: Within-Cluster Sum of Squares (WCSS) and Silhouette Score. WCSS measures cluster compactness, with lower values indicating tighter grouping. Silhouette Score captures how well-separated clusters are, with higher values reflecting clearer separation. These metrics were computed using the results from the built-in KMeans function to ensure consistency and enable direct comparison with the custom implementation.

The clustering results from the custom algorithm were visually and numerically comparable to those from sklearn.KMeans. The shapes and positions of clusters, as well as the centroids, aligned closely between the two implementations. This consistency validated the accuracy of the custom method. The primary advantage of the built-in method lies in its use of the k-means++ initialization technique, which improves stability and performance, but the final clustering patterns were essentially the same in both cases.

#### Choosing the Optimal Number of Clusters
The WCSS curve showed a steep decline from k = 2 to k = 3, followed by a more gradual decrease, suggesting an “elbow” at k = 3 or k = 4. This elbow point indicates a diminishing return on increasing the number of clusters beyond three or four. In contrast, the Silhouette Score was highest at k = 2, indicating that two clusters provided the clearest separation among groups. However, as more clusters were added, the scores declined slightly, suggesting increasing overlap or less distinct clustering.

Balancing these insights, the most reasonable choice for the number of clusters is k = 3. While k = 2 provides the highest Silhouette Score, it may oversimplify the data and miss meaningful subgroup structures. Meanwhile, k = 3 captures more nuance in the data while still maintaining relatively compact and well-separated clusters, as evidenced by both visual plots and metric values.

#### Conclusion
This assignment demonstrated the full implementation and evaluation of K-Means clustering, reinforcing the importance of using both visualizations and metrics to determine the best number of clusters. The custom implementation aligned closely with the built-in version, confirming its correctness. Ultimately, k = 3 emerged as the best compromise between compactness and interpretability, providing valuable insights into the structure of the penguin dataset.





## 2a. K Nearest Neighbors



```{python}
import numpy as np
import pandas as pd

np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
X_train = np.column_stack((x1, x2))
boundary = np.sin(4 * x1) + x1
y_train = np.where(x2 > boundary, 1, 0)
y_train = pd.Categorical(y_train)

```


#### Training Data with the Wiggly Boundary
```{python}
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(x1, x2, c=y_train.codes, cmap='coolwarm', edgecolor='k')
plt.plot(np.sort(x1), np.sin(4 * np.sort(x1)) + np.sort(x1), linestyle='--', color='black', label='Boundary')
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Training Data with Wiggly Boundary")
plt.legend()
plt.grid(True)
plt.show()
```

#### Test Dataset with a Different Seed
```{python}
np.random.seed(99)
x1_test = np.random.uniform(-3, 3, n)
x2_test = np.random.uniform(-3, 3, n)
X_test = np.column_stack((x1_test, x2_test))
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = np.where(x2_test > boundary_test, 1, 0)
y_test = pd.Categorical(y_test)
```

#### KNN Evaluate Accuracy from k = 1 to 30
```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

accuracies = []
accuracy_sklearn = []

for k in range(1, 31):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train.codes)
    preds = knn.predict(X_test)
    acc = accuracy_score(y_test.codes, preds)
    accuracies.append(acc)

    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    y_pred_builtin = model.predict(X_test)
    acc_builtin = accuracy_score(y_test, y_pred_builtin)
    accuracy_sklearn.append(acc_builtin)
```

#### Accuracy Results

```{python}
plt.figure(figsize=(10, 5))
plt.plot(range(1, 31), accuracies, label='Custom KNN', marker='o')
plt.plot(range(1, 31), accuracy_sklearn, label='Sklearn KNN', marker='x')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy on Test Set')
plt.title('KNN Accuracy vs. k')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

To explore the K-Nearest Neighbors (KNN) classification algorithm, I generated a synthetic dataset with two features, x1 and x2, and a binary outcome variable y. The target variable was determined by whether x2 was above or below a non-linear boundary defined by the function y = sin(4x1) + x1. This setup creates a wiggly decision boundary, ideal for testing the flexibility and local sensitivity of KNN classifiers. I used NumPy to simulate 100 data points for training, each with x1 and x2 values sampled uniformly from the range [-3, 3].

The data was visualized with x1 on the horizontal axis and x2 on the vertical axis, where points were colored by class label. I overlaid the true boundary using the equation sin(4x1) + x1, which clearly separated the red and blue points. This plot served as a useful diagnostic tool to visually assess the complexity of the boundary KNN must learn.

To evaluate classification performance, I generated a test dataset of 100 new points using the same logic but with a different random seed to ensure separation between training and evaluation. The model was trained using scikit-learn’s KNeighborsClassifier, and its accuracy was tested on the new data across values of k from 1 to 30. For each k, I recorded the classification accuracy and visualized the results in a line plot.

The plot of accuracy versus k shows that performance is highest at small values of k, peaking at k = 1 and k = 2 with an accuracy of approximately 92%. As k increases, accuracy gradually declines, with slight fluctuations, and stabilizes around 88% for k values beyond 20. This trend is expected; lower k values allow the model to capture fine-grained, local decision boundaries but may overfit, while higher k values generalize more but can miss non-linear patterns like those in this dataset.

Based on the plot, the optimal value of k is likely 1 or 2, as these values yield the highest classification accuracy on the test set. However, for more robust generalization, a slightly higher value of k, such as 5, might be preferred in real-world settings to mitigate the risk of overfitting. This exercise demonstrated how KNN can adapt to complex decision surfaces and how model performance is closely tied to the choice of k.









