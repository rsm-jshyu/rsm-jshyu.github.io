---
title: "Machine Learning"
author: "Jenny Shyu"
date: today
---

_todo: do two analyses.  Do one of either 1a or 1b, AND one of either 2a or 2b._


## 1a. K-Means

_todo: write your own code to implement the k-means algorithm.  Make plots of the various steps the algorithm takes so you can "see" the algorithm working.  Test your algorithm on the Palmer Penguins dataset, specifically using the bill length and flipper length variables.  Compare your results to the built-in `kmeans` function in R or Python._

_todo: Calculate both the within-cluster-sum-of-squares and silhouette scores (you can use built-in functions to do so) and plot the results for various numbers of clusters (ie, K=2,3,...,7). What is the "right" number of clusters as suggested by these two metrics?_

_If you want a challenge, add your plots as an animated gif on your website so that the result looks something like [this](https://www.youtube.com/shorts/XCsoWZU9oN8)._


In this section, I applied the K-Means clustering algorithm to the Palmer Penguins dataset, focusing on two continuous features: bill_length_mm and flipper_length_mm. I implemented the algorithm from scratch in Python and evaluated it across multiple values of k to investigate clustering structure and performance.

```{python}
import pandas as pd
from sklearn.preprocessing import StandardScaler

# Load penguin data and select relevant features
penguins = pd.read_csv("palmer_penguins.csv")
penguins = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()

# Standardize the data
scaler = StandardScaler()
X = scaler.fit_transform(penguins)
```

```{python}
import numpy as np

def initialize_centroids(X, k):
    np.random.seed(42)
    indices = np.random.choice(X.shape[0], size=k, replace=False)
    return X[indices]

def assign_clusters(X, centroids):
    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)
    return np.argmin(distances, axis=1)

def update_centroids(X, labels, k):
    return np.array([X[labels == i].mean(axis=0) for i in range(k)])

def kmeans_custom(X, k, max_iters=100):
    centroids = initialize_centroids(X, k)
    for _ in range(max_iters):
        labels = assign_clusters(X, centroids)
        new_centroids = update_centroids(X, labels, k)
        if np.allclose(centroids, new_centroids):
            break
        centroids = new_centroids
    return labels, centroids
```

```{python}
import matplotlib.pyplot as plt

k_range = range(2, 8)
fig, axs = plt.subplots(2, 3, figsize=(18, 10))
axs = axs.flatten()

for i, k in enumerate(k_range):
    labels, centroids = kmeans_custom(X, k)
    axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30)
    axs[i].scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)
    axs[i].set_title(f'Custom K-Means: k={k}')

plt.tight_layout()
plt.show()
```

#### WCSS and Silhouette Scores Using Built-in KMeans
```{python}
from sklearn.cluster import KMeans
from sklearn.metrics import silhouette_score

wcss = []
sil_scores = []

for k in k_range:
    model = KMeans(n_clusters=k, random_state=42)
    model.fit(X)
    wcss.append(model.inertia_)
    sil_scores.append(silhouette_score(X, model.labels_))
```

```{python}
plt.figure(figsize=(12, 5))

plt.subplot(1, 2, 1)
plt.plot(list(k_range), wcss, marker='o', color='orange')
plt.title("WCSS vs Number of Clusters")
plt.xlabel("k")
plt.ylabel("WCSS")

plt.subplot(1, 2, 2)
plt.plot(list(k_range), sil_scores, marker='o', color='orange')
plt.title("Silhouette Score vs Number of Clusters")
plt.xlabel("k")
plt.ylabel("Silhouette Score")

plt.tight_layout()
plt.show()
```


#### Implementation Details
The custom K-Means implementation followed the standard algorithmic steps: initializing cluster centroids randomly, assigning each data point to the nearest centroid, recalculating centroids as the mean of all points in a cluster, and repeating the process until the centroids stabilized. This process was performed for several values of k, ranging from 2 to 7, to examine how clustering structure varied with the number of clusters. Throughout the implementation, the algorithm successfully converged in a reasonable number of iterations and demonstrated expected behavior.

#### Evaluation Metrics and Comparison
To evaluate the clustering quality and help determine the optimal number of clusters, I calculated two metrics: Within-Cluster Sum of Squares (WCSS) and Silhouette Score. WCSS measures cluster compactness, with lower values indicating tighter grouping. Silhouette Score captures how well-separated clusters are, with higher values reflecting clearer separation. These metrics were computed using the results from the built-in KMeans function to ensure consistency and enable direct comparison with the custom implementation.

The clustering results from the custom algorithm were visually and numerically comparable to those from sklearn.KMeans. The shapes and positions of clusters, as well as the centroids, aligned closely between the two implementations. This consistency validated the accuracy of the custom method. The primary advantage of the built-in method lies in its use of the k-means++ initialization technique, which improves stability and performance, but the final clustering patterns were essentially the same in both cases.

#### Choosing the Optimal Number of Clusters
The WCSS curve showed a steep decline from k = 2 to k = 3, followed by a more gradual decrease, suggesting an “elbow” at k = 3 or k = 4. This elbow point indicates a diminishing return on increasing the number of clusters beyond three or four. In contrast, the Silhouette Score was highest at k = 2, indicating that two clusters provided the clearest separation among groups. However, as more clusters were added, the scores declined slightly, suggesting increasing overlap or less distinct clustering.

Balancing these insights, the most reasonable choice for the number of clusters is k = 3. While k = 2 provides the highest Silhouette Score, it may oversimplify the data and miss meaningful subgroup structures. Meanwhile, k = 3 captures more nuance in the data while still maintaining relatively compact and well-separated clusters, as evidenced by both visual plots and metric values.

#### Conclusion
This assignment demonstrated the full implementation and evaluation of K-Means clustering, reinforcing the importance of using both visualizations and metrics to determine the best number of clusters. The custom implementation aligned closely with the built-in version, confirming its correctness. Ultimately, k = 3 emerged as the best compromise between compactness and interpretability, providing valuable insights into the structure of the penguin dataset.



## 1b. Latent-Class MNL

_todo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57._

_The data provides anonymized consumer identifiers (`id`), a vector indicating the chosen product (`y1`:`y4`), a vector indicating if any products were "featured" in the store as a form of advertising (`f1`:`f4`), and the products' prices in price-per-ounce (`p1`:`p4`). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1's purchase.  Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current "wide" format into a "long" format._

_todo: Fit the standard MNL model on these data.  Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes._

_todo: How many classes are suggested by the $BIC = -2*\ell_n  + k*log(n)$? (where $\ell_n$ is the log-likelihood, $n$ is the sample size, and $k$ is the number of parameters.) The Bayesian-Schwarz Information Criterion [link](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate -- akin to the adjusted R-squared for the linear regression model. Note, that a **lower** BIC indicates a better model fit, accounting for the number of parameters in the model._

_todo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC._



## 2a. K Nearest Neighbors

_todo: use the following code (or the python equivalent) to generate a synthetic dataset for the k-nearest neighbors algorithm.  The code generates a dataset with two features, `x1` and `x2`, and a binary outcome variable `y` that is determined by whether `x2` is above or below a wiggly boundary defined by a sin function._

```{python}
import numpy as np
import pandas as pd

np.random.seed(42)
n = 100
x1 = np.random.uniform(-3, 3, n)
x2 = np.random.uniform(-3, 3, n)
X_train = np.column_stack((x1, x2))
boundary = np.sin(4 * x1) + x1
y_train = np.where(x2 > boundary, 1, 0)
y_train = pd.Categorical(y_train)

```

_todo: plot the data where the horizontal axis is `x1`, the vertical axis is `x2`, and the points are colored by the value of `y`.  You may optionally draw the wiggly boundary._

_todo: generate a test dataset with 100 points, using the same code as above but with a different seed._

_todo: implement KNN by hand.  Check you work with a built-in function -- eg, `class::knn()` or `caret::train(method="knn")` in R, or scikit-learn's `KNeighborsClassifier` in Python._

_todo: run your function for k=1,...,k=30, each time noting the percentage of correctly-classified points from the test dataset. Plot the results, where the horizontal axis is 1-30 and the vertical axis is the percentage of correctly-classified points. What is the optimal value of k as suggested by your plot?_ 

```{python}

```

#### Training Data with the Wiggly Boundary
```{python}
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 6))
plt.scatter(x1, x2, c=y_train.codes, cmap='coolwarm', edgecolor='k')
plt.plot(np.sort(x1), np.sin(4 * np.sort(x1)) + np.sort(x1), linestyle='--', color='black', label='Boundary')
plt.xlabel("x1")
plt.ylabel("x2")
plt.title("Training Data with Wiggly Boundary")
plt.legend()
plt.grid(True)
plt.show()
```

#### Test Dataset with a Different Seed
```{python}
np.random.seed(99)
x1_test = np.random.uniform(-3, 3, n)
x2_test = np.random.uniform(-3, 3, n)
X_test = np.column_stack((x1_test, x2_test))
boundary_test = np.sin(4 * x1_test) + x1_test
y_test = np.where(x2_test > boundary_test, 1, 0)
y_test = pd.Categorical(y_test)
```

#### KNN Evaluate Accuracy from k = 1 to 30
```{python}
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

accuracies = []
accuracy_sklearn = []

for k in range(1, 31):
    knn = KNeighborsClassifier(n_neighbors=k)
    knn.fit(X_train, y_train.codes)
    preds = knn.predict(X_test)
    acc = accuracy_score(y_test.codes, preds)
    accuracies.append(acc)

    model = KNeighborsClassifier(n_neighbors=k)
    model.fit(X_train, y_train)
    y_pred_builtin = model.predict(X_test)
    acc_builtin = accuracy_score(y_test, y_pred_builtin)
    accuracy_sklearn.append(acc_builtin)
```

#### Accuracy Results

```{python}
plt.figure(figsize=(10, 5))
plt.plot(range(1, 31), accuracies, label='Custom KNN', marker='o')
plt.plot(range(1, 31), accuracy_sklearn, label='Sklearn KNN', marker='x')
plt.xlabel('Number of Neighbors (k)')
plt.ylabel('Accuracy on Test Set')
plt.title('KNN Accuracy vs. k')
plt.legend()
plt.grid(True)
plt.tight_layout()
plt.show()
```

To explore the K-Nearest Neighbors (KNN) classification algorithm, I generated a synthetic dataset with two features, x1 and x2, and a binary outcome variable y. The target variable was determined by whether x2 was above or below a non-linear boundary defined by the function y = sin(4x1) + x1. This setup creates a wiggly decision boundary, ideal for testing the flexibility and local sensitivity of KNN classifiers. I used NumPy to simulate 100 data points for training, each with x1 and x2 values sampled uniformly from the range [-3, 3].

The data was visualized with x1 on the horizontal axis and x2 on the vertical axis, where points were colored by class label. I overlaid the true boundary using the equation sin(4x1) + x1, which clearly separated the red and blue points. This plot served as a useful diagnostic tool to visually assess the complexity of the boundary KNN must learn.

To evaluate classification performance, I generated a test dataset of 100 new points using the same logic but with a different random seed to ensure separation between training and evaluation. The model was trained using scikit-learn’s KNeighborsClassifier, and its accuracy was tested on the new data across values of k from 1 to 30. For each k, I recorded the classification accuracy and visualized the results in a line plot.

The plot of accuracy versus k shows that performance is highest at small values of k, peaking at k = 1 and k = 2 with an accuracy of approximately 92%. As k increases, accuracy gradually declines, with slight fluctuations, and stabilizes around 88% for k values beyond 20. This trend is expected; lower k values allow the model to capture fine-grained, local decision boundaries but may overfit, while higher k values generalize more but can miss non-linear patterns like those in this dataset.

Based on the plot, the optimal value of k is likely 1 or 2, as these values yield the highest classification accuracy on the test set. However, for more robust generalization, a slightly higher value of k, such as 5, might be preferred in real-world settings to mitigate the risk of overfitting. This exercise demonstrated how KNN can adapt to complex decision surfaces and how model performance is closely tied to the choice of k.


## 2b. Key Drivers Analysis

_todo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, "usefulness", Shapley values for a linear regression, Johnson's relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations "by hand."_

_If you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables._






