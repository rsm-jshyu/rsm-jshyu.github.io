[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Hi there! I’m Jenny Shyu, I’m passionate about leveraging quantitative methods, data, and technology to generate meaningful insights and tackle real-world business problems.\n\n\n\n\n\n\nM.S. Business Analytics | UCSD 2024-Present\nB.S. Mathematics & Economics, Computational Social Science Minor | UCSD 2021-2024\n\n\n\n\n\n\nExperian | Capstone Project | March 2025 - Present\nUCSD Rady | Graduate Teaching Assistant | MGT 172 Business Project Management | October 2024 - Present\nLPL Financial | Technology Intern | June 2024 - August 2024\nLumnus Consulting | VP Data Analytics | November 2022 - June 2024"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Hi there! I’m Jenny Shyu, a Master’s student in Business Analytics at UC San Diego’s Rady School of Management with a background in Math-Econ.\n\n\nI’m passionate about leveraging quantitative methods, data, and technology to generate meaningful insights and tackle real-world business problems.\n\n\n\nMost recently, I interned at LPL Financial, where I worked with financial data and dashboards. I also served as the VP of Data Analytics in a student consulting organization and currently work as a Graduate Teaching Assistant for a Business Project Management course—helping students apply frameworks like prioritization matrices in real-world project planning.\nIn the past, I also trained and competed as a figure skater representing Chinese Taipei."
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Capstone Project | Experian\nGraduate Teaching Assistant, MGT 172 Business Project Management | UCSD Rady"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Jenny Shyu",
    "section": "",
    "text": "GBK Collective | 2022 - Present\nBain & Co | 2020 - 2021\nCornerstone Research | 2006 - 2014"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jenny Shyu",
    "section": "",
    "text": "M.S. Business Analytics | UCSD 2024-Present\nB.S. Mathematics & Economics, Computational Social Science Minor | UCSD 2021-2024"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Experian | Capstone Project | March 2025 - Present\nUCSD Rady | Graduate Teaching Assistant | MGT 172 Business Project Management | October 2024 - Present\nLPL Financial | Technology Intern | June 2024 - August 2024\nLumnus Consulting | VP Data Analytics | November 2022 - June 2024"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nJenny Shyu\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMultinomial Logit Model\n\n\n\n\n\n\nJenny Shyu\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nMachine Learning\n\n\n\n\n\n\nJenny Shyu\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJenny Shyu\n\n\nJun 11, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndata.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nAs an ad hoc test of the randomization mechanism, I compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another at the 95% confidence level. If randomization was properly executed, we should expect no statistically significant differences in pre-treatment characteristics between the groups.\nI begin by testing the variable mrm2, which captures the number of months since the last donation. This variable is useful for checking balance because it is unrelated to the treatment assignment and reflects donor history.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\nvars_to_test = [\"mrm2\", \"amount\", \"years\", \"freq\"]\nresults = []\n\nfor var in vars_to_test:\n    subset = data[[\"treatment\", var]].dropna()\n    control = subset[subset[\"treatment\"] == 0][var]\n    treatment = subset[subset[\"treatment\"] == 1][var]\n    \n    # T-test\n    t_stat, p_val = stats.ttest_ind(treatment, control, equal_var=False)\n    \n    # Linear regression\n    regression = smf.ols(f\"{var} ~ treatment\", data=subset).fit()\n    coef = regression.params[\"treatment\"]\n    reg_p = regression.pvalues[\"treatment\"]\n    \n    # Difference in means\n    diff = treatment.mean() - control.mean()\n    \n    results.append({\n        \"Variable\": var,\n        \"Diff (Treat - Control)\": round(diff, 5),\n        \"T-test p-value\": round(p_val, 5),\n        \"Regression Coef\": round(coef, 5),\n        \"Regression p-value\": round(reg_p, 5)\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\nVariable\nDiff (Treat - Control)\nT-test p-value\nRegression Coef\nRegression p-value\n\n\n\n\n0\nmrm2\n0.01369\n0.90485\n0.01369\n0.90489\n\n\n1\namount\n0.15361\n0.05509\n0.15361\n0.06282\n\n\n2\nyears\n-0.05755\n0.27532\n-0.05755\n0.27002\n\n\n3\nfreq\n-0.01198\n0.91174\n-0.01198\n0.91170\n\n\n\n\n\n\n\nThe table of results above shows no statistically significant differences at the 5% level for any variable (p-values &gt; 0.05), though amount is marginally close (p ≈ 0.06 in the regression). This is consistent with proper random assignment.\nThese checks are similar to what Karlan and List report in Table 1 of the original paper, which reassures readers that the treatment effect estimates later in the paper can be interpreted as causal. If pre-treatment covariates are balanced, then observed differences in outcomes are more likely attributable to the randomized treatment itself.\n\nInterpretation\nThese results mirror those presented in Table 1 of Karlan and List (2007), which shows no significant differences between the groups in prior donation behavior and demographic characteristics. Table 1 serves to reassure the reader that any observed treatment effects later in the analysis can be confidently attributed to the randomized intervention rather than pre-existing differences between groups."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nDonation Rate by Group\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"group\"] = donation_rates[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Create barplot\nplt.figure(figsize=(6, 4))\nax = sns.barplot(data=donation_rates, x=\"group\", y=\"gave\")\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"\")\nplt.title(\"Donation Rate by Group\")\nplt.ylim(0, 0.03)\nplt.grid(axis='y')\n\n# Add percentage labels on top\nfor i, val in enumerate(donation_rates[\"gave\"]):\n    ax.text(i, val + 0.0005, f\"{val:.3%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar plot shows that the treatment group—who received matching grant letters—had a higher donation rate than the control group.\n\n\nT-Test and Linear Regression\n\ntreat_gave = data[data['treatment'] == 1]['gave']\ncontrol_gave = data[data['treatment'] == 0]['gave']\nt_stat, t_pval = stats.ttest_ind(treat_gave, control_gave, equal_var=False)\n\n# Format output\npd.DataFrame([{\n    \"T-test Statistic\": round(t_stat, 3),\n    \"T-test p-value\": round(t_pval, 5)\n}])\n\n\n\n\n\n\n\n\nT-test Statistic\nT-test p-value\n\n\n\n\n0\n3.209\n0.00133\n\n\n\n\n\n\n\n\ngave_regression = smf.ols(\"gave ~ treatment\", data=data).fit()\ncoef = gave_regression.params[\"treatment\"]\nstd_err = gave_regression.bse[\"treatment\"]\np_val = gave_regression.pvalues[\"treatment\"]\nconf_int = gave_regression.conf_int().loc[\"treatment\"]\n\n# Format output\npd.DataFrame([{\n    \"Treatment Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.00418\n0.00135\n0.00193\n0.00154\n0.00682\n\n\n\n\n\n\n\nThe difference in donation rates is statistically significant at the 1% level.\nThe treatment group is more likely to donate, increasing the probability of giving by about 0.42 percentage points.\nThis replicates the result from Table 2A Panel A in Karlan & List (2007), showing that a match offer significantly boosts participation.\nOLS regression shows a statistically significant positive coefficient (≈ 0.0042) on the treatment variable. This confirms the t-test: assignment to the treatment group increased the likelihood of making a donation.\nThis suggests that even a small behavioral nudge like mentioning a matching donation makes people more likely to contribute to charity. People respond to the perception of increased impact.\n\n# Probit model\nimport statsmodels.api as sm\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\ncoef = probit_model.params[\"treatment\"]\nstd_err = probit_model.bse[\"treatment\"]\np_val = probit_model.pvalues[\"treatment\"]\nconf_int = probit_model.conf_int().loc[\"treatment\"]\n\n# Output summary\npd.DataFrame([{\n    \"Probit Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nProbit Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.08678\n0.02788\n0.00185\n0.03214\n0.14143\n\n\n\n\n\n\n\nThe probit model replicates Table 3, Column 1 of Karlan and List (2007), with a significant positive treatment effect (coefficient ≈ 0.087, p ≈ 0.002). This again confirms that individuals are more likely to donate when offered a matching grant.\nTogether, these results demonstrate a consistent and statistically significant treatment effect, providing strong evidence that the framing of charitable solicitations matters for donor behavior.\nThe match incentive not only has a practical impact but also a statistically robust one, even under a probit framework.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\nalso include p-velue 2:1 vs 1:1, 3:1 vs 2:1, 3:1 vs 1:1\nfor ols regresssion only have intercept, ration2, ratio3\nraw difference (2:1-1:1) raw difference (3:1 - 2:1) fitted difference (2:1 - 1:1) fitted difference (3:1 - 2:1)\n\nResponse Rates by Match Ratio\n\nmatched_data = data[(data[\"treatment\"] == 1) & (data[\"ratio\"].isin([1, 2, 3]))]\n\n# Calculate means\nresponse_rates = matched_data.groupby(\"ratio\")[\"gave\"].mean()\n\n# Separate groups for pairwise comparisons\ngave_1 = matched_data[matched_data[\"ratio\"] == 1][\"gave\"]\ngave_2 = matched_data[matched_data[\"ratio\"] == 2][\"gave\"]\ngave_3 = matched_data[matched_data[\"ratio\"] == 3][\"gave\"]\n\n# T-tests for pairwise comparisons\nfrom scipy import stats\n\nsummary = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\", \"3:1 vs 1:1\"],\n    \"p-value\": [\n        round(stats.ttest_ind(gave_2, gave_1, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_2, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_1, equal_var=False).pvalue, 5)\n    ],\n    \"Rate A\": [round(gave_2.mean(), 5), round(gave_3.mean(), 5), round(gave_3.mean(), 5)],\n    \"Rate B\": [round(gave_1.mean(), 5), round(gave_2.mean(), 5), round(gave_1.mean(), 5)],\n    \"Difference (A - B)\": [\n        round(gave_2.mean() - gave_1.mean(), 5),\n        round(gave_3.mean() - gave_2.mean(), 5),\n        round(gave_3.mean() - gave_1.mean(), 5)\n    ]\n})\n\nsummary\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_20691/2990980929.py:4: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nComparison\np-value\nRate A\nRate B\nDifference (A - B)\n\n\n\n\n0\n2:1 vs 1:1\n0.33453\n0.02263\n0.02075\n0.00188\n\n\n1\n3:1 vs 2:1\n0.96003\n0.02273\n0.02263\n0.00010\n\n\n2\n3:1 vs 1:1\n0.31011\n0.02273\n0.02075\n0.00198\n\n\n\n\n\n\n\nObserved donation rates:\n1:1 match — 2.07%\n2:1 match — 2.26%\n3:1 match — 2.27%\nThe increase from 1:1 to 2:1 and 3:1 appears small.\nNone of the pairwise comparisons are statistically significant. This supports the paper’s statement on page 8 that larger match ratios do not lead to meaningfully higher donation rates.\n\n\nRegression: Match Ratio Effects\n\n# Regression with dummy variables (baseline: 1:1 match)\nimport statsmodels.formula.api as smf\n\nmatched_data[\"ratio2\"] = (matched_data[\"ratio\"] == 2).astype(int)\nmatched_data[\"ratio3\"] = (matched_data[\"ratio\"] == 3).astype(int)\n\nreg_model = smf.ols(\"gave ~ ratio2 + ratio3\", data=matched_data).fit()\n\n# Clean formatted output\ncoefs = reg_model.params\nstderr = reg_model.bse\npvals = reg_model.pvalues\nconfint = reg_model.conf_int()\n\npd.DataFrame({\n    \"Coefficient\": coefs.round(5),\n    \"Std. Error\": stderr.round(5),\n    \"p-value\": pvals.round(5),\n    \"95% CI Lower\": confint[0].round(5),\n    \"95% CI Upper\": confint[1].round(5)\n}).loc[[\"Intercept\", \"ratio2\", \"ratio3\"]].reset_index().rename(columns={\"index\": \"Term\"})\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_20691/759364550.py:4: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_20691/759364550.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.02075\n0.00139\n0.00000\n0.01802\n0.02348\n\n\n1\nratio2\n0.00188\n0.00197\n0.33828\n-0.00197\n0.00574\n\n\n2\nratio3\n0.00198\n0.00197\n0.31332\n-0.00187\n0.00584\n\n\n\n\n\n\n\nRegression results:\nThe baseline (1:1 match) donation rate is about 2.07%.\nThe 2:1 match effect: +0.19 percentage points (not statistically significant).\nThe 3:1 match effect: +0.20 percentage points (also not statistically significant).\nNeither the 2:1 nor 3:1 match ratio coefficients are statistically significant. The results suggest that changing the size of the match ratio does not significantly change donation likelihood relative to 1:1.\n\n\nDifference in Response Rates (Data vs. Regression Coefficients)\n\n# Mean differences and coefficient comparisons\npd.DataFrame([\n    {\n        \"Comparison\": \"2:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_2.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 2:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_2.mean(), 5),\n        \"Regression Coefficient Diff (3 - 2)\": round(reg_model.params[\"ratio3\"] - reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio3\"], 5)\n    }\n])\n\n\n\n\n\n\n\n\nComparison\nRaw Mean Difference\nRegression Coefficient\nRegression Coefficient Diff (3 - 2)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\nNaN\n\n\n1\n3:1 vs 2:1\n0.00010\nNaN\n0.0001\n\n\n2\n3:1 vs 1:1\n0.00198\n0.00198\nNaN\n\n\n\n\n\n\n\nThe differences in donation rates between 1:1, 2:1, and 3:1 match offers are very small and not statistically significant. These findings replicate the comment on page 8 of Karlan and List (2007): “Larger match ratios relative to a smaller match ratio had no additional impact.”\nThis suggests that donors may respond to the presence of a match, but not necessarily to the size of the match. Psychologically, the idea of having one’s donation matched could serve as a signal of trust or endorsement—but the exact multiplier does not further influence behavior.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\ncharts side by side and scale #### All Respondents: Does Treatment Affect Amount Donated?\n\namount_reg_all = smf.ols(\"amount ~ treatment\", data=data).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_all.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_all.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_all.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_all.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_all.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_all.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.15361\n0.08256\n1.861\n0.06282\n-0.00822\n0.31543\n\n\n\n\n\n\n\nCoefficient on treatment ≈ 0.15\np-value ≈ 0.063\nAmong all individuals (including non-donors), the treatment group donated about $0.15 more on average. This effect is marginally significant (p ≈ 0.063). It suggests that matching increases expected donations slightly, but much of that effect may be driven by more people giving (rather than giving more).\nThis suggests that those who received a matching letter donated slightly more on average. However, the result is only marginally significant (at the 10% level). This weak evidence may indicate that the offer of a match has a small impact on the total amount donated—though for most people, the presence of the match does not substantially alter donation size.\n\nConditional on Donation: Do Donors Give More if Matched?\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1]\n\n# Linear regression among donors only\namount_reg_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_donors.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_donors.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_donors.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_donors.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_donors.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_donors.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n-1.66839\n2.87238\n-0.581\n0.56148\n-7.30477\n3.96799\n\n\n\n\n\n\n\nCoefficient on treatment ≈ -1.67\np-value = 0.561\nAmong those who did donate, receiving a match letter did not significantly change the amount given. In fact, the coefficient is slightly negative, though not significant. Thus, we conclude that while match offers may increase the number of donors, they do not cause donors to give more, conditional on giving.\nThis coefficient does not have a strong causal interpretation, because donation decisions and donation amounts are jointly determined and the sample is selected on gave == 1.\n\n\nDistribution of Donations Among Donors\n\ntreatment_donors = data[(data[\"treatment\"] == 1) & (data[\"gave\"] == 1)]\ncontrol_donors = data[(data[\"treatment\"] == 0) & (data[\"gave\"] == 1)]\n\nmean_treat = treatment_donors[\"amount\"].mean()\nmean_control = control_donors[\"amount\"].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\nsns.histplot(treatment_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[0])\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f\"Mean: ${mean_treat:.2f}\")\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\nsns.histplot(control_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[1])\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f\"Mean: ${mean_control:.2f}\")\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nTwo histograms show the distribution of donation amounts for the treatment group and control group, restricted to those who donated. The vertical red line marks the average for each group:\nTreatment Mean: ~$43.87\nControl Mean: ~$45.54\nBoth distributions are right-skewed, with most donors giving small amounts and a few contributing large sums. There is no visible shift in the average due to the treatment.\n\n\nConclusion\nThese analyses support the idea that matching offers increase response rate, but do not change how much people give once they’ve decided to donate. This distinction is important for fundraising strategies: matching may motivate more people to give, but it doesn’t necessarily increase per-donor revenue."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_sim = 10000\nnp.random.seed(42)\n# Simulate 10,000 binary outcomes for each group\ncontrol_sim = np.random.binomial(1, p_control, 100000)\ntreatment_sim = np.random.binomial(1, p_treatment, n_sim)\n\n# Vector of differences\ndiffs = treatment_sim - control_sim[:10000]\n\n# Cumulative average of the differences\ncum_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(8, 4))\nplt.plot(cum_avg, label='Cumulative Average Difference', color='orange')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers: Cumulative Average of Simulated Differences\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\nThis plot demonstrates the Law of Large Numbers. As we simulate more and more observations, the cumulative average of the differences converges toward the true mean difference (0.004). Initially, there’s randomness and fluctuation, but the line stabilizes as the number of observations increases.\nThis convergence is the foundation for using sample averages to estimate population parameters and underpins why large sample sizes give us more reliable estimates in experiments.\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nsample_sizes = [50, 200, 500, 1000]\nn_reps = 1000\n\nnp.random.seed(42)\nhistograms = {}\n\n# Simulate average differences for each sample size\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(n_reps):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    histograms[n] = diffs\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(histograms[n], bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero Reference\")\n    axes[i].axvline(true_diff, color='green', linestyle='--', label=\"True Difference (0.004)\")\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\nThese four histograms illustrate how the sampling distribution of the difference in means behaves at increasing sample sizes:\nAt n = 50, the distribution is quite wide and skewed. Zero is within the center-ish but not tightly.\nAs sample size increases, the distribution becomes tighter, more symmetric, and centered.\nBy n = 1000, the distribution of average differences closely resembles a normal distribution centered near the true mean difference (0.004).\nThis is a direct illustration of the Central Limit Theorem:\nAs sample size increases, the distribution of the sample mean difference becomes approximately normal, regardless of the original distribution shape.\nAlso note: zero shifts from being more “middle-ish” in smaller samples to being closer to the tail as the signal (the true effect) dominates the noise."
  },
  {
    "objectID": "hw1_questions (1).html",
    "href": "hw1_questions (1).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (1).html#introduction",
    "href": "hw1_questions (1).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (1).html#data",
    "href": "hw1_questions (1).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions (1).html#experimental-results",
    "href": "hw1_questions (1).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions (1).html#simulation-experiment",
    "href": "hw1_questions (1).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndata.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another at the 95% confidence level. If randomization was properly executed, we should expect no statistically significant differences in pre-treatment characteristics between the groups.\nI begin by testing the variable mrm2, which captures the number of months since the last donation. This variable is useful for checking balance because it is unrelated to the treatment assignment and reflects donor history.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\nvars_to_test = [\"mrm2\", \"amount\", \"years\", \"freq\"]\nresults = []\n\nfor var in vars_to_test:\n    subset = data[[\"treatment\", var]].dropna()\n    control = subset[subset[\"treatment\"] == 0][var]\n    treatment = subset[subset[\"treatment\"] == 1][var]\n    \n    # T-test\n    t_stat, p_val = stats.ttest_ind(treatment, control, equal_var=False)\n    \n    # Linear regression\n    regression = smf.ols(f\"{var} ~ treatment\", data=subset).fit()\n    coef = regression.params[\"treatment\"]\n    reg_p = regression.pvalues[\"treatment\"]\n    \n    # Difference in means\n    diff = treatment.mean() - control.mean()\n    \n    results.append({\n        \"Variable\": var,\n        \"Diff (Treat - Control)\": round(diff, 5),\n        \"T-test p-value\": round(p_val, 5),\n        \"Regression Coef\": round(coef, 5),\n        \"Regression p-value\": round(reg_p, 5)\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\nVariable\nDiff (Treat - Control)\nT-test p-value\nRegression Coef\nRegression p-value\n\n\n\n\n0\nmrm2\n0.01369\n0.90485\n0.01369\n0.90489\n\n\n1\namount\n0.15361\n0.05509\n0.15361\n0.06282\n\n\n2\nyears\n-0.05755\n0.27532\n-0.05755\n0.27002\n\n\n3\nfreq\n-0.01198\n0.91174\n-0.01198\n0.91170\n\n\n\n\n\n\n\nThe table of results above shows no statistically significant differences at the 5% level for any variable (p-values &gt; 0.05), though amount is marginally close (p ≈ 0.06 in the regression). This is consistent with proper random assignment.\nThese checks are similar to what Karlan and List report in Table 1 of the original paper, which reassures readers that the treatment effect estimates later in the paper can be interpreted as causal. If pre-treatment covariates are balanced, then observed differences in outcomes are more likely attributable to the randomized treatment itself.\nThese results mirror those presented in Table 1 of Karlan and List (2007), which shows no significant differences between the groups in prior donation behavior and demographic characteristics. Table 1 serves to reassure the reader that any observed treatment effects later in the analysis can be confidently attributed to the randomized intervention rather than pre-existing differences between groups."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nDonation Rate by Group\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"group\"] = donation_rates[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Create barplot\nplt.figure(figsize=(6, 4))\nax = sns.barplot(data=donation_rates, x=\"group\", y=\"gave\")\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"\")\nplt.title(\"Donation Rate by Group\")\nplt.ylim(0, 0.03)\nplt.grid(axis='y')\n\n# Add percentage labels on top\nfor i, val in enumerate(donation_rates[\"gave\"]):\n    ax.text(i, val + 0.0005, f\"{val:.3%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar plot shows that the treatment group—who received matching grant letters—had a higher donation rate than the control group.\n\n\nT-Test and Linear Regression\n\ntreat_gave = data[data['treatment'] == 1]['gave']\ncontrol_gave = data[data['treatment'] == 0]['gave']\nt_stat, t_pval = stats.ttest_ind(treat_gave, control_gave, equal_var=False)\n\n# Format output\npd.DataFrame([{\n    \"T-test Statistic\": round(t_stat, 3),\n    \"T-test p-value\": round(t_pval, 5)\n}])\n\n\n\n\n\n\n\n\nT-test Statistic\nT-test p-value\n\n\n\n\n0\n3.209\n0.00133\n\n\n\n\n\n\n\n\ngave_regression = smf.ols(\"gave ~ treatment\", data=data).fit()\ncoef = gave_regression.params[\"treatment\"]\nstd_err = gave_regression.bse[\"treatment\"]\np_val = gave_regression.pvalues[\"treatment\"]\nconf_int = gave_regression.conf_int().loc[\"treatment\"]\n\n# Format output\npd.DataFrame([{\n    \"Treatment Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.00418\n0.00135\n0.00193\n0.00154\n0.00682\n\n\n\n\n\n\n\nThe difference in donation rates is statistically significant at the 1% level.\nThe treatment group is more likely to donate, increasing the probability of giving by about 0.42 percentage points.\nThis replicates the result from Table 2A Panel A in Karlan & List (2007), showing that a match offer significantly boosts participation.\nOLS regression shows a statistically significant positive coefficient (≈ 0.0042) on the treatment variable. This confirms the t-test: assignment to the treatment group increased the likelihood of making a donation.\nThis suggests that even a small behavioral nudge like mentioning a matching donation makes people more likely to contribute to charity. People respond to the perception of increased impact.\n\n# Probit model\nimport statsmodels.api as sm\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\ncoef = probit_model.params[\"treatment\"]\nstd_err = probit_model.bse[\"treatment\"]\np_val = probit_model.pvalues[\"treatment\"]\nconf_int = probit_model.conf_int().loc[\"treatment\"]\n\n# Output summary\npd.DataFrame([{\n    \"Probit Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nProbit Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.08678\n0.02788\n0.00185\n0.03214\n0.14143\n\n\n\n\n\n\n\nThe probit model replicates Table 3, Column 1 of Karlan and List (2007), with a significant positive treatment effect (coefficient ≈ 0.087, p ≈ 0.002). This again confirms that individuals are more likely to donate when offered a matching grant.\nTogether, these results demonstrate a consistent and statistically significant treatment effect, providing strong evidence that the framing of charitable solicitations matters for donor behavior.\nThe match incentive not only has a practical impact but also a statistically robust one, even under a probit framework.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nResponse Rates by Match Ratio\n\nmatched_data = data[(data[\"treatment\"] == 1) & (data[\"ratio\"].isin([1, 2, 3]))]\n\n# Calculate means\nresponse_rates = matched_data.groupby(\"ratio\")[\"gave\"].mean()\n\n# Separate groups for pairwise comparisons\ngave_1 = matched_data[matched_data[\"ratio\"] == 1][\"gave\"]\ngave_2 = matched_data[matched_data[\"ratio\"] == 2][\"gave\"]\ngave_3 = matched_data[matched_data[\"ratio\"] == 3][\"gave\"]\n\n# T-tests for pairwise comparisons\nfrom scipy import stats\n\nsummary = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\", \"3:1 vs 1:1\"],\n    \"p-value\": [\n        round(stats.ttest_ind(gave_2, gave_1, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_2, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_1, equal_var=False).pvalue, 5)\n    ],\n    \"Rate A\": [round(gave_2.mean(), 5), round(gave_3.mean(), 5), round(gave_3.mean(), 5)],\n    \"Rate B\": [round(gave_1.mean(), 5), round(gave_2.mean(), 5), round(gave_1.mean(), 5)],\n    \"Difference (A - B)\": [\n        round(gave_2.mean() - gave_1.mean(), 5),\n        round(gave_3.mean() - gave_2.mean(), 5),\n        round(gave_3.mean() - gave_1.mean(), 5)\n    ]\n})\n\nsummary\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_40228/2990980929.py:4: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nComparison\np-value\nRate A\nRate B\nDifference (A - B)\n\n\n\n\n0\n2:1 vs 1:1\n0.33453\n0.02263\n0.02075\n0.00188\n\n\n1\n3:1 vs 2:1\n0.96003\n0.02273\n0.02263\n0.00010\n\n\n2\n3:1 vs 1:1\n0.31011\n0.02273\n0.02075\n0.00198\n\n\n\n\n\n\n\nObserved donation rates:\n1:1 match — 2.07%\n2:1 match — 2.26%\n3:1 match — 2.27%\nThe increase from 1:1 to 2:1 and 3:1 appears small.\nNone of the pairwise comparisons are statistically significant. This supports the paper’s statement on page 8 that larger match ratios do not lead to meaningfully higher donation rates.\n\n\nRegression: Match Ratio Effects\n\n# Regression with dummy variables (baseline: 1:1 match)\nimport statsmodels.formula.api as smf\n\nmatched_data[\"ratio2\"] = (matched_data[\"ratio\"] == 2).astype(int)\nmatched_data[\"ratio3\"] = (matched_data[\"ratio\"] == 3).astype(int)\n\nreg_model = smf.ols(\"gave ~ ratio2 + ratio3\", data=matched_data).fit()\n\n# Clean formatted output\ncoefs = reg_model.params\nstderr = reg_model.bse\npvals = reg_model.pvalues\nconfint = reg_model.conf_int()\n\npd.DataFrame({\n    \"Coefficient\": coefs.round(5),\n    \"Std. Error\": stderr.round(5),\n    \"p-value\": pvals.round(5),\n    \"95% CI Lower\": confint[0].round(5),\n    \"95% CI Upper\": confint[1].round(5)\n}).loc[[\"Intercept\", \"ratio2\", \"ratio3\"]].reset_index().rename(columns={\"index\": \"Term\"})\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_40228/759364550.py:4: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_40228/759364550.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.02075\n0.00139\n0.00000\n0.01802\n0.02348\n\n\n1\nratio2\n0.00188\n0.00197\n0.33828\n-0.00197\n0.00574\n\n\n2\nratio3\n0.00198\n0.00197\n0.31332\n-0.00187\n0.00584\n\n\n\n\n\n\n\nRegression results:\nThe baseline (1:1 match) donation rate is about 2.07%.\nThe 2:1 match effect: +0.19 percentage points (not statistically significant).\nThe 3:1 match effect: +0.20 percentage points (also not statistically significant).\nNeither the 2:1 nor 3:1 match ratio coefficients are statistically significant. The results suggest that changing the size of the match ratio does not significantly change donation likelihood relative to 1:1.\n\n\nDifference in Response Rates (Data vs. Regression Coefficients)\n\n# Mean differences and coefficient comparisons\npd.DataFrame([\n    {\n        \"Comparison\": \"2:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_2.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 2:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_2.mean(), 5),\n        \"Regression Coefficient Diff (3 - 2)\": round(reg_model.params[\"ratio3\"] - reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio3\"], 5)\n    }\n])\n\n\n\n\n\n\n\n\nComparison\nRaw Mean Difference\nRegression Coefficient\nRegression Coefficient Diff (3 - 2)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\nNaN\n\n\n1\n3:1 vs 2:1\n0.00010\nNaN\n0.0001\n\n\n2\n3:1 vs 1:1\n0.00198\n0.00198\nNaN\n\n\n\n\n\n\n\nThe differences in donation rates between 1:1, 2:1, and 3:1 match offers are very small and not statistically significant. These findings replicate the comment on page 8 of Karlan and List (2007): “Larger match ratios relative to a smaller match ratio had no additional impact.”\nThis suggests that donors may respond to the presence of a match, but not necessarily to the size of the match. Psychologically, the idea of having one’s donation matched could serve as a signal of trust or endorsement—but the exact multiplier does not further influence behavior.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ncharts side by side and scale #### All Respondents: Does Treatment Affect Amount Donated?\n\namount_reg_all = smf.ols(\"amount ~ treatment\", data=data).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_all.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_all.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_all.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_all.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_all.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_all.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.15361\n0.08256\n1.861\n0.06282\n-0.00822\n0.31543\n\n\n\n\n\n\n\nCoefficient on treatment ≈ 0.15\np-value ≈ 0.063\nAmong all individuals (including non-donors), the treatment group donated about $0.15 more on average. This effect is marginally significant (p ≈ 0.063). It suggests that matching increases expected donations slightly, but much of that effect may be driven by more people giving (rather than giving more).\nThis suggests that those who received a matching letter donated slightly more on average. However, the result is only marginally significant (at the 10% level). This weak evidence may indicate that the offer of a match has a small impact on the total amount donated—though for most people, the presence of the match does not substantially alter donation size.\n\nConditional on Donation: Do Donors Give More if Matched?\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1]\n\n# Linear regression among donors only\namount_reg_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_donors.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_donors.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_donors.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_donors.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_donors.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_donors.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n-1.66839\n2.87238\n-0.581\n0.56148\n-7.30477\n3.96799\n\n\n\n\n\n\n\nCoefficient on treatment ≈ -1.67\np-value = 0.561\nAmong those who did donate, receiving a match letter did not significantly change the amount given. In fact, the coefficient is slightly negative, though not significant. Thus, we conclude that while match offers may increase the number of donors, they do not cause donors to give more, conditional on giving.\nThis coefficient does not have a strong causal interpretation, because donation decisions and donation amounts are jointly determined and the sample is selected on gave == 1.\n\n\nDistribution of Donations Among Donors\n\ntreatment_donors = data[(data[\"treatment\"] == 1) & (data[\"gave\"] == 1)]\ncontrol_donors = data[(data[\"treatment\"] == 0) & (data[\"gave\"] == 1)]\n\nmean_treat = treatment_donors[\"amount\"].mean()\nmean_control = control_donors[\"amount\"].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\nsns.histplot(treatment_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[0])\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f\"Mean: ${mean_treat:.2f}\")\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\nsns.histplot(control_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[1])\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f\"Mean: ${mean_control:.2f}\")\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nTwo histograms show the distribution of donation amounts for the treatment group and control group, restricted to those who donated. The vertical red line marks the average for each group:\nTreatment Mean: ~$43.87\nControl Mean: ~$45.54\nBoth distributions are right-skewed, with most donors giving small amounts and a few contributing large sums. There is no visible shift in the average due to the treatment.\n\n\nConclusion\nThese analyses support the idea that matching offers increase response rate, but do not change how much people give once they’ve decided to donate. This distinction is important for fundraising strategies: matching may motivate more people to give, but it doesn’t necessarily increase per-donor revenue."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_sim = 10000\nnp.random.seed(42)\n# Simulate 10,000 binary outcomes for each group\ncontrol_sim = np.random.binomial(1, p_control, 100000)\ntreatment_sim = np.random.binomial(1, p_treatment, n_sim)\n\n# Vector of differences\ndiffs = treatment_sim - control_sim[:10000]\n\n# Cumulative average of the differences\ncum_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(8, 4))\nplt.plot(cum_avg, label='Cumulative Average Difference', color='orange')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers: Cumulative Average of Simulated Differences\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot demonstrates the Law of Large Numbers. As we simulate more and more observations, the cumulative average of the differences converges toward the true mean difference (0.004). Initially, there’s randomness and fluctuation, but the line stabilizes as the number of observations increases.\nThis convergence is the foundation for using sample averages to estimate population parameters and underpins why large sample sizes give us more reliable estimates in experiments.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nsample_sizes = [50, 200, 500, 1000]\nn_reps = 1000\n\nnp.random.seed(42)\nhistograms = {}\n\n# Simulate average differences for each sample size\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(n_reps):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    histograms[n] = diffs\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(histograms[n], bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero Reference\")\n    axes[i].axvline(true_diff, color='green', linestyle='--', label=\"True Difference (0.004)\")\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese four histograms illustrate how the sampling distribution of the difference in means behaves at increasing sample sizes:\nAt n = 50, the distribution is quite wide and skewed. Zero is within the center-ish but not tightly.\nAs sample size increases, the distribution becomes tighter, more symmetric, and centered.\nBy n = 1000, the distribution of average differences closely resembles a normal distribution centered near the true mean difference (0.004).\nThis is a direct illustration of the Central Limit Theorem:\nAs sample size increases, the distribution of the sample mean difference becomes approximately normal, regardless of the original distribution shape.\nAlso note: zero shifts from being more “middle-ish” in smaller samples to being closer to the tail as the signal (the true effect) dominates the noise."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n\n\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python.\n\n\n\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "href": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nTo model the number of patents awarded per firm, we assume the data follow a Poisson distribution with parameter λ. This is appropriate for count data observed over a fixed time period. In this section, we define the Poisson log-likelihood, visualize it, and estimate the maximum likelihood value of λ both analytically and numerically.\n\n\nStep 1: Load and Inspect the Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Load patent data\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\nWe load the blueprinty.csv dataset and extract the number of patents for each firm, stored in the variable Y.\n\n\n\nStep 2: Define the Log-Likelihood Function\n\n# Negative log-likelihood function (for minimization)\ndef poisson_neg_log_likelihood(lmbda, Y):\n    return -np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nThis is the log-likelihood of the Poisson model (negated for use with optimization). We use gammaln(Y + 1) for numerical stability in place of log(Y!).\n\n\n\nStep 3: Visualize the Log-Likelihood Curve\n\nlambdas = np.linspace(0.1, 10, 200)\nlog_liks = [-poisson_neg_log_likelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks, label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label=\"Sample Mean (Ȳ)\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis curve shows how the log-likelihood varies with different values of λ. The red line indicates the sample mean, which aligns closely with the peak of the curve.\n\n\n\nStep 4: Analytical MLE for Poisson\n\nlambda_mle_analytical = np.mean(Y)\nlambda_mle_analytical\n\n3.6846666666666668\n\n\n\nThis result comes from solving the first derivative of the log-likelihood with respect to λ, setting it to zero:\n\\[ \\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_i \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = 0 \\] Solving gives: \\[ \\hat{\\lambda}_{MLE} = \\bar{Y} \\]\n\nThis derivation confirms that the maximum likelihood estimator for λ is simply the mean of the observed data.\n\n\n\nStep 5: Estimate λ Numerically\n\n# Find lambda using numerical optimization\nopt_result = minimize(lambda l: poisson_neg_log_likelihood(l, Y), x0=[1.0], bounds=[(0.001, None)])\nlambda_mle_numerical = opt_result.x[0]\nlambda_mle_numerical\n\n3.684666485763343\n\n\nUsing numerical optimization, we minimize the negative log-likelihood to obtain λ̂. The result closely matches the sample mean, confirming our analytical solution.\n\n\n\nConclusion\nThis process demonstrates how to derive and estimate the Poisson MLE both mathematically and computationally. We verified that: - The log-likelihood peaks at the sample mean - The analytical and numerical MLEs are identical This lays a strong foundation for moving into more complex models like Poisson regression.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model",
    "href": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load and prepare the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create design matrix X with intercept, age, age squared, region dummies, and customer status\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display summary\npoisson_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npatents\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nSun, 04 May 2025\nDeviance:\n2143.3\n\n\nTime:\n13:48:09\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nC(region)[T.Northeast]\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nC(region)[T.Northwest]\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nC(region)[T.South]\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nC(region)[T.Southwest]\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "href": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load and prepare the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create design matrix X with intercept, age, age squared, region dummies, and customer status\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\n# Extract summary data\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    'index': 'Variable',\n    'Coef.': 'Coefficient',\n    'Std.Err.': 'Std. Error',\n    'z': 'z-value',\n    'P&gt;|z|': 'p-value',\n    '[0.025': '95% CI Lower',\n    '0.975]': '95% CI Upper'\n}, inplace=True)\n\n# Keep only selected columns and round values\nformatted_summary = summary_table[[\n    'Variable', 'Coefficient', 'Std. Error', 'z-value', 'p-value', '95% CI Lower', '95% CI Upper'\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nC(region)[T.Northeast]\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n2\nC(region)[T.Northwest]\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n3\nC(region)[T.South]\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n4\nC(region)[T.Southwest]\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n5\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n6\nage2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n7\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study-1",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study-1",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nWe assume that the number of reviews is a reasonable proxy for the number of bookings. While not every booking results in a review, listings with more bookings are generally expected to accumulate more reviews over time. Therefore, analyzing review counts can help us understand what drives customer engagement."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-2",
    "href": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-2",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load and prepare the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create design matrix X with intercept, age, age squared, region dummies, and customer status\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\nPoisson Regression Results (Formatted)\n\n# Extract summary data\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    'index': 'Variable',\n    'Coef.': 'Coefficient',\n    'Std.Err.': 'Std. Error',\n    'z': 'z-value',\n    'P&gt;|z|': 'p-value',\n    '[0.025': '95% CI Lower',\n    '0.975]': '95% CI Upper'\n}, inplace=True)\n\n# Keep only selected columns and round values\nformatted_summary = summary_table[[\n    'Variable', 'Coefficient', 'Std. Error', 'z-value', 'p-value', '95% CI Lower', '95% CI Upper'\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nC(region)[T.Northeast]\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n2\nC(region)[T.Northwest]\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n3\nC(region)[T.South]\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n4\nC(region)[T.Southwest]\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n5\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n6\nage2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n7\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study-poisson-regression",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study-poisson-regression",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study: Poisson Regression",
    "text": "AirBnB Case Study: Poisson Regression\n\nData Preparation\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n\ndf = pd.read_csv(\"airbnb.csv\")\n\n\ncols = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"number_of_reviews\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[cols].dropna()\n\n\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"f\": 0, \"t\": 1})\n\nWe selected key predictors that are likely to influence bookings. This includes listing price, type of room offered, number of bedrooms/bathrooms, various review scores, and whether the listing allows instant booking. We removed rows with missing values to ensure model stability.\n\n\nReview Count Distribution\n\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nReview counts are highly skewed, with many listings having fewer than 50 reviews and a long right tail — a classic case for using a Poisson model.\n\n\nBoxplot: Reviews by Room Type\n\nsns.boxplot(data=df_clean, x=\"room_type\", y=\"number_of_reviews\")\nplt.ylim(0, 100)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\nEntire homes tend to receive the most reviews, suggesting higher demand compared to shared and private rooms.\n\n\nCorrelation Heatmap\n\nnumeric_cols = [\"price\", \"bedrooms\", \"bathrooms\", \n                \"review_scores_cleanliness\", \"review_scores_location\", \n                \"review_scores_value\", \"number_of_reviews\"]\nsns.heatmap(df_clean[numeric_cols].corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Between Variables\")\nplt.show()\n\n\n\n\n\n\n\n\nThis helps us check for multicollinearity and informs variable selection in our regression model.\n\n\nSummary Statistics\n\ndf_clean.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nbathrooms\n30160.0\n1.122132\n0.384916\n0.0\n1.0\n1.0\n1.0\n6.0\n\n\nbedrooms\n30160.0\n1.151459\n0.699010\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n30160.0\n140.206863\n188.392314\n10.0\n70.0\n103.0\n169.0\n10000.0\n\n\nnumber_of_reviews\n30160.0\n21.170889\n32.007541\n1.0\n3.0\n8.0\n26.0\n421.0\n\n\nreview_scores_cleanliness\n30160.0\n9.201724\n1.114261\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30160.0\n9.415351\n0.843185\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30160.0\n9.333952\n0.900472\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ninstant_bookable\n30160.0\n0.196187\n0.397118\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nThis table gives a statistical overview of the cleaned dataset. For example, we can see the median number of reviews, typical price levels, and average review scores. It also helps confirm the need for count modeling, given the range and skew of the number_of_reviews variable.\n\n\nModel Specification and Fitting\n\nX = dmatrix(\n    \"1 + price + bedrooms + bathrooms + review_scores_cleanliness + \"\n    \"review_scores_location + review_scores_value + C(room_type) + instant_bookable\",\n    data=df_clean,\n    return_type=\"dataframe\"\n)\nY = df_clean[\"number_of_reviews\"]\n\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nThe model uses a log link function, where the log of the expected number of reviews is modeled as a linear function of the predictors. This means that coefficients represent multiplicative effects on the count of reviews. For example, a coefficient of 0.1 implies approximately a 10.5% increase in expected review count (exp(0.1) ≈ 1.105).\n\n\nPoisson Regression Results (Formatted)\n\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"z\": \"z-value\",\n    \"P&gt;|z|\": \"p-value\",\n    \"[0.025\": \"95% CI Lower\",\n    \"0.975]\": \"95% CI Upper\"\n}, inplace=True)\n\nformatted_summary = summary_table[[\n    \"Variable\", \"Coefficient\", \"Std. Error\", \"z-value\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n3.5725\n0.0160\n223.2145\n0.0000\n3.5411\n3.6039\n\n\n1\nC(room_type)[T.Private room]\n-0.0145\n0.0027\n-5.3104\n0.0000\n-0.0199\n-0.0092\n\n\n2\nC(room_type)[T.Shared room]\n-0.2519\n0.0086\n-29.2286\n0.0000\n-0.2688\n-0.2350\n\n\n3\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n-0.0000\n0.0000\n\n\n4\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n0.0710\n0.0788\n\n\n5\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n-0.1313\n-0.1167\n\n\n6\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n0.1103\n0.1161\n\n\n7\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n-0.0799\n-0.0736\n\n\n8\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n-0.0951\n-0.0880\n\n\n9\ninstant_bookable\n0.3344\n0.0029\n115.7477\n0.0000\n0.3287\n0.3401\n\n\n\n\n\n\n\n\n\nInterpretation\nThe coefficients tell us how listing features are associated with review count, holding other variables constant:\n\nIntercept: The baseline log count for an average listing (Entire home, not instant bookable, average scores).\nroom_type (Shared room): A strong negative effect. These listings get ~22% as many reviews as entire home listings (exp(-0.25) ≈ 0.78). This aligns with expectations—shared rooms are less popular.\nroom_type (Private room): Also shows a slight negative effect, but far smaller than shared rooms.\ninstant_bookable: This feature has a large positive effect—instant booking listings are expected to get ~40% more reviews, all else equal (exp(0.33) ≈ 1.39). This underscores the importance of convenience to guests.\ncleanliness score: Every 1-point increase in cleanliness adds over 11% more reviews, highlighting the value of positive guest experiences.\nbedrooms: Positively associated with reviews, possibly because larger listings serve more guests or accommodate longer stays.\nbathrooms: Shows a surprising negative coefficient, possibly reflecting multicollinearity with bedrooms or nonlinear effects.\nprice: The effect is slightly negative but not statistically significant at the 5% level. This may suggest that higher prices deter bookings only marginally, or other variables are absorbing the effect.\nlocation/value scores: Unexpectedly negative; this might reflect reverse causality (low-activity listings getting inflated scores), or correlation with other quality measures.\n\n\n\nModel Implications\nThe results show that instant bookability, room type, cleanliness, and listing size all play major roles in driving bookings/reviews. In particular, allowing instant booking and maintaining high cleanliness ratings seem to be key strategies for increasing engagement.\nHowever, some findings warrant further investigation—especially the negative coefficients on review score variables. These may reflect issues like multicollinearity, endogeneity, or nonlinear relationships not captured by this simple model.\nA natural next step would be to explore: - Interaction effects (e.g. cleanliness × room type) - Nonlinear terms (e.g. log(price)) - Alternative models like Negative Binomial to handle overdispersion\n\n\nConclusion\nPoisson regression provides a valuable framework for modeling Airbnb review counts as a function of listing characteristics. This analysis offers actionable insights for hosts seeking to increase visibility and bookings, while also highlighting areas where further modeling could improve interpretability and accuracy."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-2",
    "href": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-2",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nTo analyze the number of patents awarded to engineering firms, we assume the data follows a Poisson distribution. This is appropriate since patent counts are non-negative integers and often modeled using count distributions. We define the Poisson log-likelihood and use both analytical and numerical methods to estimate the parameter λ, which represents the average rate of events (patents per firm).\n\n\nStep 1: Load Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Load patent data\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\nWe begin by loading the dataset. Our outcome of interest is patents, the number of patents awarded over five years, stored as the array Y.\n\n\n\nStep 2: Define the Poisson Log-Likelihood\n\n# Define negative log-likelihood function for optimization\ndef poisson_neg_log_likelihood(lmbda, Y):\n    return -np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nThis function defines the negative log-likelihood of the Poisson model. We use gammaln(Y + 1) instead of log(Y!) for numerical stability. The log-likelihood is a function of λ and the observed data Y.\n\n\n\nStep 3: Plot the Log-Likelihood Curve\n\nlambdas = np.linspace(0.1, 10, 200)\nlog_liks = [-poisson_neg_log_likelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks, label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label=\"Sample Mean (Ȳ)\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows the log-likelihood as a function of λ. The peak of this curve corresponds to the maximum likelihood estimate (MLE) of λ. We also mark the sample mean of Y, which visually aligns with the MLE—a known property of the Poisson distribution.\n\n\n\nStep 4: Derive the Analytical MLE\n\nlambda_mle_analytical = np.mean(Y)\nlambda_mle_analytical\n\n3.6846666666666668\n\n\nFrom calculus, we know that the MLE for λ in a Poisson distribution is:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis makes intuitive sense because λ is the expected number of events, and the sample mean is our best estimate.\n\n\n\nStep 5: Estimate λ Numerically Using Optimization\n\nopt_result = minimize(lambda l: poisson_neg_log_likelihood(l, Y), x0=[1.0], bounds=[(0.001, None)])\nlambda_mle_numerical = opt_result.x[0]\nlambda_mle_numerical\n\n3.684666485763343\n\n\nWe also compute the MLE numerically by minimizing the negative log-likelihood using scipy.optimize.minimize. This confirms that the MLE from optimization closely matches the sample mean, providing validation for both methods.\n\n\n\nConclusion\nThis exercise demonstrates how to estimate a Poisson model using maximum likelihood, both analytically and numerically. The results confirm that the mean of the observed data is the MLE for λ in a Poisson distribution, consistent with theoretical expectations. This provides a strong foundation for moving toward more complex models, such as Poisson regression with covariates.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#exploratory-data-analysis-eda",
    "href": "blog/project2/hw2_questions.html#exploratory-data-analysis-eda",
    "title": "Poisson Regression Examples",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load and filter dataset\ndf = pd.read_csv(\"airbnb.csv\")\ncols = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"number_of_reviews\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[cols].dropna()\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"f\": 0, \"t\": 1})\n\n\nReview Count Distribution\n\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nReview counts are highly skewed, with many listings having fewer than 50 reviews and a long right tail — a classic case for using a Poisson model.\n\n\n\nBoxplot: Reviews by Room Type\n\nsns.boxplot(data=df_clean, x=\"room_type\", y=\"number_of_reviews\")\nplt.ylim(0, 100)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\nEntire homes tend to receive the most reviews, suggesting higher demand compared to shared and private rooms.\n\n\n\nCorrelation Heatmap\n\nnumeric_cols = [\"price\", \"bedrooms\", \"bathrooms\", \n                \"review_scores_cleanliness\", \"review_scores_location\", \n                \"review_scores_value\", \"number_of_reviews\"]\nsns.heatmap(df_clean[numeric_cols].corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Between Variables\")\nplt.show()\n\n\n\n\n\n\n\n\nThis helps us check for multicollinearity and informs variable selection in our regression model.\n\n\nSummary Statistics\n\ndf_clean.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nbathrooms\n30160.0\n1.122132\n0.384916\n0.0\n1.0\n1.0\n1.0\n6.0\n\n\nbedrooms\n30160.0\n1.151459\n0.699010\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n30160.0\n140.206863\n188.392314\n10.0\n70.0\n103.0\n169.0\n10000.0\n\n\nnumber_of_reviews\n30160.0\n21.170889\n32.007541\n1.0\n3.0\n8.0\n26.0\n421.0\n\n\nreview_scores_cleanliness\n30160.0\n9.201724\n1.114261\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30160.0\n9.415351\n0.843185\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30160.0\n9.333952\n0.900472\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ninstant_bookable\n30160.0\n0.196187\n0.397118\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nThis table gives a statistical overview of the cleaned dataset. For example, we can see the median number of reviews, typical price levels, and average review scores. It also helps confirm the need for count modeling, given the range and skew of the number_of_reviews variable."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#model-specification-and-fitting-1",
    "href": "blog/project2/hw2_questions.html#model-specification-and-fitting-1",
    "title": "Poisson Regression Examples",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\n\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\nX = dmatrix(\n    \"1 + price + bedrooms + bathrooms + review_scores_cleanliness + \"\n    \"review_scores_location + review_scores_value + C(room_type) + instant_bookable\",\n    data=df_clean,\n    return_type=\"dataframe\"\n)\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nThe Poisson model is appropriate for count data, using a log link to model the expected number of reviews as an exponential function of listing characteristics."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#poisson-regression-results-formatted-1",
    "href": "blog/project2/hw2_questions.html#poisson-regression-results-formatted-1",
    "title": "Poisson Regression Examples",
    "section": "Poisson Regression Results (Formatted)",
    "text": "Poisson Regression Results (Formatted)\n\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"z\": \"z-value\",\n    \"P&gt;|z|\": \"p-value\",\n    \"[0.025\": \"95% CI Lower\",\n    \"0.975]\": \"95% CI Upper\"\n}, inplace=True)\n\nformatted_summary = summary_table[[\n    \"Variable\", \"Coefficient\", \"Std. Error\", \"z-value\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n3.5725\n0.0160\n223.2145\n0.0000\n3.5411\n3.6039\n\n\n1\nC(room_type)[T.Private room]\n-0.0145\n0.0027\n-5.3104\n0.0000\n-0.0199\n-0.0092\n\n\n2\nC(room_type)[T.Shared room]\n-0.2519\n0.0086\n-29.2286\n0.0000\n-0.2688\n-0.2350\n\n\n3\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n-0.0000\n0.0000\n\n\n4\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n0.0710\n0.0788\n\n\n5\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n-0.1313\n-0.1167\n\n\n6\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n0.1103\n0.1161\n\n\n7\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n-0.0799\n-0.0736\n\n\n8\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n-0.0951\n-0.0880\n\n\n9\ninstant_bookable\n0.3344\n0.0029\n115.7477\n0.0000\n0.3287\n0.3401"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#interpretation-2",
    "href": "blog/project2/hw2_questions.html#interpretation-2",
    "title": "Poisson Regression Examples",
    "section": "Interpretation",
    "text": "Interpretation\nThe Poisson regression results provide insights into what drives bookings (proxied by reviews):\n\nShared rooms receive significantly fewer reviews than entire homes (exp(-0.25) ≈ 0.78).\nInstant bookable listings receive ~40% more reviews (exp(0.33) ≈ 1.39).\nCleanliness score has a strong positive effect on reviews.\nPrice is not statistically significant at the 5% level.\nBedrooms positively affect review count, while bathrooms show a small negative effect.\nLocation/value scores surprisingly show negative associations, possibly due to multicollinearity or reverse causality."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#conclusion-2",
    "href": "blog/project2/hw2_questions.html#conclusion-2",
    "title": "Poisson Regression Examples",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis fulfills the goal of modeling what factors drive review counts on Airbnb. EDA helped guide variable selection, and Poisson regression offered interpretable, statistically significant insights. The model shows that room type, convenience (instant booking), and cleanliness are key predictors of customer engagement."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model",
    "href": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\nTo model the number of patents awarded per firm, we assume the data follow a Poisson distribution with parameter λ. This is appropriate for count data observed over a fixed time period. In this section, we define the Poisson log-likelihood, visualize it, and estimate the maximum likelihood value of λ both analytically and numerically.\n\nStep 1: Load and Inspect the Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\nWe load the blueprinty.csv dataset and extract the number of patents for each firm, stored in the variable Y.\n\n\nStep 2: Define the Log-Likelihood Function\n\n# Negative log-likelihood function (for minimization)\ndef poisson_neg_log_likelihood(lmbda, Y):\n    return -np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nThis is the log-likelihood of the Poisson model (negated for use with optimization). We use gammaln(Y + 1) for numerical stability in place of log(Y!).\n\n\n\nStep 3: Visualize the Log-Likelihood Curve\n\nlambdas = np.linspace(0.1, 10, 200)\nlog_liks = [-poisson_neg_log_likelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks, label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label=\"Sample Mean (Ȳ)\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis curve shows how the log-likelihood varies with different values of λ. The red line indicates the sample mean, which aligns closely with the peak of the curve.\n\n\n\nStep 4: Analytical MLE for Poisson\n\nlambda_mle_analytical = np.mean(Y)\nlambda_mle_analytical\n\n3.6846666666666668\n\n\n\nThis result comes from solving the first derivative of the log-likelihood with respect to λ, setting it to zero:\n\\[ \\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_i \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = 0 \\] Solving gives: \\[ \\hat{\\lambda}_{MLE} = \\bar{Y} \\]\n\nThis derivation confirms that the maximum likelihood estimator for λ is simply the mean of the observed data.\n\n\nStep 5: Estimate λ Numerically\n\n# Find lambda using numerical optimization\nopt_result = minimize(lambda l: poisson_neg_log_likelihood(l, Y), x0=[1.0], bounds=[(0.001, None)])\nlambda_mle_numerical = opt_result.x[0]\nlambda_mle_numerical\n\n3.684666485763343\n\n\nUsing numerical optimization, we minimize the negative log-likelihood to obtain λ̂. The result closely matches the sample mean, confirming our analytical solution.\n\n\n\nConclusion\nThis process demonstrates how to derive and estimate the Poisson MLE both mathematically and computationally. We verified that: - The log-likelihood peaks at the sample mean - The analytical and numerical MLEs are identical This lays a strong foundation for moving into more complex models like Poisson regression.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\n# Extract summary data\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    'index': 'Variable',\n    'Coef.': 'Coefficient',\n    'Std.Err.': 'Std. Error',\n    'z': 'z-value',\n    'P&gt;|z|': 'p-value',\n    '[0.025': '95% CI Lower',\n    '0.975]': '95% CI Upper'\n}, inplace=True)\n\n# Keep only selected columns and round values\nformatted_summary = summary_table[[\n    'Variable', 'Coefficient', 'Std. Error', 'z-value', 'p-value', '95% CI Lower', '95% CI Upper'\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nC(region)[T.Northeast]\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n2\nC(region)[T.Northwest]\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n3\nC(region)[T.South]\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n4\nC(region)[T.Southwest]\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n5\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n6\nage2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n7\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project3/hw3_questions.html",
    "href": "blog/project3/hw3_questions.html",
    "title": "Multinomial Logit Model",
    "section": "",
    "text": "This assignment expores two methods for estimating the MNL model: (1) via Maximum Likelihood, and (2) via a Bayesian approach using a Metropolis-Hastings MCMC algorithm."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "href": "blog/project3/hw3_questions.html#likelihood-for-the-multi-nomial-logit-mnl-model",
    "title": "Multinomial Logit Model",
    "section": "1. Likelihood for the Multi-nomial Logit (MNL) Model",
    "text": "1. Likelihood for the Multi-nomial Logit (MNL) Model\nSuppose we have \\(i=1,\\ldots,n\\) consumers who each select exactly one product \\(j\\) from a set of \\(J\\) products. The outcome variable is the identity of the product chosen \\(y_i \\in \\{1, \\ldots, J\\}\\) or equivalently a vector of \\(J-1\\) zeros and \\(1\\) one, where the \\(1\\) indicates the selected product. For example, if the third product was chosen out of 3 products, then either \\(y=3\\) or \\(y=(0,0,1)\\) depending on how we want to represent it. Suppose also that we have a vector of data on each product \\(x_j\\) (eg, brand, price, etc.).\nWe model the consumer’s decision as the selection of the product that provides the most utility, and we’ll specify the utility function as a linear function of the product characteristics:\n\\[ U_{ij} = x_j'\\beta + \\epsilon_{ij} \\]\nwhere \\(\\epsilon_{ij}\\) is an i.i.d. extreme value error term.\nThe choice of the i.i.d. extreme value error term leads to a closed-form expression for the probability that consumer \\(i\\) chooses product \\(j\\):\n\\[ \\mathbb{P}_i(j) = \\frac{e^{x_j'\\beta}}{\\sum_{k=1}^Je^{x_k'\\beta}} \\]\nFor example, if there are 3 products, the probability that consumer \\(i\\) chooses product 3 is:\n\\[ \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{e^{x_1'\\beta} + e^{x_2'\\beta} + e^{x_3'\\beta}} \\]\nA clever way to write the individual likelihood function for consumer \\(i\\) is the product of the \\(J\\) probabilities, each raised to the power of an indicator variable (\\(\\delta_{ij}\\)) that indicates the chosen product:\n\\[ L_i(\\beta) = \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} = \\mathbb{P}_i(1)^{\\delta_{i1}} \\times \\ldots \\times \\mathbb{P}_i(J)^{\\delta_{iJ}}\\]\nNotice that if the consumer selected product \\(j=3\\), then \\(\\delta_{i3}=1\\) while \\(\\delta_{i1}=\\delta_{i2}=0\\) and the likelihood is:\n\\[ L_i(\\beta) = \\mathbb{P}_i(1)^0 \\times \\mathbb{P}_i(2)^0 \\times \\mathbb{P}_i(3)^1 = \\mathbb{P}_i(3) = \\frac{e^{x_3'\\beta}}{\\sum_{k=1}^3e^{x_k'\\beta}} \\]\nThe joint likelihood (across all consumers) is the product of the \\(n\\) individual likelihoods:\n\\[ L_n(\\beta) = \\prod_{i=1}^n L_i(\\beta) = \\prod_{i=1}^n \\prod_{j=1}^J \\mathbb{P}_i(j)^{\\delta_{ij}} \\]\nAnd the joint log-likelihood function is:\n\\[ \\ell_n(\\beta) = \\sum_{i=1}^n \\sum_{j=1}^J \\delta_{ij} \\log(\\mathbb{P}_i(j)) \\]"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#simulate-conjoint-data",
    "href": "blog/project3/hw3_questions.html#simulate-conjoint-data",
    "title": "Multinomial Logit Model",
    "section": "2. Simulate Conjoint Data",
    "text": "2. Simulate Conjoint Data\nWe will simulate data from a conjoint experiment about video content streaming services. We elect to simulate 100 respondents, each completing 10 choice tasks, where they choose from three alternatives per task. For simplicity, there is not a “no choice” option; each simulated respondent must select one of the 3 alternatives.\nEach alternative is a hypothetical streaming offer consistent of three attributes: (1) brand is either Netflix, Amazon Prime, or Hulu; (2) ads can either be part of the experience, or it can be ad-free, and (3) price per month ranges from $4 to $32 in increments of $4.\nThe part-worths (ie, preference weights or beta parameters) for the attribute levels will be 1.0 for Netflix, 0.5 for Amazon Prime (with 0 for Hulu as the reference brand); -0.8 for included adverstisements (0 for ad-free); and -0.1*price so that utility to consumer \\(i\\) for hypothethical streaming service \\(j\\) is\n\\[\nu_{ij} = (1 \\times Netflix_j) + (0.5 \\times Prime_j) + (-0.8*Ads_j) - 0.1\\times Price_j + \\varepsilon_{ij}\n\\]\nwhere the variables are binary indicators and \\(\\varepsilon\\) is Type 1 Extreme Value (ie, Gumble) distributed.\nThe following code provides the simulation of the conjoint data.\n\n\n\n\n\n\nNote\n\n\n\n\n\n\nimport pandas as pd\nimport numpy as np\n\n# Set seed for reproducibility\nnp.random.seed(123)\n\n# Define attributes\nbrand = [\"N\", \"P\", \"H\"]  # Netflix, Prime, Hulu\nad = [\"Yes\", \"No\"]\nprice = list(range(8, 33, 4))\n\n# Generate all possible profiles\nprofiles = pd.DataFrame(\n    [(b, a, p) for b in brand for a in ad for p in price],\n    columns=[\"brand\", \"ad\", \"price\"]\n)\nm = len(profiles)\n\n# Assign part-worth utilities (true parameters)\nb_util = {\"N\": 1.0, \"P\": 0.5, \"H\": 0.0}\na_util = {\"Yes\": -0.8, \"No\": 0.0}\np_util = lambda p: -0.1 * p\n\n# Number of respondents, choice tasks, and alternatives per task\nn_peeps = 100\nn_tasks = 10\nn_alts = 3\n\n# Function to simulate one respondent’s data\ndef sim_one(id):\n    datlist = []\n    \n    for t in range(1, n_tasks + 1):\n        sample_indices = np.random.choice(m, size=n_alts, replace=False)\n        dat = profiles.iloc[sample_indices].copy()\n        dat.insert(0, \"task\", t)\n        dat.insert(0, \"resp\", id)\n        \n        # Compute deterministic portion of utility\n        dat[\"v\"] = (\n            dat[\"brand\"].map(b_util) +\n            dat[\"ad\"].map(a_util) +\n            dat[\"price\"].apply(p_util)\n        ).round(10)\n        \n        # Add Gumbel noise (Type I extreme value)\n        dat[\"e\"] = -np.log(-np.log(np.random.uniform(size=n_alts)))\n        dat[\"u\"] = dat[\"v\"] + dat[\"e\"]\n        \n        # Identify chosen alternative\n        dat[\"choice\"] = (dat[\"u\"] == dat[\"u\"].max()).astype(int)\n        \n        datlist.append(dat)\n    \n    return pd.concat(datlist, ignore_index=True)\n\n# Simulate data for all respondents\nconjoint_data = pd.concat([sim_one(i) for i in range(1, n_peeps + 1)], ignore_index=True)\n\n# Remove values unobservable to the researcher\nconjoint_data = conjoint_data[[\"resp\", \"task\", \"brand\", \"ad\", \"price\", \"choice\"]]"
  },
  {
    "objectID": "blog/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "href": "blog/project3/hw3_questions.html#preparing-the-data-for-estimation",
    "title": "Multinomial Logit Model",
    "section": "3. Preparing the Data for Estimation",
    "text": "3. Preparing the Data for Estimation\nThe “hard part” of the MNL likelihood function is organizing the data, as we need to keep track of 3 dimensions (consumer \\(i\\), covariate \\(k\\), and product \\(j\\)) instead of the typical 2 dimensions for cross-sectional regression models (consumer \\(i\\) and covariate \\(k\\)). The fact that each task for each respondent has the same number of alternatives (3) helps. In addition, we need to convert the categorical variables for brand and ads into binary variables.\nWe now load the CSV, convert categorical variables into dummy variables, and prepare the data for modeling.\n\nimport pandas as pd\n\n# Load the data\nconjoint_data = pd.read_csv(\"conjoint_data.csv\")\n\n# Preview the raw data\nconjoint_data.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nbrand\nad\nprice\n\n\n\n\n0\n1\n1\n1\nN\nYes\n28\n\n\n1\n1\n1\n0\nH\nYes\n16\n\n\n2\n1\n1\n0\nP\nYes\n16\n\n\n3\n1\n2\n0\nN\nYes\n32\n\n\n4\n1\n2\n1\nP\nYes\n16\n\n\n\n\n\n\n\nOne-Hot Encode Brand and Ad Columns\n\n# One-hot encode brand and ad\ndata_encoded = pd.get_dummies(conjoint_data, columns=[\"brand\", \"ad\"], drop_first=True)\n\n# Rename columns for clarity (optional)\ndata_encoded.rename(columns={\n    \"brand_P\": \"Prime\",\n    \"brand_N\": \"Netflix\",\n    \"ad_Yes\": \"Ads\"\n}, inplace=True)\n\n# View cleaned data\ndata_encoded.head()\n\n\n\n\n\n\n\n\nresp\ntask\nchoice\nprice\nNetflix\nPrime\nAds\n\n\n\n\n0\n1\n1\n1\n28\nTrue\nFalse\nTrue\n\n\n1\n1\n1\n0\n16\nFalse\nFalse\nTrue\n\n\n2\n1\n1\n0\n16\nFalse\nTrue\nTrue\n\n\n3\n1\n2\n0\n32\nTrue\nFalse\nTrue\n\n\n4\n1\n2\n1\n16\nFalse\nTrue\nTrue\n\n\n\n\n\n\n\nCheck Shape and Unique Combinations (Optional Debug)\n\nimport pandas as pd\n# Confirm expected shape and unique groupings\nprint(f\"Data shape: {data_encoded.shape}\")\nprint(f\"Unique respondents: {data_encoded['resp'].nunique()}\")\nprint(f\"Unique tasks: {data_encoded['task'].nunique()}\")\n\nData shape: (3000, 7)\nUnique respondents: 100\nUnique tasks: 10\n\n\n\nDescription\nThe ad variable in the original conjoint_data CSV indicates whether the streaming service offering includes advertisements (\"Yes\") or is ad-free (\"No\"). This is a binary categorical feature and is one of the three key product attributes being varied across profiles: brand, ads, and price.\nIn the data preparation step, we use pd.get_dummies(..., drop_first=True) to convert ad into a dummy variable: - ad_Yes becomes Ads after renaming, - A value of True in the Ads column means the alternative includes ads, - A value of False implies the alternative is ad-free (the reference category).\n\n\nInterpretation in the Model\nThis binary indicator enters the utility function for each alternative. Based on how the data was simulated, the true utility function included a negative coefficient for ads:\n[ u_{ij} = - 0.8 _{j} + ]\nThis implies that: - Consumers prefer ad-free options, all else being equal. - The presence of ads reduces utility by 0.8 units, a substantial effect compared to the coefficients on brand or price.\nIn the estimated models (MLE and Bayesian), we expect the coefficient on Ads to be negative, validating this aversion to ads. The magnitude of the coefficient will reflect how strong this aversion is relative to price or brand loyalty.\n\n\nWhy It Matters\nIn a real-world context, this variable helps answer business-critical questions such as: - “How much more do consumers value an ad-free experience?” - “Would a $2 increase in price be tolerated if ads are removed?” - “Which segment of consumers is more tolerant of ads?”\nWhen extended to a hierarchical model, we could explore individual-level heterogeneity — for example, budget-conscious consumers might be more tolerant of ads if the price is lower.\n\n\nFinal Notes\nEncoding binary variables like ad correctly is crucial in discrete choice modeling: - One-hot encoding allows us to model preferences without falsely assuming numerical relationships. - Choosing a reference level (here, ad-free) defines the baseline utility and interpretation of coefficients.\nBy treating ads as a structured, binary attribute in our experiment, we can quantify consumer trade-offs and inform feature-bundling, pricing, and personalization strategies."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "href": "blog/project3/hw3_questions.html#estimation-via-maximum-likelihood",
    "title": "Multinomial Logit Model",
    "section": "4. Estimation via Maximum Likelihood",
    "text": "4. Estimation via Maximum Likelihood\nWe now estimate the MNL model using Maximum Likelihood Estimation (MLE).\n\nimport numpy as np\nimport pandas as pd\nfrom scipy.optimize import minimize\n\n# Ensure the input matrix is numeric\nX_cols = [\"Netflix\", \"Prime\", \"Ads\", \"price\"]\nX = data_encoded[X_cols].astype(float).values\ny = data_encoded[\"choice\"].values\n\n# Identify each unique choice set (i.e., a respondent-task combo)\ngroups = data_encoded.groupby([\"resp\", \"task\"]).ngroup().values\nn_groups = groups.max() + 1\nn_alts = 3  # 3 alternatives per choice task\n\nDefine the Log-Likelihood Function\n\ndef neg_log_likelihood(beta):\n    # Compute utilities\n    utilities = X @ beta  # linear combination\n    utilities = utilities.reshape((n_groups, n_alts))\n\n    # Compute probabilities\n    exp_u = np.exp(utilities)\n    denom = exp_u.sum(axis=1).reshape(-1, 1)\n    probs = exp_u / denom\n\n    # Reshape observed choices\n    chosen = y.reshape((n_groups, n_alts))\n\n    # Only keep probability of chosen alternative\n    chosen_probs = (probs * chosen).sum(axis=1)\n\n    # Return negative log-likelihood\n    return -np.sum(np.log(chosen_probs))\n\nEstimate Parameters\n\n# Initial guess for parameters\nbeta_init = np.zeros(X.shape[1])\n\n# Minimize negative log-likelihood\nresult = minimize(neg_log_likelihood, beta_init, method='BFGS')\n\n# Estimated coefficients and standard errors\nbeta_hat = result.x\nhessian_inv = result.hess_inv\nstd_errors = np.sqrt(np.diag(hessian_inv))\n\n# 95% Confidence intervals\nconf_int = np.vstack([\n    beta_hat - 1.96 * std_errors,\n    beta_hat + 1.96 * std_errors\n]).T\n\n# Create summary table\nparam_summary = pd.DataFrame({\n    \"Parameter\": X_cols,\n    \"Estimate\": beta_hat,\n    \"Std. Error\": std_errors,\n    \"CI Lower\": conf_int[:, 0],\n    \"CI Upper\": conf_int[:, 1]\n})\n\nparam_summary\n\n\n\n\n\n\n\n\nParameter\nEstimate\nStd. Error\nCI Lower\nCI Upper\n\n\n\n\n0\nNetflix\n0.941195\n0.114679\n0.716425\n1.165965\n\n\n1\nPrime\n0.501616\n0.120757\n0.264932\n0.738299\n\n\n2\nAds\n-0.731994\n0.088476\n-0.905408\n-0.558580\n\n\n3\nprice\n-0.099480\n0.006357\n-0.111940\n-0.087021\n\n\n\n\n\n\n\n####Objective\nIn this section, we apply Maximum Likelihood Estimation (MLE) to estimate the part-worth utilities (i.e., β coefficients) in a Multinomial Logit (MNL) model. Each β represents the marginal utility of an attribute (e.g., brand, price, ads) and is estimated by maximizing the likelihood of the observed choices made by consumers in the simulated conjoint dataset.\n\nModel Structure\nThe MNL model assumes that each respondent chooses the alternative with the highest utility from a set of 3 options. The utility for respondent ( i ), task ( t ), and alternative ( j ) is modeled as:\n[ U_{ijt} = X_{ijt} + _{ijt} ]\n\n( X_{ijt} ) is a vector of features (brand, ads, price),\n( ) is a vector of fixed coefficients,\n( _{ijt} (0, 1) ), implying the choice probabilities follow a softmax distribution.\n\nThe log-likelihood is computed as the sum of the log probabilities of the chosen alternatives, and we use scipy.optimize.minimize with the BFGS algorithm to find the β vector that maximizes it.\n\n\nEstimation Results\n\n\n\nParameter\nEstimate\nStd. Error\n95% CI Lower\n95% CI Upper\n\n\n\n\nNetflix\n0.9412\n0.1147\n0.7164\n1.1697\n\n\nPrime\n0.5016\n0.1208\n0.2647\n0.7383\n\n\nAds\n-0.7320\n0.0885\n-0.9054\n-0.5586\n\n\nPrice\n-0.0999\n0.0064\n-0.1119\n-0.0870\n\n\n\n\n\nInterpretation\n\nBrand effects:\n\nβ_Netflix ≈ 0.94: Respondents derive substantial additional utility from Netflix over the baseline (Hulu).\nβ_Prime ≈ 0.50: Prime is also positively valued, but less so than Netflix.\nInterpretation: All else equal, Netflix is more likely to be chosen than Prime, and both are preferred over Hulu.\n\nAd effect:\n\nβ_Ads ≈ -0.73: The presence of ads significantly reduces utility, confirming that consumers strongly prefer ad-free content.\nThe tight confidence interval (−0.905 to −0.558) confirms high certainty about this aversion.\n\nPrice effect:\n\nβ_Price ≈ -0.10: As expected, higher prices reduce utility. For every $1 increase in price, the utility drops by ~0.10 units.\nInterpretation: A consumer would tolerate about a $7.3 price increase to remove ads, holding other factors constant (( 0.73 / 0.10 )).\n\n\n\n\nModel Fit and Validity\nWhile we don’t compute formal goodness-of-fit measures here (e.g., log-likelihood at convergence, McFadden’s R²), several signs point to a well-performing model:\n\nThe coefficients have expected signs and plausible magnitudes,\nStandard errors are small, leading to narrow confidence intervals,\nThe model aligns with economic theory and consumer behavior literature.\n\n\n\nPractical Implications\nThis MLE model enables firms to: - Quantify trade-offs consumers are willing to make (e.g., how much more they’d pay for no ads), - Forecast market shares by plugging in attribute levels for new offerings, - Design optimal product bundles (e.g., price tiering based on ad presence), - Perform What-if scenario simulations: “What happens if we increase Netflix’s price by $2?”\n\n\nLimitations\n\nHomogeneous preferences: The MNL model assumes all consumers share the same β. This masks preference heterogeneity and may limit personalization or segmentation.\nIID error assumption: The Gumbel-distributed error terms assume independence across alternatives, which may not hold in nested or correlated choice sets (e.g., two similar streaming services).\n\nThese limitations can be addressed with a random coefficients (mixed logit) model, as discussed in later sections.\n\n\nSummary\nThe MLE estimation has successfully recovered the true structure of simulated consumer preferences:\n\nConsumers strongly prefer Netflix &gt; Prime &gt; Hulu,\nThey dislike ads and prefer lower prices,\nThe model provides interpretable and actionable utility weights that can drive strategic decision-making.\n\nWe now turn to Bayesian methods to compare posterior distributions with these point estimates and better capture uncertainty and individual variation."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "href": "blog/project3/hw3_questions.html#estimation-via-bayesian-methods",
    "title": "Multinomial Logit Model",
    "section": "5. Estimation via Bayesian Methods",
    "text": "5. Estimation via Bayesian Methods\nWe implement Metropolis-Hastings MCMC to sample from the posterior distribution of the 4 beta coefficients.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Log-prior function\ndef log_prior(beta):\n    # Priors: N(0, 5^2) for binary predictors, N(0, 1^2) for price\n    var = np.array([25.0, 25.0, 25.0, 1.0])\n    return -0.5 * np.sum((beta**2) / var) - 0.5 * np.sum(np.log(2 * np.pi * var))\n\n# Log-posterior: log-likelihood + log-prior\ndef log_posterior(beta):\n    return -neg_log_likelihood(beta) + log_prior(beta)\n\nMetropolis-Hastings MCMC Sampler\n\nn_iter = 11000\nburn_in = 1000\nn_params = X.shape[1]\n\n# Starting values\ncurrent = np.zeros(n_params)\nsamples = np.zeros((n_iter, n_params))\naccepts = 0\n\n# Proposal SDs: N(0, 0.05) for first 3, N(0, 0.005) for price\nproposal_sds = np.array([0.05, 0.05, 0.05, 0.005])\n\n# Sampling loop\nfor i in range(n_iter):\n    proposal = current + np.random.normal(0, proposal_sds)\n    log_post_current = log_posterior(current)\n    log_post_proposal = log_posterior(proposal)\n    \n    accept_ratio = np.exp(log_post_proposal - log_post_current)\n    \n    if np.random.rand() &lt; accept_ratio:\n        current = proposal\n        accepts += 1\n    \n    samples[i, :] = current\n\naccept_rate = accepts / n_iter\nprint(f\"Acceptance rate: {accept_rate:.3f}\")\n\nAcceptance rate: 0.574\n\n\nTrace Plot and Posterior Histogram\n\nposterior = samples[burn_in:, :]\n\nparam_names = [\"Netflix\", \"Prime\", \"Ads\", \"price\"]\n\n# Trace + histogram for one parameter (e.g., Netflix)\nfig, ax = plt.subplots(1, 2, figsize=(12, 4))\nax[0].plot(posterior[:, 0])\nax[0].set_title(\"Trace plot: Netflix\")\nax[1].hist(posterior[:, 0], bins=30, density=True)\nax[1].set_title(\"Posterior: Netflix\")\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nPosterior Summary Statistics\n\nposterior_means = posterior.mean(axis=0)\nposterior_std = posterior.std(axis=0)\ncredible_intervals = np.percentile(posterior, [2.5, 97.5], axis=0).T\n\nposterior_summary = pd.DataFrame({\n    \"Parameter\": param_names,\n    \"Posterior Mean\": posterior_means,\n    \"Posterior Std. Dev.\": posterior_std,\n    \"CI Lower\": credible_intervals[:, 0],\n    \"CI Upper\": credible_intervals[:, 1]\n})\n\nposterior_summary\n\n\n\n\n\n\n\n\nParameter\nPosterior Mean\nPosterior Std. Dev.\nCI Lower\nCI Upper\n\n\n\n\n0\nNetflix\n0.930949\n0.108311\n0.725842\n1.157317\n\n\n1\nPrime\n0.488224\n0.106468\n0.280797\n0.702842\n\n\n2\nAds\n-0.723895\n0.091051\n-0.898098\n-0.538874\n\n\n3\nprice\n-0.099514\n0.006269\n-0.112154\n-0.087608\n\n\n\n\n\n\n\nIn this section, we estimate the parameters of the Multinomial Logit (MNL) model using Bayesian inference via the Metropolis-Hastings MCMC algorithm. Unlike Maximum Likelihood Estimation (MLE), which provides only point estimates and standard errors, the Bayesian approach yields full posterior distributions for each parameter. These distributions combine prior beliefs with evidence from the data, giving us a more complete understanding of parameter uncertainty.\nWe retain the same utility function as in the MLE setup: ( U_{ijt} = X_{ijt} + {ijt} ), where the error term is i.i.d. Gumbel-distributed. We specify priors as follows: ( {}, {}, {} (0, 25) ), and ( _{} (0, 1) ). These priors are centered at zero but relatively diffuse, allowing the data to drive inference while still regularizing the estimates.\nWe implemented a Metropolis-Hastings sampler with 11,000 iterations, discarding the first 1,000 as burn-in, leaving 10,000 posterior draws. The proposal distribution was independent normal for each parameter: ( (0, 0.05^2) ) for Netflix, Prime, and Ads, and ( (0, 0.005^2) ) for Price, reflecting its smaller expected scale. The acceptance rate was 0.574, which is within the ideal range (0.2–0.6), suggesting good mixing and a well-tuned sampler.\nWe assessed convergence visually using trace plots and posterior histograms. For example, the trace plot for the β coefficient on Netflix shows stable fluctuation without drift, indicating convergence. The corresponding histogram is smooth and unimodal, supporting the reliability of the posterior.\nThe posterior means and 95% credible intervals closely align with the MLE results. For instance, β_Netflix had a posterior mean of 0.931 (vs. 0.941 MLE), β_Prime 0.488 (vs. 0.502), β_Ads −0.724 (vs. −0.732), and β_Price −0.0996 (vs. −0.0999). Standard deviations were small, and none of the credible intervals overlapped zero, confirming that all four coefficients are statistically significant.\nThese results have intuitive interpretations. The strong positive coefficients for Netflix and Prime (relative to the base category Hulu) indicate consumer brand preference. The negative coefficient on Ads shows that consumers experience disutility from advertisements, and the negative Price coefficient confirms price sensitivity. The Bayesian results validate the MLE findings but add richer detail by quantifying our uncertainty over a full posterior distribution rather than a single estimate.\nOne of the key strengths of the Bayesian framework is its extensibility. For instance, it enables us to transition naturally into a hierarchical (random-effects) model, where each respondent has their own set of β coefficients drawn from a population-level distribution. This is especially important in real-world conjoint analysis, where we expect heterogeneous preferences across individuals.\nIn summary, Bayesian estimation via Metropolis-Hastings successfully recovers the true structure of preferences while providing full distributional insight. The results confirm that consumers prefer Netflix over Prime, dislike advertisements, and are price-sensitive. This approach offers both robustness and flexibility, and sets the stage for modeling more complex preference structures in future work."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#discussion",
    "href": "blog/project3/hw3_questions.html#discussion",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nSuppose we had not simulated the data ourselves and instead received this conjoint dataset from a real-world consumer study on streaming service preferences. Based on the results from both Maximum Likelihood Estimation (MLE) and Bayesian MCMC, we observe consistent patterns in the estimated coefficients: Netflix is more preferred than Prime (β_Netflix &gt; β_Prime &gt; 0), advertisements reduce utility (β_Ads &lt; 0), and price sensitivity is negative (β_price &lt; 0).\nThe finding that β_Netflix is greater than β_Prime suggests that, on average, consumers derive more utility from a streaming service branded as Netflix than from one branded as Amazon Prime, even when ad presence and price are held constant. This highlights that brand identity alone contributes meaningfully to perceived value. Such differences likely stem from factors like content library quality, brand familiarity, and user interface design. The fact that both coefficients are positive but different in magnitude indicates that Prime is valued by consumers, just less so than Netflix. This pattern reflects measurable and quantifiable brand preference in the MNL framework.\nThe negative coefficient on price is both expected and consistent with rational economic behavior. A negative β_price means that, holding all else equal, higher subscription costs lower a service’s utility, making consumers less likely to choose it. This aligns with foundational assumptions in discrete choice models: consumers prefer lower prices. Likewise, the significantly negative coefficient on Ads (β_Ads) indicates that users prefer ad-free experiences. The more negative this value, the greater the utility loss associated with ads — which also implies that consumers may be willing to pay more to avoid them.\nWhile the MNL model provides useful insights, it assumes that all individuals share the same set of preferences, that is, one global β vector. In reality, this assumption is overly simplistic. Consumers differ widely in their sensitivity to price, their tolerance for ads, and their brand preferences, often influenced by demographics, habits, or prior experiences.\nTo model this heterogeneity, we can extend to a random coefficients logit model, also known as a mixed logit or hierarchical Bayes model. In this framework, each respondent ( i ) has their own vector of coefficients ( _i ), which is assumed to be drawn from a population distribution ( _i (, ) ). Here, ( ) captures the average preference across the population, and ( ) captures the variance and potential correlations in preferences across individuals.\nSimulating from such a model would involve first drawing a unique β vector for each respondent and then simulating their choices based on their personalized utility function. This introduces variation that mirrors real consumer behavior more closely. Estimating this model can be done using Bayesian MCMC techniques, where both the individual-level betas and population-level parameters are inferred simultaneously, or through empirical Bayes or maximum simulated likelihood methods, which integrate over the assumed population distribution.\nThe benefit of this hierarchical approach is that it supports more granular analyses. It enables personalized predictions, such as forecasting what a specific user would choose given their estimated β. It also enables more accurate market segmentation, where users with similar preferences can be grouped and targeted effectively. Finally, it improves policy simulation and pricing strategy, providing a more nuanced understanding of willingness-to-pay across different segments of the population.\nIn summary, while the basic MNL model delivers valuable population-level insights, extending it to a hierarchical form allows us to better reflect and leverage the diversity in real-world preferences. This makes it a more realistic and powerful tool for analyzing conjoint data in practice."
  },
  {
    "objectID": "blog/project3/hw3_questions.html#discussion-1",
    "href": "blog/project3/hw3_questions.html#discussion-1",
    "title": "Multinomial Logit Model",
    "section": "6. Discussion",
    "text": "6. Discussion\nSuppose we had not simulated the data ourselves and instead received this conjoint dataset from a real-world consumer study on streaming service preferences. Based on the results from both Maximum Likelihood Estimation (MLE) and Bayesian MCMC, we observe consistent patterns in the estimated coefficients: Netflix is more preferred than Prime (β_Netflix &gt; β_Prime &gt; 0), advertisements reduce utility (β_Ads &lt; 0), and price sensitivity is negative (β_price &lt; 0).\nThe finding that β_Netflix is greater than β_Prime suggests that, on average, consumers derive more utility from a streaming service branded as Netflix than from one branded as Amazon Prime, even when ad presence and price are held constant. This highlights that brand identity alone contributes meaningfully to perceived value. Such differences likely stem from factors like content library quality, brand familiarity, and user interface design. The fact that both coefficients are positive but different in magnitude indicates that Prime is valued by consumers, just less so than Netflix. This pattern reflects measurable and quantifiable brand preference in the MNL framework.\nThe negative coefficient on price is both expected and consistent with rational economic behavior. A negative β_price means that, holding all else equal, higher subscription costs lower a service’s utility, making consumers less likely to choose it. This aligns with foundational assumptions in discrete choice models: consumers prefer lower prices. Likewise, the significantly negative coefficient on Ads (β_Ads) indicates that users prefer ad-free experiences. The more negative this value, the greater the utility loss associated with ads — which also implies that consumers may be willing to pay more to avoid them.\nWhile the MNL model provides useful insights, it assumes that all individuals share the same set of preferences, that is, one global β vector. In reality, this assumption is overly simplistic. Consumers differ widely in their sensitivity to price, their tolerance for ads, and their brand preferences, often influenced by demographics, habits, or prior experiences.\nTo model this heterogeneity, we can extend to a random coefficients logit model, also known as a mixed logit or hierarchical Bayes model. In this framework, each respondent ( i ) has their own vector of coefficients ( _i ), which is assumed to be drawn from a population distribution ( _i (, ) ). Here, ( ) captures the average preference across the population, and ( ) captures the variance and potential correlations in preferences across individuals.\nSimulating from such a model would involve first drawing a unique β vector for each respondent and then simulating their choices based on their personalized utility function. This introduces variation that mirrors real consumer behavior more closely. Estimating this model can be done using Bayesian MCMC techniques, where both the individual-level betas and population-level parameters are inferred simultaneously, or through empirical Bayes or maximum simulated likelihood methods, which integrate over the assumed population distribution.\nThe benefit of this hierarchical approach is that it supports more granular analyses. It enables personalized predictions, such as forecasting what a specific user would choose given their estimated β. It also enables more accurate market segmentation, where users with similar preferences can be grouped and targeted effectively. Finally, it improves policy simulation and pricing strategy, providing a more nuanced understanding of willingness-to-pay across different segments of the population.\nIn summary, while the basic MNL model delivers valuable population-level insights, extending it to a hierarchical form allows us to better reflect and leverage the diversity in real-world preferences. This makes it a more realistic and powerful tool for analyzing conjoint data in practice."
  },
  {
    "objectID": "blog/project4/hw4_questions.html",
    "href": "blog/project4/hw4_questions.html",
    "title": "Machine Learning",
    "section": "",
    "text": "In this section, I applied the K-Means clustering algorithm to the Palmer Penguins dataset, focusing on two continuous features: bill_length_mm and flipper_length_mm. I implemented the algorithm from scratch in Python and evaluated it across multiple values of k to investigate clustering structure and performance.\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load penguin data and select relevant features\npenguins = pd.read_csv(\"palmer_penguins.csv\")\npenguins = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\n\n# Standardize the data\nscaler = StandardScaler()\nX = scaler.fit_transform(penguins)\n\n\nimport numpy as np\n\ndef initialize_centroids(X, k):\n    np.random.seed(42)\n    indices = np.random.choice(X.shape[0], size=k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans_custom(X, k, max_iters=100):\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n\nimport matplotlib.pyplot as plt\n\nk_range = range(2, 8)\nfig, axs = plt.subplots(2, 3, figsize=(18, 10))\naxs = axs.flatten()\n\nfor i, k in enumerate(k_range):\n    labels, centroids = kmeans_custom(X, k)\n    axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30)\n    axs[i].scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)\n    axs[i].set_title(f'Custom K-Means: k={k}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsil_scores = []\n\nfor k in k_range:\n    model = KMeans(n_clusters=k, random_state=42)\n    model.fit(X)\n    wcss.append(model.inertia_)\n    sil_scores.append(silhouette_score(X, model.labels_))\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(list(k_range), wcss, marker='o', color='orange')\nplt.title(\"WCSS vs Number of Clusters\")\nplt.xlabel(\"k\")\nplt.ylabel(\"WCSS\")\n\nplt.subplot(1, 2, 2)\nplt.plot(list(k_range), sil_scores, marker='o', color='orange')\nplt.title(\"Silhouette Score vs Number of Clusters\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Silhouette Score\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe custom K-Means implementation followed the standard algorithmic steps: initializing cluster centroids randomly, assigning each data point to the nearest centroid, recalculating centroids as the mean of all points in a cluster, and repeating the process until the centroids stabilized. This process was performed for several values of k, ranging from 2 to 7, to examine how clustering structure varied with the number of clusters. Throughout the implementation, the algorithm successfully converged in a reasonable number of iterations and demonstrated expected behavior.\n\n\n\nTo evaluate the clustering quality and help determine the optimal number of clusters, I calculated two metrics: Within-Cluster Sum of Squares (WCSS) and Silhouette Score. WCSS measures cluster compactness, with lower values indicating tighter grouping. Silhouette Score captures how well-separated clusters are, with higher values reflecting clearer separation. These metrics were computed using the results from the built-in KMeans function to ensure consistency and enable direct comparison with the custom implementation.\nThe clustering results from the custom algorithm were visually and numerically comparable to those from sklearn.KMeans. The shapes and positions of clusters, as well as the centroids, aligned closely between the two implementations. This consistency validated the accuracy of the custom method. The primary advantage of the built-in method lies in its use of the k-means++ initialization technique, which improves stability and performance, but the final clustering patterns were essentially the same in both cases.\n\n\n\nThe WCSS curve showed a steep decline from k = 2 to k = 3, followed by a more gradual decrease, suggesting an “elbow” at k = 3 or k = 4. This elbow point indicates a diminishing return on increasing the number of clusters beyond three or four. In contrast, the Silhouette Score was highest at k = 2, indicating that two clusters provided the clearest separation among groups. However, as more clusters were added, the scores declined slightly, suggesting increasing overlap or less distinct clustering.\nBalancing these insights, the most reasonable choice for the number of clusters is k = 3. While k = 2 provides the highest Silhouette Score, it may oversimplify the data and miss meaningful subgroup structures. Meanwhile, k = 3 captures more nuance in the data while still maintaining relatively compact and well-separated clusters, as evidenced by both visual plots and metric values.\n\n\n\nThis assignment demonstrated the full implementation and evaluation of K-Means clustering, reinforcing the importance of using both visualizations and metrics to determine the best number of clusters. The custom implementation aligned closely with the built-in version, confirming its correctness. Ultimately, k = 3 emerged as the best compromise between compactness and interpretability, providing valuable insights into the structure of the penguin dataset."
  },
  {
    "objectID": "blog/project4/hw4_questions.html#a.-k-means",
    "href": "blog/project4/hw4_questions.html#a.-k-means",
    "title": "Machine Learning",
    "section": "",
    "text": "In this section, I applied the K-Means clustering algorithm to the Palmer Penguins dataset, focusing on two continuous features: bill_length_mm and flipper_length_mm. I implemented the algorithm from scratch in Python and evaluated it across multiple values of k to investigate clustering structure and performance.\n\nimport pandas as pd\nfrom sklearn.preprocessing import StandardScaler\n\n# Load penguin data and select relevant features\npenguins = pd.read_csv(\"palmer_penguins.csv\")\npenguins = penguins[['bill_length_mm', 'flipper_length_mm']].dropna()\n\n# Standardize the data\nscaler = StandardScaler()\nX = scaler.fit_transform(penguins)\n\n\nimport numpy as np\n\ndef initialize_centroids(X, k):\n    np.random.seed(42)\n    indices = np.random.choice(X.shape[0], size=k, replace=False)\n    return X[indices]\n\ndef assign_clusters(X, centroids):\n    distances = np.linalg.norm(X[:, np.newaxis] - centroids, axis=2)\n    return np.argmin(distances, axis=1)\n\ndef update_centroids(X, labels, k):\n    return np.array([X[labels == i].mean(axis=0) for i in range(k)])\n\ndef kmeans_custom(X, k, max_iters=100):\n    centroids = initialize_centroids(X, k)\n    for _ in range(max_iters):\n        labels = assign_clusters(X, centroids)\n        new_centroids = update_centroids(X, labels, k)\n        if np.allclose(centroids, new_centroids):\n            break\n        centroids = new_centroids\n    return labels, centroids\n\n\nimport matplotlib.pyplot as plt\n\nk_range = range(2, 8)\nfig, axs = plt.subplots(2, 3, figsize=(18, 10))\naxs = axs.flatten()\n\nfor i, k in enumerate(k_range):\n    labels, centroids = kmeans_custom(X, k)\n    axs[i].scatter(X[:, 0], X[:, 1], c=labels, cmap='tab10', s=30)\n    axs[i].scatter(centroids[:, 0], centroids[:, 1], c='black', marker='x', s=100)\n    axs[i].set_title(f'Custom K-Means: k={k}')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import silhouette_score\n\nwcss = []\nsil_scores = []\n\nfor k in k_range:\n    model = KMeans(n_clusters=k, random_state=42)\n    model.fit(X)\n    wcss.append(model.inertia_)\n    sil_scores.append(silhouette_score(X, model.labels_))\n\n\nplt.figure(figsize=(12, 5))\n\nplt.subplot(1, 2, 1)\nplt.plot(list(k_range), wcss, marker='o', color='orange')\nplt.title(\"WCSS vs Number of Clusters\")\nplt.xlabel(\"k\")\nplt.ylabel(\"WCSS\")\n\nplt.subplot(1, 2, 2)\nplt.plot(list(k_range), sil_scores, marker='o', color='orange')\nplt.title(\"Silhouette Score vs Number of Clusters\")\nplt.xlabel(\"k\")\nplt.ylabel(\"Silhouette Score\")\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\n\n\nThe custom K-Means implementation followed the standard algorithmic steps: initializing cluster centroids randomly, assigning each data point to the nearest centroid, recalculating centroids as the mean of all points in a cluster, and repeating the process until the centroids stabilized. This process was performed for several values of k, ranging from 2 to 7, to examine how clustering structure varied with the number of clusters. Throughout the implementation, the algorithm successfully converged in a reasonable number of iterations and demonstrated expected behavior.\n\n\n\nTo evaluate the clustering quality and help determine the optimal number of clusters, I calculated two metrics: Within-Cluster Sum of Squares (WCSS) and Silhouette Score. WCSS measures cluster compactness, with lower values indicating tighter grouping. Silhouette Score captures how well-separated clusters are, with higher values reflecting clearer separation. These metrics were computed using the results from the built-in KMeans function to ensure consistency and enable direct comparison with the custom implementation.\nThe clustering results from the custom algorithm were visually and numerically comparable to those from sklearn.KMeans. The shapes and positions of clusters, as well as the centroids, aligned closely between the two implementations. This consistency validated the accuracy of the custom method. The primary advantage of the built-in method lies in its use of the k-means++ initialization technique, which improves stability and performance, but the final clustering patterns were essentially the same in both cases.\n\n\n\nThe WCSS curve showed a steep decline from k = 2 to k = 3, followed by a more gradual decrease, suggesting an “elbow” at k = 3 or k = 4. This elbow point indicates a diminishing return on increasing the number of clusters beyond three or four. In contrast, the Silhouette Score was highest at k = 2, indicating that two clusters provided the clearest separation among groups. However, as more clusters were added, the scores declined slightly, suggesting increasing overlap or less distinct clustering.\nBalancing these insights, the most reasonable choice for the number of clusters is k = 3. While k = 2 provides the highest Silhouette Score, it may oversimplify the data and miss meaningful subgroup structures. Meanwhile, k = 3 captures more nuance in the data while still maintaining relatively compact and well-separated clusters, as evidenced by both visual plots and metric values.\n\n\n\nThis assignment demonstrated the full implementation and evaluation of K-Means clustering, reinforcing the importance of using both visualizations and metrics to determine the best number of clusters. The custom implementation aligned closely with the built-in version, confirming its correctness. Ultimately, k = 3 emerged as the best compromise between compactness and interpretability, providing valuable insights into the structure of the penguin dataset."
  },
  {
    "objectID": "blog/project4/hw4_questions.html#b.-latent-class-mnl",
    "href": "blog/project4/hw4_questions.html#b.-latent-class-mnl",
    "title": "Machine Learning",
    "section": "1b. Latent-Class MNL",
    "text": "1b. Latent-Class MNL\ntodo: Use the Yogurt dataset to estimate a latent-class MNL model. This model was formally introduced in the paper by Kamakura & Russell (1989); you may want to read or reference page 2 of the pdf, which is described in the class slides, session 4, slides 56-57.\nThe data provides anonymized consumer identifiers (id), a vector indicating the chosen product (y1:y4), a vector indicating if any products were “featured” in the store as a form of advertising (f1:f4), and the products’ prices in price-per-ounce (p1:p4). For example, consumer 1 purchased yogurt 4 at a price of 0.079/oz and none of the yogurts were featured/advertised at the time of consumer 1’s purchase. Consumers 2 through 7 each bought yogurt 2, etc. You may want to reshape the data from its current “wide” format into a “long” format.\ntodo: Fit the standard MNL model on these data. Then fit the latent-class MNL on these data separately for 2, 3, 4, and 5 latent classes.\ntodo: How many classes are suggested by the \\(BIC = -2*\\ell_n  + k*log(n)\\)? (where \\(\\ell_n\\) is the log-likelihood, \\(n\\) is the sample size, and \\(k\\) is the number of parameters.) The Bayesian-Schwarz Information Criterion link is a metric that assess the benefit of a better log likelihood at the expense of additional parameters to estimate – akin to the adjusted R-squared for the linear regression model. Note, that a lower BIC indicates a better model fit, accounting for the number of parameters in the model.\ntodo: compare the parameter estimates between (1) the aggregate MNL, and (2) the latent-class MNL with the number of classes suggested by the BIC."
  },
  {
    "objectID": "blog/project4/hw4_questions.html#a.-k-nearest-neighbors",
    "href": "blog/project4/hw4_questions.html#a.-k-nearest-neighbors",
    "title": "Machine Learning",
    "section": "2a. K Nearest Neighbors",
    "text": "2a. K Nearest Neighbors\n\nimport numpy as np\nimport pandas as pd\n\nnp.random.seed(42)\nn = 100\nx1 = np.random.uniform(-3, 3, n)\nx2 = np.random.uniform(-3, 3, n)\nX_train = np.column_stack((x1, x2))\nboundary = np.sin(4 * x1) + x1\ny_train = np.where(x2 &gt; boundary, 1, 0)\ny_train = pd.Categorical(y_train)\n\n\nTraining Data with the Wiggly Boundary\n\nimport matplotlib.pyplot as plt\n\nplt.figure(figsize=(8, 6))\nplt.scatter(x1, x2, c=y_train.codes, cmap='coolwarm', edgecolor='k')\nplt.plot(np.sort(x1), np.sin(4 * np.sort(x1)) + np.sort(x1), linestyle='--', color='black', label='Boundary')\nplt.xlabel(\"x1\")\nplt.ylabel(\"x2\")\nplt.title(\"Training Data with Wiggly Boundary\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\n\n\nTest Dataset with a Different Seed\n\nnp.random.seed(99)\nx1_test = np.random.uniform(-3, 3, n)\nx2_test = np.random.uniform(-3, 3, n)\nX_test = np.column_stack((x1_test, x2_test))\nboundary_test = np.sin(4 * x1_test) + x1_test\ny_test = np.where(x2_test &gt; boundary_test, 1, 0)\ny_test = pd.Categorical(y_test)\n\n\n\nKNN Evaluate Accuracy from k = 1 to 30\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\n\naccuracies = []\naccuracy_sklearn = []\n\nfor k in range(1, 31):\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train.codes)\n    preds = knn.predict(X_test)\n    acc = accuracy_score(y_test.codes, preds)\n    accuracies.append(acc)\n\n    model = KNeighborsClassifier(n_neighbors=k)\n    model.fit(X_train, y_train)\n    y_pred_builtin = model.predict(X_test)\n    acc_builtin = accuracy_score(y_test, y_pred_builtin)\n    accuracy_sklearn.append(acc_builtin)\n\n\n\nAccuracy Results\n\nplt.figure(figsize=(10, 5))\nplt.plot(range(1, 31), accuracies, label='Custom KNN', marker='o')\nplt.plot(range(1, 31), accuracy_sklearn, label='Sklearn KNN', marker='x')\nplt.xlabel('Number of Neighbors (k)')\nplt.ylabel('Accuracy on Test Set')\nplt.title('KNN Accuracy vs. k')\nplt.legend()\nplt.grid(True)\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nTo explore the K-Nearest Neighbors (KNN) classification algorithm, I generated a synthetic dataset with two features, x1 and x2, and a binary outcome variable y. The target variable was determined by whether x2 was above or below a non-linear boundary defined by the function y = sin(4x1) + x1. This setup creates a wiggly decision boundary, ideal for testing the flexibility and local sensitivity of KNN classifiers. I used NumPy to simulate 100 data points for training, each with x1 and x2 values sampled uniformly from the range [-3, 3].\nThe data was visualized with x1 on the horizontal axis and x2 on the vertical axis, where points were colored by class label. I overlaid the true boundary using the equation sin(4x1) + x1, which clearly separated the red and blue points. This plot served as a useful diagnostic tool to visually assess the complexity of the boundary KNN must learn.\nTo evaluate classification performance, I generated a test dataset of 100 new points using the same logic but with a different random seed to ensure separation between training and evaluation. The model was trained using scikit-learn’s KNeighborsClassifier, and its accuracy was tested on the new data across values of k from 1 to 30. For each k, I recorded the classification accuracy and visualized the results in a line plot.\nThe plot of accuracy versus k shows that performance is highest at small values of k, peaking at k = 1 and k = 2 with an accuracy of approximately 92%. As k increases, accuracy gradually declines, with slight fluctuations, and stabilizes around 88% for k values beyond 20. This trend is expected; lower k values allow the model to capture fine-grained, local decision boundaries but may overfit, while higher k values generalize more but can miss non-linear patterns like those in this dataset.\nBased on the plot, the optimal value of k is likely 1 or 2, as these values yield the highest classification accuracy on the test set. However, for more robust generalization, a slightly higher value of k, such as 5, might be preferred in real-world settings to mitigate the risk of overfitting. This exercise demonstrated how KNN can adapt to complex decision surfaces and how model performance is closely tied to the choice of k."
  },
  {
    "objectID": "blog/project4/hw4_questions.html#b.-key-drivers-analysis",
    "href": "blog/project4/hw4_questions.html#b.-key-drivers-analysis",
    "title": "Machine Learning",
    "section": "2b. Key Drivers Analysis",
    "text": "2b. Key Drivers Analysis\ntodo: replicate the table on slide 75 of the session 5 slides. Specifically, using the dataset provided in the file data_for_drivers_analysis.csv, calculate: pearson correlations, standardized regression coefficients, “usefulness”, Shapley values for a linear regression, Johnson’s relative weights, and the mean decrease in the gini coefficient from a random forest. You may use packages built into R or Python; you do not need to perform these calculations “by hand.”\nIf you want a challenge, add additional measures to the table such as the importance scores from XGBoost, from a Neural Network, or from any additional method that measures the importance of variables."
  }
]