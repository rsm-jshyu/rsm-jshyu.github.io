[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Hi there! I’m Jenny Shyu, I’m passionate about leveraging quantitative methods, data, and technology to generate meaningful insights and tackle real-world business problems.\n\n\n\n\n\n\nM.S. Business Analytics | UCSD 2024-Present\nB.S. Mathematics & Economics, Computational Social Science Minor | UCSD 2021-2024\n\n\n\n\n\n\nExperian | Capstone Project | March 2025 - Present\nUCSD Rady | Graduate Teaching Assistant | MGT 172 Business Project Management | October 2024 - Present\nLPL Financial | Technology Intern | June 2024 - August 2024\nLumnus Consulting | VP Data Analytics | November 2022 - June 2024"
  },
  {
    "objectID": "resume.html",
    "href": "resume.html",
    "title": "Resume",
    "section": "",
    "text": "Download PDF file."
  },
  {
    "objectID": "index.html#about-me",
    "href": "index.html#about-me",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Hi there! I’m Jenny Shyu, a Master’s student in Business Analytics at UC San Diego’s Rady School of Management with a background in Math-Econ.\n\n\nI’m passionate about leveraging quantitative methods, data, and technology to generate meaningful insights and tackle real-world business problems.\n\n\n\nMost recently, I interned at LPL Financial, where I worked with financial data and dashboards. I also served as the VP of Data Analytics in a student consulting organization and currently work as a Graduate Teaching Assistant for a Business Project Management course—helping students apply frameworks like prioritization matrices in real-world project planning.\nIn the past, I also trained and competed as a figure skater representing Chinese Taipei."
  },
  {
    "objectID": "index.html#current",
    "href": "index.html#current",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Capstone Project | Experian\nGraduate Teaching Assistant, MGT 172 Business Project Management | UCSD Rady"
  },
  {
    "objectID": "index.html#work-history",
    "href": "index.html#work-history",
    "title": "Jenny Shyu",
    "section": "",
    "text": "GBK Collective | 2022 - Present\nBain & Co | 2020 - 2021\nCornerstone Research | 2006 - 2014"
  },
  {
    "objectID": "index.html#education",
    "href": "index.html#education",
    "title": "Jenny Shyu",
    "section": "",
    "text": "M.S. Business Analytics | UCSD 2024-Present\nB.S. Mathematics & Economics, Computational Social Science Minor | UCSD 2021-2024"
  },
  {
    "objectID": "index.html#experience",
    "href": "index.html#experience",
    "title": "Jenny Shyu",
    "section": "",
    "text": "Experian | Capstone Project | March 2025 - Present\nUCSD Rady | Graduate Teaching Assistant | MGT 172 Business Project Management | October 2024 - Present\nLPL Financial | Technology Intern | June 2024 - August 2024\nLumnus Consulting | VP Data Analytics | November 2022 - June 2024"
  },
  {
    "objectID": "blog.html",
    "href": "blog.html",
    "title": "My Projects",
    "section": "",
    "text": "A Replication of Karlan and List (2007)\n\n\n\n\n\n\nJenny Shyu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\n\n\n\n\nPoisson Regression Examples\n\n\n\n\n\n\nJenny Shyu\n\n\nMay 7, 2025\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "hw1_questions.html",
    "href": "hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "hw1_questions.html#introduction",
    "href": "hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "hw1_questions.html#data",
    "href": "hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndata.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper).\nAs an ad hoc test of the randomization mechanism, I compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another at the 95% confidence level. If randomization was properly executed, we should expect no statistically significant differences in pre-treatment characteristics between the groups.\nI begin by testing the variable mrm2, which captures the number of months since the last donation. This variable is useful for checking balance because it is unrelated to the treatment assignment and reflects donor history.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\nvars_to_test = [\"mrm2\", \"amount\", \"years\", \"freq\"]\nresults = []\n\nfor var in vars_to_test:\n    subset = data[[\"treatment\", var]].dropna()\n    control = subset[subset[\"treatment\"] == 0][var]\n    treatment = subset[subset[\"treatment\"] == 1][var]\n    \n    # T-test\n    t_stat, p_val = stats.ttest_ind(treatment, control, equal_var=False)\n    \n    # Linear regression\n    regression = smf.ols(f\"{var} ~ treatment\", data=subset).fit()\n    coef = regression.params[\"treatment\"]\n    reg_p = regression.pvalues[\"treatment\"]\n    \n    # Difference in means\n    diff = treatment.mean() - control.mean()\n    \n    results.append({\n        \"Variable\": var,\n        \"Diff (Treat - Control)\": round(diff, 5),\n        \"T-test p-value\": round(p_val, 5),\n        \"Regression Coef\": round(coef, 5),\n        \"Regression p-value\": round(reg_p, 5)\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\nVariable\nDiff (Treat - Control)\nT-test p-value\nRegression Coef\nRegression p-value\n\n\n\n\n0\nmrm2\n0.01369\n0.90485\n0.01369\n0.90489\n\n\n1\namount\n0.15361\n0.05509\n0.15361\n0.06282\n\n\n2\nyears\n-0.05755\n0.27532\n-0.05755\n0.27002\n\n\n3\nfreq\n-0.01198\n0.91174\n-0.01198\n0.91170\n\n\n\n\n\n\n\nThe table of results above shows no statistically significant differences at the 5% level for any variable (p-values &gt; 0.05), though amount is marginally close (p ≈ 0.06 in the regression). This is consistent with proper random assignment.\nThese checks are similar to what Karlan and List report in Table 1 of the original paper, which reassures readers that the treatment effect estimates later in the paper can be interpreted as causal. If pre-treatment covariates are balanced, then observed differences in outcomes are more likely attributable to the randomized treatment itself.\n\nInterpretation\nThese results mirror those presented in Table 1 of Karlan and List (2007), which shows no significant differences between the groups in prior donation behavior and demographic characteristics. Table 1 serves to reassure the reader that any observed treatment effects later in the analysis can be confidently attributed to the randomized intervention rather than pre-existing differences between groups."
  },
  {
    "objectID": "hw1_questions.html#experimental-results",
    "href": "hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\nDonation Rate by Group\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"group\"] = donation_rates[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Create barplot\nplt.figure(figsize=(6, 4))\nax = sns.barplot(data=donation_rates, x=\"group\", y=\"gave\")\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"\")\nplt.title(\"Donation Rate by Group\")\nplt.ylim(0, 0.03)\nplt.grid(axis='y')\n\n# Add percentage labels on top\nfor i, val in enumerate(donation_rates[\"gave\"]):\n    ax.text(i, val + 0.0005, f\"{val:.3%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar plot shows that the treatment group—who received matching grant letters—had a higher donation rate than the control group.\n\n\nT-Test and Linear Regression\n\ntreat_gave = data[data['treatment'] == 1]['gave']\ncontrol_gave = data[data['treatment'] == 0]['gave']\nt_stat, t_pval = stats.ttest_ind(treat_gave, control_gave, equal_var=False)\n\n# Format output\npd.DataFrame([{\n    \"T-test Statistic\": round(t_stat, 3),\n    \"T-test p-value\": round(t_pval, 5)\n}])\n\n\n\n\n\n\n\n\nT-test Statistic\nT-test p-value\n\n\n\n\n0\n3.209\n0.00133\n\n\n\n\n\n\n\n\ngave_regression = smf.ols(\"gave ~ treatment\", data=data).fit()\ncoef = gave_regression.params[\"treatment\"]\nstd_err = gave_regression.bse[\"treatment\"]\np_val = gave_regression.pvalues[\"treatment\"]\nconf_int = gave_regression.conf_int().loc[\"treatment\"]\n\n# Format output\npd.DataFrame([{\n    \"Treatment Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.00418\n0.00135\n0.00193\n0.00154\n0.00682\n\n\n\n\n\n\n\nThe difference in donation rates is statistically significant at the 1% level.\nThe treatment group is more likely to donate, increasing the probability of giving by about 0.42 percentage points.\nThis replicates the result from Table 2A Panel A in Karlan & List (2007), showing that a match offer significantly boosts participation.\nOLS regression shows a statistically significant positive coefficient (≈ 0.0042) on the treatment variable. This confirms the t-test: assignment to the treatment group increased the likelihood of making a donation.\nThis suggests that even a small behavioral nudge like mentioning a matching donation makes people more likely to contribute to charity. People respond to the perception of increased impact.\n\n# Probit model\nimport statsmodels.api as sm\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\ncoef = probit_model.params[\"treatment\"]\nstd_err = probit_model.bse[\"treatment\"]\np_val = probit_model.pvalues[\"treatment\"]\nconf_int = probit_model.conf_int().loc[\"treatment\"]\n\n# Output summary\npd.DataFrame([{\n    \"Probit Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nProbit Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.08678\n0.02788\n0.00185\n0.03214\n0.14143\n\n\n\n\n\n\n\nThe probit model replicates Table 3, Column 1 of Karlan and List (2007), with a significant positive treatment effect (coefficient ≈ 0.087, p ≈ 0.002). This again confirms that individuals are more likely to donate when offered a matching grant.\nTogether, these results demonstrate a consistent and statistically significant treatment effect, providing strong evidence that the framing of charitable solicitations matters for donor behavior.\nThe match incentive not only has a practical impact but also a statistically robust one, even under a probit framework.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\nalso include p-velue 2:1 vs 1:1, 3:1 vs 2:1, 3:1 vs 1:1\nfor ols regresssion only have intercept, ration2, ratio3\nraw difference (2:1-1:1) raw difference (3:1 - 2:1) fitted difference (2:1 - 1:1) fitted difference (3:1 - 2:1)\n\nResponse Rates by Match Ratio\n\nmatched_data = data[(data[\"treatment\"] == 1) & (data[\"ratio\"].isin([1, 2, 3]))]\n\n# Calculate means\nresponse_rates = matched_data.groupby(\"ratio\")[\"gave\"].mean()\n\n# Separate groups for pairwise comparisons\ngave_1 = matched_data[matched_data[\"ratio\"] == 1][\"gave\"]\ngave_2 = matched_data[matched_data[\"ratio\"] == 2][\"gave\"]\ngave_3 = matched_data[matched_data[\"ratio\"] == 3][\"gave\"]\n\n# T-tests for pairwise comparisons\nfrom scipy import stats\n\nsummary = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\", \"3:1 vs 1:1\"],\n    \"p-value\": [\n        round(stats.ttest_ind(gave_2, gave_1, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_2, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_1, equal_var=False).pvalue, 5)\n    ],\n    \"Rate A\": [round(gave_2.mean(), 5), round(gave_3.mean(), 5), round(gave_3.mean(), 5)],\n    \"Rate B\": [round(gave_1.mean(), 5), round(gave_2.mean(), 5), round(gave_1.mean(), 5)],\n    \"Difference (A - B)\": [\n        round(gave_2.mean() - gave_1.mean(), 5),\n        round(gave_3.mean() - gave_2.mean(), 5),\n        round(gave_3.mean() - gave_1.mean(), 5)\n    ]\n})\n\nsummary\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_20691/2990980929.py:4: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nComparison\np-value\nRate A\nRate B\nDifference (A - B)\n\n\n\n\n0\n2:1 vs 1:1\n0.33453\n0.02263\n0.02075\n0.00188\n\n\n1\n3:1 vs 2:1\n0.96003\n0.02273\n0.02263\n0.00010\n\n\n2\n3:1 vs 1:1\n0.31011\n0.02273\n0.02075\n0.00198\n\n\n\n\n\n\n\nObserved donation rates:\n1:1 match — 2.07%\n2:1 match — 2.26%\n3:1 match — 2.27%\nThe increase from 1:1 to 2:1 and 3:1 appears small.\nNone of the pairwise comparisons are statistically significant. This supports the paper’s statement on page 8 that larger match ratios do not lead to meaningfully higher donation rates.\n\n\nRegression: Match Ratio Effects\n\n# Regression with dummy variables (baseline: 1:1 match)\nimport statsmodels.formula.api as smf\n\nmatched_data[\"ratio2\"] = (matched_data[\"ratio\"] == 2).astype(int)\nmatched_data[\"ratio3\"] = (matched_data[\"ratio\"] == 3).astype(int)\n\nreg_model = smf.ols(\"gave ~ ratio2 + ratio3\", data=matched_data).fit()\n\n# Clean formatted output\ncoefs = reg_model.params\nstderr = reg_model.bse\npvals = reg_model.pvalues\nconfint = reg_model.conf_int()\n\npd.DataFrame({\n    \"Coefficient\": coefs.round(5),\n    \"Std. Error\": stderr.round(5),\n    \"p-value\": pvals.round(5),\n    \"95% CI Lower\": confint[0].round(5),\n    \"95% CI Upper\": confint[1].round(5)\n}).loc[[\"Intercept\", \"ratio2\", \"ratio3\"]].reset_index().rename(columns={\"index\": \"Term\"})\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_20691/759364550.py:4: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_20691/759364550.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.02075\n0.00139\n0.00000\n0.01802\n0.02348\n\n\n1\nratio2\n0.00188\n0.00197\n0.33828\n-0.00197\n0.00574\n\n\n2\nratio3\n0.00198\n0.00197\n0.31332\n-0.00187\n0.00584\n\n\n\n\n\n\n\nRegression results:\nThe baseline (1:1 match) donation rate is about 2.07%.\nThe 2:1 match effect: +0.19 percentage points (not statistically significant).\nThe 3:1 match effect: +0.20 percentage points (also not statistically significant).\nNeither the 2:1 nor 3:1 match ratio coefficients are statistically significant. The results suggest that changing the size of the match ratio does not significantly change donation likelihood relative to 1:1.\n\n\nDifference in Response Rates (Data vs. Regression Coefficients)\n\n# Mean differences and coefficient comparisons\npd.DataFrame([\n    {\n        \"Comparison\": \"2:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_2.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 2:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_2.mean(), 5),\n        \"Regression Coefficient Diff (3 - 2)\": round(reg_model.params[\"ratio3\"] - reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio3\"], 5)\n    }\n])\n\n\n\n\n\n\n\n\nComparison\nRaw Mean Difference\nRegression Coefficient\nRegression Coefficient Diff (3 - 2)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\nNaN\n\n\n1\n3:1 vs 2:1\n0.00010\nNaN\n0.0001\n\n\n2\n3:1 vs 1:1\n0.00198\n0.00198\nNaN\n\n\n\n\n\n\n\nThe differences in donation rates between 1:1, 2:1, and 3:1 match offers are very small and not statistically significant. These findings replicate the comment on page 8 of Karlan and List (2007): “Larger match ratios relative to a smaller match ratio had no additional impact.”\nThis suggests that donors may respond to the presence of a match, but not necessarily to the size of the match. Psychologically, the idea of having one’s donation matched could serve as a signal of trust or endorsement—but the exact multiplier does not further influence behavior.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot.\ncharts side by side and scale #### All Respondents: Does Treatment Affect Amount Donated?\n\namount_reg_all = smf.ols(\"amount ~ treatment\", data=data).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_all.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_all.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_all.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_all.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_all.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_all.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.15361\n0.08256\n1.861\n0.06282\n-0.00822\n0.31543\n\n\n\n\n\n\n\nCoefficient on treatment ≈ 0.15\np-value ≈ 0.063\nAmong all individuals (including non-donors), the treatment group donated about $0.15 more on average. This effect is marginally significant (p ≈ 0.063). It suggests that matching increases expected donations slightly, but much of that effect may be driven by more people giving (rather than giving more).\nThis suggests that those who received a matching letter donated slightly more on average. However, the result is only marginally significant (at the 10% level). This weak evidence may indicate that the offer of a match has a small impact on the total amount donated—though for most people, the presence of the match does not substantially alter donation size.\n\nConditional on Donation: Do Donors Give More if Matched?\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1]\n\n# Linear regression among donors only\namount_reg_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_donors.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_donors.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_donors.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_donors.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_donors.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_donors.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n-1.66839\n2.87238\n-0.581\n0.56148\n-7.30477\n3.96799\n\n\n\n\n\n\n\nCoefficient on treatment ≈ -1.67\np-value = 0.561\nAmong those who did donate, receiving a match letter did not significantly change the amount given. In fact, the coefficient is slightly negative, though not significant. Thus, we conclude that while match offers may increase the number of donors, they do not cause donors to give more, conditional on giving.\nThis coefficient does not have a strong causal interpretation, because donation decisions and donation amounts are jointly determined and the sample is selected on gave == 1.\n\n\nDistribution of Donations Among Donors\n\ntreatment_donors = data[(data[\"treatment\"] == 1) & (data[\"gave\"] == 1)]\ncontrol_donors = data[(data[\"treatment\"] == 0) & (data[\"gave\"] == 1)]\n\nmean_treat = treatment_donors[\"amount\"].mean()\nmean_control = control_donors[\"amount\"].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\nsns.histplot(treatment_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[0])\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f\"Mean: ${mean_treat:.2f}\")\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\nsns.histplot(control_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[1])\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f\"Mean: ${mean_control:.2f}\")\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nTwo histograms show the distribution of donation amounts for the treatment group and control group, restricted to those who donated. The vertical red line marks the average for each group:\nTreatment Mean: ~$43.87\nControl Mean: ~$45.54\nBoth distributions are right-skewed, with most donors giving small amounts and a few contributing large sums. There is no visible shift in the average due to the treatment.\n\n\nConclusion\nThese analyses support the idea that matching offers increase response rate, but do not change how much people give once they’ve decided to donate. This distinction is important for fundraising strategies: matching may motivate more people to give, but it doesn’t necessarily increase per-donor revenue."
  },
  {
    "objectID": "hw1_questions.html#simulation-experiment",
    "href": "hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_sim = 10000\nnp.random.seed(42)\n# Simulate 10,000 binary outcomes for each group\ncontrol_sim = np.random.binomial(1, p_control, 100000)\ntreatment_sim = np.random.binomial(1, p_treatment, n_sim)\n\n# Vector of differences\ndiffs = treatment_sim - control_sim[:10000]\n\n# Cumulative average of the differences\ncum_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(8, 4))\nplt.plot(cum_avg, label='Cumulative Average Difference', color='orange')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers: Cumulative Average of Simulated Differences\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\nThis plot demonstrates the Law of Large Numbers. As we simulate more and more observations, the cumulative average of the differences converges toward the true mean difference (0.004). Initially, there’s randomness and fluctuation, but the line stabilizes as the number of observations increases.\nThis convergence is the foundation for using sample averages to estimate population parameters and underpins why large sample sizes give us more reliable estimates in experiments.\n\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nsample_sizes = [50, 200, 500, 1000]\nn_reps = 1000\n\nnp.random.seed(42)\nhistograms = {}\n\n# Simulate average differences for each sample size\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(n_reps):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    histograms[n] = diffs\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(histograms[n], bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero Reference\")\n    axes[i].axvline(true_diff, color='green', linestyle='--', label=\"True Difference (0.004)\")\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\n\nInterpretation\nThese four histograms illustrate how the sampling distribution of the difference in means behaves at increasing sample sizes:\nAt n = 50, the distribution is quite wide and skewed. Zero is within the center-ish but not tightly.\nAs sample size increases, the distribution becomes tighter, more symmetric, and centered.\nBy n = 1000, the distribution of average differences closely resembles a normal distribution centered near the true mean difference (0.004).\nThis is a direct illustration of the Central Limit Theorem:\nAs sample size increases, the distribution of the sample mean difference becomes approximately normal, regardless of the original distribution shape.\nAlso note: zero shifts from being more “middle-ish” in smaller samples to being closer to the tail as the signal (the true effect) dominates the noise."
  },
  {
    "objectID": "hw1_questions (1).html",
    "href": "hw1_questions (1).html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (1).html#introduction",
    "href": "hw1_questions (1).html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nto do: expand on the description of the experiment.\nThis project seeks to replicate their results."
  },
  {
    "objectID": "hw1_questions (1).html#data",
    "href": "hw1_questions (1).html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\ntodo: Read the data into R/Python and describe the data\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I provide a series of tests that compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another.\ntodo: test a few variables other than the key outcome variables (for example, test months since last donation) to see if the treatment and control groups are statistically significantly different at the 95% confidence level. Do each as a t-test and separately as a linear regression, and confirm you get the exact same results from both methods. When doing a t-test, use the formula in the class slides. When doing the linear regression, regress for example mrm2 on treatment and look at the estimated coefficient on the treatment variable. It might be helpful to compare parts of your analysis to Table 1 in the paper. Be sure to comment on your results (hint: why is Table 1 included in the paper)."
  },
  {
    "objectID": "hw1_questions (1).html#experimental-results",
    "href": "hw1_questions (1).html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\ntodo: make a barplot with two bars. Each bar is the proportion of people who donated. One bar for treatment and one bar for control.\ntodo: run a t-test between the treatment and control groups on the binary outcome of whether any charitable donation was made. Also run a bivariate linear regression that demonstrates the same finding. (It may help to confirm your calculations match Table 2a Panel A.) Report your statistical results and interpret them in the context of the experiment (e.g., if you found a difference with a small p-value or that was statistically significant at some threshold, what have you learned about human behavior? Use mostly English words, not numbers or stats, to explain your finding.)\ntodo: run a probit regression where the outcome variable is whether any charitable donation was made and the explanatory variable is assignment to treatment or control. Confirm that your results replicate Table 3 column 1 in the paper.\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\ntodo: Use a series of t-tests to test whether the size of the match ratio has an effect on whether people donate or not. For example, does the 2:1 match rate lead increase the likelihood that someone donates as compared to the 1:1 match rate? Do your results support the “figures suggest” comment the authors make on page 8?\ntodo: Assess the same issue using a regression. Specifically, create the variable ratio1 then regress gave on ratio1, ratio2, and ratio3 (or alternatively, regress gave on the categorical variable ratio). Interpret the coefficients and their statistical precision.\ntodo: Calculate the response rate difference between the 1:1 and 2:1 match ratios and the 2:1 and 3:1 ratios. Do this directly from the data, and do it by computing the differences in the fitted coefficients of the previous regression. what do you conclude regarding the effectiveness of different sizes of matched donations?\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ntodo: Calculate a t-test or run a bivariate linear regression of the donation amount on the treatment status. What do we learn from doing this analysis?\ntodo: now limit the data to just people who made a donation and repeat the previous analysis. This regression allows you to analyze how much respondents donate conditional on donating some positive amount. Interpret the regression coefficients – what did we learn? Does the treatment coefficient have a causal interpretation?\ntodo: Make two plot: one for the treatment group and one for the control. Each plot should be a histogram of the donation amounts only among people who donated. Add a red vertical bar or some other annotation to indicate the sample average for each plot."
  },
  {
    "objectID": "hw1_questions (1).html#simulation-experiment",
    "href": "hw1_questions (1).html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\nto do: Make a plot like those on slide 43 from our first class and explain the plot to the reader. To do this, you will simulate 100,00 draws from the control distribution and 10,000 draws from the treatment distribution. You’ll then calculate a vector of 10,000 differences, and then you’ll plot the cumulative average of that vector of differences. Comment on whether the cumulative average approaches the true difference in means.\n\n\nCentral Limit Theorem\nto do: Make 4 histograms like those on slide 44 from our first class at sample sizes 50, 200, 500, and 1000 and explain these plots to the reader. To do this for a sample size of e.g. 50, take 50 draws from each of the control and treatment distributions, and calculate the average difference between those draws. Then repeat that process 999 more times so that you have 1000 averages. Plot the histogram of those averages. Comment on whether zero is in the “middle” of the distribution or whether it’s in the “tail.”"
  },
  {
    "objectID": "blog/project1/hw1_questions.html",
    "href": "blog/project1/hw1_questions.html",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#introduction",
    "href": "blog/project1/hw1_questions.html#introduction",
    "title": "A Replication of Karlan and List (2007)",
    "section": "",
    "text": "Dean Karlan at Yale and John List at the University of Chicago conducted a field experiment to test the effectiveness of different fundraising letters. They sent out 50,000 fundraising letters to potential donors, randomly assigning each letter to one of three treatments: a standard letter, a matching grant letter, or a challenge grant letter. They published the results of this experiment in the American Economic Review in 2007. The article and supporting data are available from the AEA website and from Innovations for Poverty Action as part of Harvard’s Dataverse.\nThis project seeks to replicate their results. The goal of the experiment was to test whether offering a matching donation—in which a lead donor promises to match contributions from other individuals—would increase the likelihood and/or size of charitable donations. In addition to testing whether matching grants were effective in general, Karlan and List also explored whether the size of the match mattered. Some participants were told that every dollar they donated would be matched 1:1, while others were offered more generous matches (2:1 or 3:1), allowing the researchers to test for differences in donor behavior across match sizes.\nThe experiment is notable for its scale, randomization, and use of real-world donor behavior, which together provide credible evidence of causal effects. Because the fundraising letters were identical in every respect except for the treatment condition, any differences in outcomes across groups can be attributed to the match offer itself. This approach allows for insights not only into how people respond to incentives, but also into broader questions about social influence, perceived impact, and behavioral nudges in charitable giving."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#data",
    "href": "blog/project1/hw1_questions.html#data",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Data",
    "text": "Data\n\nDescription\n\nimport pandas as pd\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\ndata.head()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio\nratio2\nratio3\nsize\nsize25\nsize50\nsize100\nsizeno\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\n0\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n0.0\n1.0\n0.446493\n0.527769\n0.317591\n2.10\n28517.0\n0.499807\n0.324528\n1.0\n\n\n1\n0\n1\nControl\n0\n0\nControl\n0\n0\n0\n0\n...\n1.0\n0.0\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n2\n1\n0\n1\n0\n0\n$100,000\n0\n0\n1\n0\n...\n0.0\n1.0\n0.935706\n0.011948\n0.276128\n2.48\n51175.0\n0.721941\n0.192668\n1.0\n\n\n3\n1\n0\n1\n0\n0\nUnstated\n0\n0\n0\n1\n...\n1.0\n0.0\n0.888331\n0.010760\n0.279412\n2.65\n79269.0\n0.920431\n0.412142\n1.0\n\n\n4\n1\n0\n1\n0\n0\n$50,000\n0\n1\n0\n0\n...\n0.0\n1.0\n0.759014\n0.127421\n0.442389\n1.85\n40908.0\n0.416072\n0.439965\n1.0\n\n\n\n\n5 rows × 51 columns\n\n\n\n\ndata.describe()\n\n\n\n\n\n\n\n\ntreatment\ncontrol\nratio2\nratio3\nsize25\nsize50\nsize100\nsizeno\naskd1\naskd2\n...\nredcty\nbluecty\npwhite\npblack\npage18_39\nave_hh_sz\nmedian_hhincome\npowner\npsch_atlstba\npop_propurban\n\n\n\n\ncount\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n50083.000000\n...\n49978.000000\n49978.000000\n48217.000000\n48047.000000\n48217.000000\n48221.000000\n48209.000000\n48214.000000\n48215.000000\n48217.000000\n\n\nmean\n0.666813\n0.333187\n0.222311\n0.222211\n0.166723\n0.166623\n0.166723\n0.166743\n0.222311\n0.222291\n...\n0.510245\n0.488715\n0.819599\n0.086710\n0.321694\n2.429012\n54815.700533\n0.669418\n0.391661\n0.871968\n\n\nstd\n0.471357\n0.471357\n0.415803\n0.415736\n0.372732\n0.372643\n0.372732\n0.372750\n0.415803\n0.415790\n...\n0.499900\n0.499878\n0.168560\n0.135868\n0.103039\n0.378105\n22027.316665\n0.193405\n0.186599\n0.258633\n\n\nmin\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.009418\n0.000000\n0.000000\n0.000000\n5000.000000\n0.000000\n0.000000\n0.000000\n\n\n25%\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n0.000000\n0.000000\n0.755845\n0.014729\n0.258311\n2.210000\n39181.000000\n0.560222\n0.235647\n0.884929\n\n\n50%\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n0.000000\n0.872797\n0.036554\n0.305534\n2.440000\n50673.000000\n0.712296\n0.373744\n1.000000\n\n\n75%\n1.000000\n1.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n0.000000\n...\n1.000000\n1.000000\n0.938827\n0.090882\n0.369132\n2.660000\n66005.000000\n0.816798\n0.530036\n1.000000\n\n\nmax\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n1.000000\n...\n1.000000\n1.000000\n1.000000\n0.989622\n0.997544\n5.270000\n200001.000000\n1.000000\n1.000000\n1.000000\n\n\n\n\n8 rows × 48 columns\n\n\n\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\ntreatment\nTreatment\n\n\ncontrol\nControl\n\n\nratio\nMatch ratio\n\n\nratio2\n2:1 match ratio\n\n\nratio3\n3:1 match ratio\n\n\nsize\nMatch threshold\n\n\nsize25\n$25,000 match threshold\n\n\nsize50\n$50,000 match threshold\n\n\nsize100\n$100,000 match threshold\n\n\nsizeno\nUnstated match threshold\n\n\nask\nSuggested donation amount\n\n\naskd1\nSuggested donation was highest previous contribution\n\n\naskd2\nSuggested donation was 1.25 x highest previous contribution\n\n\naskd3\nSuggested donation was 1.50 x highest previous contribution\n\n\nask1\nHighest previous contribution (for suggestion)\n\n\nask2\n1.25 x highest previous contribution (for suggestion)\n\n\nask3\n1.50 x highest previous contribution (for suggestion)\n\n\namount\nDollars given\n\n\ngave\nGave anything\n\n\namountchange\nChange in amount given\n\n\nhpa\nHighest previous contribution\n\n\nltmedmra\nSmall prior donor: last gift was less than median $35\n\n\nfreq\nNumber of prior donations\n\n\nyears\nNumber of years since initial donation\n\n\nyear5\nAt least 5 years since initial donation\n\n\nmrm2\nNumber of months since last donation\n\n\ndormant\nAlready donated in 2005\n\n\nfemale\nFemale\n\n\ncouple\nCouple\n\n\nstate50one\nState tag: 1 for one observation of each of 50 states; 0 otherwise\n\n\nnonlit\nNonlitigation\n\n\ncases\nCourt cases from state in 2004-5 in which organization was involved\n\n\nstatecnt\nPercent of sample from state\n\n\nstateresponse\nProportion of sample from the state who gave\n\n\nstateresponset\nProportion of treated sample from the state who gave\n\n\nstateresponsec\nProportion of control sample from the state who gave\n\n\nstateresponsetminc\nstateresponset - stateresponsec\n\n\nperbush\nState vote share for Bush\n\n\nclose25\nState vote share for Bush between 47.5% and 52.5%\n\n\nred0\nRed state\n\n\nblue0\nBlue state\n\n\nredcty\nRed county\n\n\nbluecty\nBlue county\n\n\npwhite\nProportion white within zip code\n\n\npblack\nProportion black within zip code\n\n\npage18_39\nProportion age 18-39 within zip code\n\n\nave_hh_sz\nAverage household size within zip code\n\n\nmedian_hhincome\nMedian household income within zip code\n\n\npowner\nProportion house owner within zip code\n\n\npsch_atlstba\nProportion who finished college within zip code\n\n\npop_propurban\nProportion of population urban within zip code\n\n\n\n\n\n\n\n\nBalance Test\nAs an ad hoc test of the randomization mechanism, I compare aspects of the treatment and control groups to assess whether they are statistically significantly different from one another at the 95% confidence level. If randomization was properly executed, we should expect no statistically significant differences in pre-treatment characteristics between the groups.\nI begin by testing the variable mrm2, which captures the number of months since the last donation. This variable is useful for checking balance because it is unrelated to the treatment assignment and reflects donor history.\n\nimport pandas as pd\nimport statsmodels.formula.api as smf\nfrom scipy import stats\n\n\ndata = pd.read_stata(\"karlan_list_2007.dta\")\n\nvars_to_test = [\"mrm2\", \"amount\", \"years\", \"freq\"]\nresults = []\n\nfor var in vars_to_test:\n    subset = data[[\"treatment\", var]].dropna()\n    control = subset[subset[\"treatment\"] == 0][var]\n    treatment = subset[subset[\"treatment\"] == 1][var]\n    \n    # T-test\n    t_stat, p_val = stats.ttest_ind(treatment, control, equal_var=False)\n    \n    # Linear regression\n    regression = smf.ols(f\"{var} ~ treatment\", data=subset).fit()\n    coef = regression.params[\"treatment\"]\n    reg_p = regression.pvalues[\"treatment\"]\n    \n    # Difference in means\n    diff = treatment.mean() - control.mean()\n    \n    results.append({\n        \"Variable\": var,\n        \"Diff (Treat - Control)\": round(diff, 5),\n        \"T-test p-value\": round(p_val, 5),\n        \"Regression Coef\": round(coef, 5),\n        \"Regression p-value\": round(reg_p, 5)\n    })\n\npd.DataFrame(results)\n\n\n\n\n\n\n\n\nVariable\nDiff (Treat - Control)\nT-test p-value\nRegression Coef\nRegression p-value\n\n\n\n\n0\nmrm2\n0.01369\n0.90485\n0.01369\n0.90489\n\n\n1\namount\n0.15361\n0.05509\n0.15361\n0.06282\n\n\n2\nyears\n-0.05755\n0.27532\n-0.05755\n0.27002\n\n\n3\nfreq\n-0.01198\n0.91174\n-0.01198\n0.91170\n\n\n\n\n\n\n\nThe table of results above shows no statistically significant differences at the 5% level for any variable (p-values &gt; 0.05), though amount is marginally close (p ≈ 0.06 in the regression). This is consistent with proper random assignment.\nThese checks are similar to what Karlan and List report in Table 1 of the original paper, which reassures readers that the treatment effect estimates later in the paper can be interpreted as causal. If pre-treatment covariates are balanced, then observed differences in outcomes are more likely attributable to the randomized treatment itself.\nThese results mirror those presented in Table 1 of Karlan and List (2007), which shows no significant differences between the groups in prior donation behavior and demographic characteristics. Table 1 serves to reassure the reader that any observed treatment effects later in the analysis can be confidently attributed to the randomized intervention rather than pre-existing differences between groups."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#experimental-results",
    "href": "blog/project1/hw1_questions.html#experimental-results",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Experimental Results",
    "text": "Experimental Results\n\nCharitable Contribution Made\nFirst, I analyze whether matched donations lead to an increased response rate of making a donation.\n\nDonation Rate by Group\n\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Calculate donation rates\ndonation_rates = data.groupby(\"treatment\")[\"gave\"].mean().reset_index()\ndonation_rates[\"group\"] = donation_rates[\"treatment\"].map({0: \"Control\", 1: \"Treatment\"})\n\n# Create barplot\nplt.figure(figsize=(6, 4))\nax = sns.barplot(data=donation_rates, x=\"group\", y=\"gave\")\nplt.ylabel(\"Proportion Donated\")\nplt.xlabel(\"\")\nplt.title(\"Donation Rate by Group\")\nplt.ylim(0, 0.03)\nplt.grid(axis='y')\n\n# Add percentage labels on top\nfor i, val in enumerate(donation_rates[\"gave\"]):\n    ax.text(i, val + 0.0005, f\"{val:.3%}\", ha='center', va='bottom')\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis bar plot shows that the treatment group—who received matching grant letters—had a higher donation rate than the control group.\n\n\nT-Test and Linear Regression\n\ntreat_gave = data[data['treatment'] == 1]['gave']\ncontrol_gave = data[data['treatment'] == 0]['gave']\nt_stat, t_pval = stats.ttest_ind(treat_gave, control_gave, equal_var=False)\n\n# Format output\npd.DataFrame([{\n    \"T-test Statistic\": round(t_stat, 3),\n    \"T-test p-value\": round(t_pval, 5)\n}])\n\n\n\n\n\n\n\n\nT-test Statistic\nT-test p-value\n\n\n\n\n0\n3.209\n0.00133\n\n\n\n\n\n\n\n\ngave_regression = smf.ols(\"gave ~ treatment\", data=data).fit()\ncoef = gave_regression.params[\"treatment\"]\nstd_err = gave_regression.bse[\"treatment\"]\np_val = gave_regression.pvalues[\"treatment\"]\nconf_int = gave_regression.conf_int().loc[\"treatment\"]\n\n# Format output\npd.DataFrame([{\n    \"Treatment Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.00418\n0.00135\n0.00193\n0.00154\n0.00682\n\n\n\n\n\n\n\nThe difference in donation rates is statistically significant at the 1% level.\nThe treatment group is more likely to donate, increasing the probability of giving by about 0.42 percentage points.\nThis replicates the result from Table 2A Panel A in Karlan & List (2007), showing that a match offer significantly boosts participation.\nOLS regression shows a statistically significant positive coefficient (≈ 0.0042) on the treatment variable. This confirms the t-test: assignment to the treatment group increased the likelihood of making a donation.\nThis suggests that even a small behavioral nudge like mentioning a matching donation makes people more likely to contribute to charity. People respond to the perception of increased impact.\n\n# Probit model\nimport statsmodels.api as sm\n\nprobit_model = smf.probit(\"gave ~ treatment\", data=data).fit()\ncoef = probit_model.params[\"treatment\"]\nstd_err = probit_model.bse[\"treatment\"]\np_val = probit_model.pvalues[\"treatment\"]\nconf_int = probit_model.conf_int().loc[\"treatment\"]\n\n# Output summary\npd.DataFrame([{\n    \"Probit Coefficient\": round(coef, 5),\n    \"Standard Error\": round(std_err, 5),\n    \"p-value\": round(p_val, 5),\n    \"95% CI Lower\": round(conf_int[0], 5),\n    \"95% CI Upper\": round(conf_int[1], 5)\n}])\n\nOptimization terminated successfully.\n         Current function value: 0.100443\n         Iterations 7\n\n\n\n\n\n\n\n\n\nProbit Coefficient\nStandard Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.08678\n0.02788\n0.00185\n0.03214\n0.14143\n\n\n\n\n\n\n\nThe probit model replicates Table 3, Column 1 of Karlan and List (2007), with a significant positive treatment effect (coefficient ≈ 0.087, p ≈ 0.002). This again confirms that individuals are more likely to donate when offered a matching grant.\nTogether, these results demonstrate a consistent and statistically significant treatment effect, providing strong evidence that the framing of charitable solicitations matters for donor behavior.\nThe match incentive not only has a practical impact but also a statistically robust one, even under a probit framework.\n\n\n\nDifferences between Match Rates\nNext, I assess the effectiveness of different sizes of matched donations on the response rate.\n\nResponse Rates by Match Ratio\n\nmatched_data = data[(data[\"treatment\"] == 1) & (data[\"ratio\"].isin([1, 2, 3]))]\n\n# Calculate means\nresponse_rates = matched_data.groupby(\"ratio\")[\"gave\"].mean()\n\n# Separate groups for pairwise comparisons\ngave_1 = matched_data[matched_data[\"ratio\"] == 1][\"gave\"]\ngave_2 = matched_data[matched_data[\"ratio\"] == 2][\"gave\"]\ngave_3 = matched_data[matched_data[\"ratio\"] == 3][\"gave\"]\n\n# T-tests for pairwise comparisons\nfrom scipy import stats\n\nsummary = pd.DataFrame({\n    \"Comparison\": [\"2:1 vs 1:1\", \"3:1 vs 2:1\", \"3:1 vs 1:1\"],\n    \"p-value\": [\n        round(stats.ttest_ind(gave_2, gave_1, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_2, equal_var=False).pvalue, 5),\n        round(stats.ttest_ind(gave_3, gave_1, equal_var=False).pvalue, 5)\n    ],\n    \"Rate A\": [round(gave_2.mean(), 5), round(gave_3.mean(), 5), round(gave_3.mean(), 5)],\n    \"Rate B\": [round(gave_1.mean(), 5), round(gave_2.mean(), 5), round(gave_1.mean(), 5)],\n    \"Difference (A - B)\": [\n        round(gave_2.mean() - gave_1.mean(), 5),\n        round(gave_3.mean() - gave_2.mean(), 5),\n        round(gave_3.mean() - gave_1.mean(), 5)\n    ]\n})\n\nsummary\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_40228/2990980929.py:4: FutureWarning:\n\nThe default of observed=False is deprecated and will be changed to True in a future version of pandas. Pass observed=False to retain current behavior or observed=True to adopt the future default and silence this warning.\n\n\n\n\n\n\n\n\n\n\nComparison\np-value\nRate A\nRate B\nDifference (A - B)\n\n\n\n\n0\n2:1 vs 1:1\n0.33453\n0.02263\n0.02075\n0.00188\n\n\n1\n3:1 vs 2:1\n0.96003\n0.02273\n0.02263\n0.00010\n\n\n2\n3:1 vs 1:1\n0.31011\n0.02273\n0.02075\n0.00198\n\n\n\n\n\n\n\nObserved donation rates:\n1:1 match — 2.07%\n2:1 match — 2.26%\n3:1 match — 2.27%\nThe increase from 1:1 to 2:1 and 3:1 appears small.\nNone of the pairwise comparisons are statistically significant. This supports the paper’s statement on page 8 that larger match ratios do not lead to meaningfully higher donation rates.\n\n\nRegression: Match Ratio Effects\n\n# Regression with dummy variables (baseline: 1:1 match)\nimport statsmodels.formula.api as smf\n\nmatched_data[\"ratio2\"] = (matched_data[\"ratio\"] == 2).astype(int)\nmatched_data[\"ratio3\"] = (matched_data[\"ratio\"] == 3).astype(int)\n\nreg_model = smf.ols(\"gave ~ ratio2 + ratio3\", data=matched_data).fit()\n\n# Clean formatted output\ncoefs = reg_model.params\nstderr = reg_model.bse\npvals = reg_model.pvalues\nconfint = reg_model.conf_int()\n\npd.DataFrame({\n    \"Coefficient\": coefs.round(5),\n    \"Std. Error\": stderr.round(5),\n    \"p-value\": pvals.round(5),\n    \"95% CI Lower\": confint[0].round(5),\n    \"95% CI Upper\": confint[1].round(5)\n}).loc[[\"Intercept\", \"ratio2\", \"ratio3\"]].reset_index().rename(columns={\"index\": \"Term\"})\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_40228/759364550.py:4: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n/var/folders/h7/9ry6pj514cb7qp5v6btr37ym0000gn/T/ipykernel_40228/759364550.py:5: SettingWithCopyWarning:\n\n\nA value is trying to be set on a copy of a slice from a DataFrame.\nTry using .loc[row_indexer,col_indexer] = value instead\n\nSee the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n\n\n\n\n\n\n\n\n\n\nTerm\nCoefficient\nStd. Error\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n0.02075\n0.00139\n0.00000\n0.01802\n0.02348\n\n\n1\nratio2\n0.00188\n0.00197\n0.33828\n-0.00197\n0.00574\n\n\n2\nratio3\n0.00198\n0.00197\n0.31332\n-0.00187\n0.00584\n\n\n\n\n\n\n\nRegression results:\nThe baseline (1:1 match) donation rate is about 2.07%.\nThe 2:1 match effect: +0.19 percentage points (not statistically significant).\nThe 3:1 match effect: +0.20 percentage points (also not statistically significant).\nNeither the 2:1 nor 3:1 match ratio coefficients are statistically significant. The results suggest that changing the size of the match ratio does not significantly change donation likelihood relative to 1:1.\n\n\nDifference in Response Rates (Data vs. Regression Coefficients)\n\n# Mean differences and coefficient comparisons\npd.DataFrame([\n    {\n        \"Comparison\": \"2:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_2.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 2:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_2.mean(), 5),\n        \"Regression Coefficient Diff (3 - 2)\": round(reg_model.params[\"ratio3\"] - reg_model.params[\"ratio2\"], 5)\n    },\n    {\n        \"Comparison\": \"3:1 vs 1:1\",\n        \"Raw Mean Difference\": round(gave_3.mean() - gave_1.mean(), 5),\n        \"Regression Coefficient\": round(reg_model.params[\"ratio3\"], 5)\n    }\n])\n\n\n\n\n\n\n\n\nComparison\nRaw Mean Difference\nRegression Coefficient\nRegression Coefficient Diff (3 - 2)\n\n\n\n\n0\n2:1 vs 1:1\n0.00188\n0.00188\nNaN\n\n\n1\n3:1 vs 2:1\n0.00010\nNaN\n0.0001\n\n\n2\n3:1 vs 1:1\n0.00198\n0.00198\nNaN\n\n\n\n\n\n\n\nThe differences in donation rates between 1:1, 2:1, and 3:1 match offers are very small and not statistically significant. These findings replicate the comment on page 8 of Karlan and List (2007): “Larger match ratios relative to a smaller match ratio had no additional impact.”\nThis suggests that donors may respond to the presence of a match, but not necessarily to the size of the match. Psychologically, the idea of having one’s donation matched could serve as a signal of trust or endorsement—but the exact multiplier does not further influence behavior.\n\n\n\nSize of Charitable Contribution\nIn this subsection, I analyze the effect of the size of matched donation on the size of the charitable contribution.\ncharts side by side and scale #### All Respondents: Does Treatment Affect Amount Donated?\n\namount_reg_all = smf.ols(\"amount ~ treatment\", data=data).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_all.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_all.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_all.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_all.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_all.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_all.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n0.15361\n0.08256\n1.861\n0.06282\n-0.00822\n0.31543\n\n\n\n\n\n\n\nCoefficient on treatment ≈ 0.15\np-value ≈ 0.063\nAmong all individuals (including non-donors), the treatment group donated about $0.15 more on average. This effect is marginally significant (p ≈ 0.063). It suggests that matching increases expected donations slightly, but much of that effect may be driven by more people giving (rather than giving more).\nThis suggests that those who received a matching letter donated slightly more on average. However, the result is only marginally significant (at the 10% level). This weak evidence may indicate that the offer of a match has a small impact on the total amount donated—though for most people, the presence of the match does not substantially alter donation size.\n\nConditional on Donation: Do Donors Give More if Matched?\n\n# Subset to donors only\ndonors = data[data[\"gave\"] == 1]\n\n# Linear regression among donors only\namount_reg_donors = smf.ols(\"amount ~ treatment\", data=donors).fit()\n\npd.DataFrame([{\n    \"Treatment Coefficient\": round(amount_reg_donors.params[\"treatment\"], 5),\n    \"Std. Error\": round(amount_reg_donors.bse[\"treatment\"], 5),\n    \"t-statistic\": round(amount_reg_donors.tvalues[\"treatment\"], 3),\n    \"p-value\": round(amount_reg_donors.pvalues[\"treatment\"], 5),\n    \"95% CI Lower\": round(amount_reg_donors.conf_int().loc[\"treatment\", 0], 5),\n    \"95% CI Upper\": round(amount_reg_donors.conf_int().loc[\"treatment\", 1], 5)\n}])\n\n\n\n\n\n\n\n\nTreatment Coefficient\nStd. Error\nt-statistic\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\n-1.66839\n2.87238\n-0.581\n0.56148\n-7.30477\n3.96799\n\n\n\n\n\n\n\nCoefficient on treatment ≈ -1.67\np-value = 0.561\nAmong those who did donate, receiving a match letter did not significantly change the amount given. In fact, the coefficient is slightly negative, though not significant. Thus, we conclude that while match offers may increase the number of donors, they do not cause donors to give more, conditional on giving.\nThis coefficient does not have a strong causal interpretation, because donation decisions and donation amounts are jointly determined and the sample is selected on gave == 1.\n\n\nDistribution of Donations Among Donors\n\ntreatment_donors = data[(data[\"treatment\"] == 1) & (data[\"gave\"] == 1)]\ncontrol_donors = data[(data[\"treatment\"] == 0) & (data[\"gave\"] == 1)]\n\nmean_treat = treatment_donors[\"amount\"].mean()\nmean_control = control_donors[\"amount\"].mean()\n\nfig, axes = plt.subplots(1, 2, figsize=(14, 5), sharey=True)\n\nsns.histplot(treatment_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[0])\naxes[0].axvline(mean_treat, color='red', linestyle='--', label=f\"Mean: ${mean_treat:.2f}\")\naxes[0].set_title(\"Treatment Group\")\naxes[0].set_xlabel(\"Donation Amount\")\naxes[0].set_ylabel(\"Frequency\")\naxes[0].legend()\n\nsns.histplot(control_donors[\"amount\"], bins=30, color=\"orange\", edgecolor=\"black\", ax=axes[1])\naxes[1].axvline(mean_control, color='red', linestyle='--', label=f\"Mean: ${mean_control:.2f}\")\naxes[1].set_title(\"Control Group\")\naxes[1].set_xlabel(\"Donation Amount\")\naxes[1].legend()\n\nplt.tight_layout()\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nTwo histograms show the distribution of donation amounts for the treatment group and control group, restricted to those who donated. The vertical red line marks the average for each group:\nTreatment Mean: ~$43.87\nControl Mean: ~$45.54\nBoth distributions are right-skewed, with most donors giving small amounts and a few contributing large sums. There is no visible shift in the average due to the treatment.\n\n\nConclusion\nThese analyses support the idea that matching offers increase response rate, but do not change how much people give once they’ve decided to donate. This distinction is important for fundraising strategies: matching may motivate more people to give, but it doesn’t necessarily increase per-donor revenue."
  },
  {
    "objectID": "blog/project1/hw1_questions.html#simulation-experiment",
    "href": "blog/project1/hw1_questions.html#simulation-experiment",
    "title": "A Replication of Karlan and List (2007)",
    "section": "Simulation Experiment",
    "text": "Simulation Experiment\nAs a reminder of how the t-statistic “works,” in this section I use simulation to demonstrate the Law of Large Numbers and the Central Limit Theorem.\nSuppose the true distribution of respondents who do not get a charitable donation match is Bernoulli with probability p=0.018 that a donation is made.\nFurther suppose that the true distribution of respondents who do get a charitable donation match of any size is Bernoulli with probability p=0.022 that a donation is made.\n\nLaw of Large Numbers\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\nn_sim = 10000\nnp.random.seed(42)\n# Simulate 10,000 binary outcomes for each group\ncontrol_sim = np.random.binomial(1, p_control, 100000)\ntreatment_sim = np.random.binomial(1, p_treatment, n_sim)\n\n# Vector of differences\ndiffs = treatment_sim - control_sim[:10000]\n\n# Cumulative average of the differences\ncum_avg = np.cumsum(diffs) / np.arange(1, len(diffs) + 1)\n\n# Plot\nplt.figure(figsize=(8, 4))\nplt.plot(cum_avg, label='Cumulative Average Difference', color='orange')\nplt.axhline(y=p_treatment - p_control, color='red', linestyle='--', label='True Difference (0.004)')\nplt.title(\"Law of Large Numbers: Cumulative Average of Simulated Differences\")\nplt.xlabel(\"Simulation Iteration\")\nplt.ylabel(\"Cumulative Average Difference\")\nplt.legend()\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThis plot demonstrates the Law of Large Numbers. As we simulate more and more observations, the cumulative average of the differences converges toward the true mean difference (0.004). Initially, there’s randomness and fluctuation, but the line stabilizes as the number of observations increases.\nThis convergence is the foundation for using sample averages to estimate population parameters and underpins why large sample sizes give us more reliable estimates in experiments.\n\n\nCentral Limit Theorem\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n# Parameters\np_control = 0.018\np_treatment = 0.022\ntrue_diff = p_treatment - p_control\nsample_sizes = [50, 200, 500, 1000]\nn_reps = 1000\n\nnp.random.seed(42)\nhistograms = {}\n\n# Simulate average differences for each sample size\nfor n in sample_sizes:\n    diffs = []\n    for _ in range(n_reps):\n        control_sample = np.random.binomial(1, p_control, n)\n        treatment_sample = np.random.binomial(1, p_treatment, n)\n        diff = treatment_sample.mean() - control_sample.mean()\n        diffs.append(diff)\n    histograms[n] = diffs\n\n# Plot histograms\nfig, axes = plt.subplots(2, 2, figsize=(12, 8))\naxes = axes.flatten()\n\nfor i, n in enumerate(sample_sizes):\n    axes[i].hist(histograms[n], bins=30, color='skyblue', edgecolor='black')\n    axes[i].axvline(0, color='red', linestyle='--', label=\"Zero Reference\")\n    axes[i].axvline(true_diff, color='green', linestyle='--', label=\"True Difference (0.004)\")\n    axes[i].set_title(f\"Sample Size = {n}\")\n    axes[i].set_xlabel(\"Mean Difference\")\n    axes[i].set_ylabel(\"Frequency\")\n    axes[i].legend()\n\nplt.tight_layout()\nplt.show()\n\n\n\n\n\n\n\n\nThese four histograms illustrate how the sampling distribution of the difference in means behaves at increasing sample sizes:\nAt n = 50, the distribution is quite wide and skewed. Zero is within the center-ish but not tightly.\nAs sample size increases, the distribution becomes tighter, more symmetric, and centered.\nBy n = 1000, the distribution of average differences closely resembles a normal distribution centered near the true mean difference (0.004).\nThis is a direct illustration of the Central Limit Theorem:\nAs sample size increases, the distribution of the sample mean difference becomes approximately normal, regardless of the original distribution shape.\nAlso note: zero shifts from being more “middle-ish” in smaller samples to being closer to the tail as the signal (the true effect) dominates the noise."
  },
  {
    "objectID": "blog/project2/hw2_questions.html",
    "href": "blog/project2/hw2_questions.html",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n\n\nWe begin by loading the dataset and conducting exploratory analysis to compare the number of patents awarded to Blueprinty customers versus non-customers. This gives us an early sense of whether customers tend to be more successful and whether that success could be linked to the use of Blueprinty’s software.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe dataset includes 1,500 engineering firms. For each firm, we have the number of patents awarded in the past five years, their geographic region, age since incorporation, and a binary indicator for whether they are a Blueprinty customer (iscustomer = 1) or not (iscustomer = 0). The .head() command confirms the dataset structure and that key variables are available.\n\nNext, we visualize the distribution of patent counts to see if there’s any notable difference between customers and non-customers.\n\nsns.set(style=\"whitegrid\")\n\n# Histogram of patents by customer status (stacked)\nplt.figure(figsize=(10, 5))\nsns.histplot(\n    data=df,\n    x=\"patents\",\n    hue=\"iscustomer\",\n    bins=30,\n    multiple=\"stack\",  # stack instead of dodge\n    palette=\"muted\"\n)\nplt.title(\"Stacked Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\n\n\n\n\n\n\n\nThis histogram shows the number of patents across the two groups. We observe that both Blueprinty customers and non-customers most frequently hold between 2 and 4 patents. However, Blueprinty customers (in blue) are more heavily represented in the higher patent count ranges, particularly above 6. This visual evidence suggests a potential positive association between being a customer and patent success.\n\nTo quantify the difference seen in the histogram, we calculate the average number of patents for each group.\n\n# Mean number of patents by customer status\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nThe mean patent count is: - 3.47 patents for non-customers - 4.13 patents for customers\nThis represents a ~19% increase in average patent output for Blueprinty customers. While this is an encouraging finding for Blueprinty’s marketing claim, it’s important to recognize that this difference is purely descriptive. It does not account for other factors—such as firm age or regional clustering—that may be influencing patent success. We’ll need to explore these factors next and ultimately use a regression model to properly isolate the effect of customer status.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n# Boxplot of age by customer status\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=df, x=\"iscustomer\", y=\"age\", palette=\"Set2\")\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = No, 1 = Yes)\")\nplt.ylabel(\"Age\")\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot compares the age distribution of customer and non-customer firms. While there is substantial overlap, customers appear to be slightly older on average. There are also more older outliers among customers. This may indicate that more established (older) firms are more likely to adopt Blueprinty’s software, potentially due to larger budgets or greater administrative capacity.\n\n\nregion_order = (\n    df[\"region\"]\n    .value_counts(ascending=False)\n    .index\n)\n\n# Plot with reordered regions\nplt.figure(figsize=(10, 5))\nsns.countplot(\n    data=df,\n    x=\"region\",\n    hue=\"iscustomer\",\n    palette=\"Set1\",\n    order=region_order  # Apply the sorted order here\n)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart reveals striking geographic variation in customer status. The Northeast region stands out, with more Blueprinty customers than non-customers, unlike all other regions where non-customers dominate. This indicates that the Northeast may be a key market for Blueprinty, and region is likely a major confounding factor. Any causal claim about the effect of the software must adjust for these regional differences.\n\n\n# Average age by customer status\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nHere, we calculate the average firm age by customer status. On average: - Non-customers are 26.1 years old - Customers are 26.9 years old\nAlthough the difference is modest (~0.8 years), it reinforces the trend observed in the boxplot: customers tend to be slightly older, suggesting maturity and longevity might correlate with software adoption.\n\n\n# Proportion of customers by region\npd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize='index')\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n0.834821\n0.165179\n\n\nNortheast\n0.454243\n0.545757\n\n\nNorthwest\n0.844920\n0.155080\n\n\nSouth\n0.816754\n0.183246\n\n\nSouthwest\n0.824916\n0.175084\n\n\n\n\n\n\n\nThis table provides the proportion of customers within each region: - In the Northeast, over 54% of firms are customers - In every other region, only 15–18% of firms are customers\nThese proportions confirm that Blueprinty’s customer base is heavily concentrated in the Northeast. This supports the idea that customer status is not randomly distributed, and further emphasizes the need to control for region when analyzing the effect of the software on patenting outcomes.\n\n\n\n\nOur exploratory analysis shows that Blueprinty customers: - Have a slightly higher average age - Are disproportionately located in the Northeast\nThese differences suggest customer status is correlated with observable firm characteristics, making it essential to control for these factors in any model that attempts to evaluate the effect of using Blueprinty software. Ignoring age or region could lead to biased conclusions, mistakenly attributing differences in patent counts to the software when they may in fact reflect geography or firm maturity. ### Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "href": "blog/project2/hw2_questions.html#blueprinty-case-study",
    "title": "Poisson Regression Examples",
    "section": "",
    "text": "Blueprinty is a small firm that makes software for developing blueprints specifically for submitting patent applications to the US patent office. Their marketing team would like to make the claim that patent applicants using Blueprinty’s software are more successful in getting their patent applications approved. Ideal data to study such an effect might include the success rate of patent applications before using Blueprinty’s software and after using it. Unfortunately, such data is not available.\nHowever, Blueprinty has collected data on 1,500 mature (non-startup) engineering firms. The data include each firm’s number of patents awarded over the last 5 years, regional location, age since incorporation, and whether or not the firm uses Blueprinty’s software. The marketing team would like to use this data to make the claim that firms using Blueprinty’s software are more successful in getting their patent applications approved.\n\n\n\ntodo: Read in data.\ntodo: Compare histograms and means of number of patents by customer status. What do you observe?\n\n\n\nWe begin by loading the dataset and conducting exploratory analysis to compare the number of patents awarded to Blueprinty customers versus non-customers. This gives us an early sense of whether customers tend to be more successful and whether that success could be linked to the use of Blueprinty’s software.\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Load the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf.head()\n\n\n\n\n\n\n\n\npatents\nregion\nage\niscustomer\n\n\n\n\n0\n0\nMidwest\n32.5\n0\n\n\n1\n3\nSouthwest\n37.5\n0\n\n\n2\n4\nNorthwest\n27.0\n1\n\n\n3\n3\nNortheast\n24.5\n0\n\n\n4\n3\nSouthwest\n37.0\n0\n\n\n\n\n\n\n\nThe dataset includes 1,500 engineering firms. For each firm, we have the number of patents awarded in the past five years, their geographic region, age since incorporation, and a binary indicator for whether they are a Blueprinty customer (iscustomer = 1) or not (iscustomer = 0). The .head() command confirms the dataset structure and that key variables are available.\n\nNext, we visualize the distribution of patent counts to see if there’s any notable difference between customers and non-customers.\n\nsns.set(style=\"whitegrid\")\n\n# Histogram of patents by customer status (stacked)\nplt.figure(figsize=(10, 5))\nsns.histplot(\n    data=df,\n    x=\"patents\",\n    hue=\"iscustomer\",\n    bins=30,\n    multiple=\"stack\",  # stack instead of dodge\n    palette=\"muted\"\n)\nplt.title(\"Stacked Histogram of Patents by Customer Status\")\nplt.xlabel(\"Number of Patents\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1075: FutureWarning:\n\nWhen grouping with a length-1 list-like, you will need to pass a length-1 tuple to get_group in a future version of pandas. Pass `(name,)` instead of `name` to silence this warning.\n\n\n\n\n\n\n\n\n\n\nThis histogram shows the number of patents across the two groups. We observe that both Blueprinty customers and non-customers most frequently hold between 2 and 4 patents. However, Blueprinty customers (in blue) are more heavily represented in the higher patent count ranges, particularly above 6. This visual evidence suggests a potential positive association between being a customer and patent success.\n\nTo quantify the difference seen in the histogram, we calculate the average number of patents for each group.\n\n# Mean number of patents by customer status\ndf.groupby(\"iscustomer\")[\"patents\"].mean()\n\niscustomer\n0    3.473013\n1    4.133056\nName: patents, dtype: float64\n\n\nThe mean patent count is: - 3.47 patents for non-customers - 4.13 patents for customers\nThis represents a ~19% increase in average patent output for Blueprinty customers. While this is an encouraging finding for Blueprinty’s marketing claim, it’s important to recognize that this difference is purely descriptive. It does not account for other factors—such as firm age or regional clustering—that may be influencing patent success. We’ll need to explore these factors next and ultimately use a regression model to properly isolate the effect of customer status.\nBlueprinty customers are not selected at random. It may be important to account for systematic differences in the age and regional location of customers vs non-customers.\ntodo: Compare regions and ages by customer status. What do you observe?\n\n# Boxplot of age by customer status\nplt.figure(figsize=(6, 5))\nsns.boxplot(data=df, x=\"iscustomer\", y=\"age\", palette=\"Set2\")\nplt.title(\"Age Distribution by Customer Status\")\nplt.xlabel(\"Customer Status (0 = No, 1 = Yes)\")\nplt.ylabel(\"Age\")\nplt.show()\n\n\n\n\n\n\n\n\nThis boxplot compares the age distribution of customer and non-customer firms. While there is substantial overlap, customers appear to be slightly older on average. There are also more older outliers among customers. This may indicate that more established (older) firms are more likely to adopt Blueprinty’s software, potentially due to larger budgets or greater administrative capacity.\n\n\nregion_order = (\n    df[\"region\"]\n    .value_counts(ascending=False)\n    .index\n)\n\n# Plot with reordered regions\nplt.figure(figsize=(10, 5))\nsns.countplot(\n    data=df,\n    x=\"region\",\n    hue=\"iscustomer\",\n    palette=\"Set1\",\n    order=region_order  # Apply the sorted order here\n)\nplt.title(\"Region Distribution by Customer Status\")\nplt.xlabel(\"Region\")\nplt.ylabel(\"Count\")\nplt.legend(title=\"Customer Status\", labels=[\"Non-Customer\", \"Customer\"])\nplt.xticks(rotation=45)\nplt.show()\n\n\n\n\n\n\n\n\nThis bar chart reveals striking geographic variation in customer status. The Northeast region stands out, with more Blueprinty customers than non-customers, unlike all other regions where non-customers dominate. This indicates that the Northeast may be a key market for Blueprinty, and region is likely a major confounding factor. Any causal claim about the effect of the software must adjust for these regional differences.\n\n\n# Average age by customer status\ndf.groupby(\"iscustomer\")[\"age\"].mean()\n\niscustomer\n0    26.101570\n1    26.900208\nName: age, dtype: float64\n\n\nHere, we calculate the average firm age by customer status. On average: - Non-customers are 26.1 years old - Customers are 26.9 years old\nAlthough the difference is modest (~0.8 years), it reinforces the trend observed in the boxplot: customers tend to be slightly older, suggesting maturity and longevity might correlate with software adoption.\n\n\n# Proportion of customers by region\npd.crosstab(df[\"region\"], df[\"iscustomer\"], normalize='index')\n\n\n\n\n\n\n\niscustomer\n0\n1\n\n\nregion\n\n\n\n\n\n\nMidwest\n0.834821\n0.165179\n\n\nNortheast\n0.454243\n0.545757\n\n\nNorthwest\n0.844920\n0.155080\n\n\nSouth\n0.816754\n0.183246\n\n\nSouthwest\n0.824916\n0.175084\n\n\n\n\n\n\n\nThis table provides the proportion of customers within each region: - In the Northeast, over 54% of firms are customers - In every other region, only 15–18% of firms are customers\nThese proportions confirm that Blueprinty’s customer base is heavily concentrated in the Northeast. This supports the idea that customer status is not randomly distributed, and further emphasizes the need to control for region when analyzing the effect of the software on patenting outcomes.\n\n\n\n\nOur exploratory analysis shows that Blueprinty customers: - Have a slightly higher average age - Are disproportionately located in the Northeast\nThese differences suggest customer status is correlated with observable firm characteristics, making it essential to control for these factors in any model that attempts to evaluate the effect of using Blueprinty software. Ignoring age or region could lead to biased conclusions, mistakenly attributing differences in patent counts to the software when they may in fact reflect geography or firm maturity. ### Estimation of Simple Poisson Model\nSince our outcome variable of interest can only be small integer values per a set unit of time, we can use a Poisson density to model the number of patents awarded to each engineering firm over the last 5 years. We start by estimating a simple Poisson model via Maximum Likelihood.\ntodo: Write down mathematically the likelihood for \\(Y \\sim \\text{Poisson}(\\lambda)\\). Note that \\(f(Y|\\lambda) = e^{-\\lambda}\\lambda^Y/Y!\\).\ntodo: Code the likelihood (or log-likelihood) function for the Poisson model. This is a function of lambda and Y. For example:\npoisson_loglikelihood &lt;- function(lambda, Y){\n   ...\n}\ntodo: Use your function to plot lambda on the horizontal axis and the likelihood (or log-likelihood) on the vertical axis for a range of lambdas (use the observed number of patents as the input for Y).\ntodo: If you’re feeling mathematical, take the first derivative of your likelihood or log-likelihood, set it equal to zero and solve for lambda. You will find lambda_mle is Ybar, which “feels right” because the mean of a Poisson distribution is lambda.\ntodo: Find the MLE by optimizing your likelihood function with optim() in R or sp.optimize() in Python."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nAirBnB is a popular platform for booking short-term rentals. In March 2017, students Annika Awad, Evan Lebo, and Anna Linden scraped of 40,000 Airbnb listings from New York City. The data include the following variables:\n\n\n\n\n\n\nVariable Definitions\n\n\n\n\n\n- `id` = unique ID number for each unit\n- `last_scraped` = date when information scraped\n- `host_since` = date when host first listed the unit on Airbnb\n- `days` = `last_scraped` - `host_since` = number of days the unit has been listed\n- `room_type` = Entire home/apt., Private room, or Shared room\n- `bathrooms` = number of bathrooms\n- `bedrooms` = number of bedrooms\n- `price` = price per night (dollars)\n- `number_of_reviews` = number of reviews for the unit on Airbnb\n- `review_scores_cleanliness` = a cleanliness score from reviews (1-10)\n- `review_scores_location` = a \"quality of location\" score from reviews (1-10)\n- `review_scores_value` = a \"quality of value\" score from reviews (1-10)\n- `instant_bookable` = \"t\" if instantly bookable, \"f\" if not\n\n\n\ntodo: Assume the number of reviews is a good proxy for the number of bookings. Perform some exploratory data analysis to get a feel for the data, handle or drop observations with missing values on relevant variables, build one or more models (e.g., a poisson regression model for the number of bookings as proxied by the number of reviews), and interpret model coefficients to describe variation in the number of reviews as a function of the variables provided. ## AirBnB Case Study: Poisson Regression\n\n\nData Preparation\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load the dataset\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Keep relevant variables and drop rows with missing data\ncols = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"number_of_reviews\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[cols].dropna()\n\n# Convert categorical variable to numeric\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"f\": 0, \"t\": 1})\n\nWe selected key predictors that are likely to influence bookings. This includes listing price, type of room offered, number of bedrooms/bathrooms, various review scores, and whether the listing allows instant booking. We removed rows with missing values to ensure model stability.\n\n\nReview Count Distribution\n\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nReview counts are highly skewed, with many listings having fewer than 50 reviews and a long right tail — a classic case for using a Poisson model.\n\n\nBoxplot: Reviews by Room Type\n\nsns.boxplot(data=df_clean, x=\"room_type\", y=\"number_of_reviews\")\nplt.ylim(0, 100)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\nEntire homes tend to receive the most reviews, suggesting higher demand compared to shared and private rooms.\n\n\nCorrelation Heatmap\n\nnumeric_cols = [\"price\", \"bedrooms\", \"bathrooms\", \n                \"review_scores_cleanliness\", \"review_scores_location\", \n                \"review_scores_value\", \"number_of_reviews\"]\nsns.heatmap(df_clean[numeric_cols].corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Between Variables\")\nplt.show()\n\n\n\n\n\n\n\n\nThis helps us check for multicollinearity and informs variable selection in our regression model.\n\n\nSummary Statistics\n\ndf_clean.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nbathrooms\n30160.0\n1.122132\n0.384916\n0.0\n1.0\n1.0\n1.0\n6.0\n\n\nbedrooms\n30160.0\n1.151459\n0.699010\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n30160.0\n140.206863\n188.392314\n10.0\n70.0\n103.0\n169.0\n10000.0\n\n\nnumber_of_reviews\n30160.0\n21.170889\n32.007541\n1.0\n3.0\n8.0\n26.0\n421.0\n\n\nreview_scores_cleanliness\n30160.0\n9.201724\n1.114261\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30160.0\n9.415351\n0.843185\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30160.0\n9.333952\n0.900472\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ninstant_bookable\n30160.0\n0.196187\n0.397118\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nThis table gives a statistical overview of the cleaned dataset. For example, we can see the median number of reviews, typical price levels, and average review scores. It also helps confirm the need for count modeling, given the range and skew of the number_of_reviews variable.\n\n\nModel Specification and Fitting\n\n# Create design matrix for Poisson regression\nX = dmatrix(\n    \"1 + price + bedrooms + bathrooms + review_scores_cleanliness + \"\n    \"review_scores_location + review_scores_value + C(room_type) + instant_bookable\",\n    data=df_clean,\n    return_type=\"dataframe\"\n)\nY = df_clean[\"number_of_reviews\"]\n\n# Fit Poisson model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nThe model uses a log link function, where the log of the expected number of reviews is modeled as a linear function of the predictors. This means that coefficients represent multiplicative effects on the count of reviews. For example, a coefficient of 0.1 implies approximately a 10.5% increase in expected review count (exp(0.1) ≈ 1.105).\n\n\nPoisson Regression Results (Formatted)\n\n# Extract and clean summary table\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"z\": \"z-value\",\n    \"P&gt;|z|\": \"p-value\",\n    \"[0.025\": \"95% CI Lower\",\n    \"0.975]\": \"95% CI Upper\"\n}, inplace=True)\n\n# Format output\nformatted_summary = summary_table[[\n    \"Variable\", \"Coefficient\", \"Std. Error\", \"z-value\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n3.5725\n0.0160\n223.2145\n0.0000\n3.5411\n3.6039\n\n\n1\nC(room_type)[T.Private room]\n-0.0145\n0.0027\n-5.3104\n0.0000\n-0.0199\n-0.0092\n\n\n2\nC(room_type)[T.Shared room]\n-0.2519\n0.0086\n-29.2286\n0.0000\n-0.2688\n-0.2350\n\n\n3\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n-0.0000\n0.0000\n\n\n4\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n0.0710\n0.0788\n\n\n5\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n-0.1313\n-0.1167\n\n\n6\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n0.1103\n0.1161\n\n\n7\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n-0.0799\n-0.0736\n\n\n8\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n-0.0951\n-0.0880\n\n\n9\ninstant_bookable\n0.3344\n0.0029\n115.7477\n0.0000\n0.3287\n0.3401\n\n\n\n\n\n\n\n\n\nInterpretation\nThe coefficients tell us how listing features are associated with review count, holding other variables constant:\n\nIntercept: The baseline log count for an average listing (Entire home, not instant bookable, average scores).\nroom_type (Shared room): A strong negative effect. These listings get ~22% as many reviews as entire home listings (exp(-0.25) ≈ 0.78). This aligns with expectations—shared rooms are less popular.\nroom_type (Private room): Also shows a slight negative effect, but far smaller than shared rooms.\ninstant_bookable: This feature has a large positive effect—instant booking listings are expected to get ~40% more reviews, all else equal (exp(0.33) ≈ 1.39). This underscores the importance of convenience to guests.\ncleanliness score: Every 1-point increase in cleanliness adds over 11% more reviews, highlighting the value of positive guest experiences.\nbedrooms: Positively associated with reviews, possibly because larger listings serve more guests or accommodate longer stays.\nbathrooms: Shows a surprising negative coefficient, possibly reflecting multicollinearity with bedrooms or nonlinear effects.\nprice: The effect is slightly negative but not statistically significant at the 5% level. This may suggest that higher prices deter bookings only marginally, or other variables are absorbing the effect.\nlocation/value scores: Unexpectedly negative; this might reflect reverse causality (low-activity listings getting inflated scores), or correlation with other quality measures.\n\n\n\nModel Implications\nThe results show that instant bookability, room type, cleanliness, and listing size all play major roles in driving bookings/reviews. In particular, allowing instant booking and maintaining high cleanliness ratings seem to be key strategies for increasing engagement.\nHowever, some findings warrant further investigation—especially the negative coefficients on review score variables. These may reflect issues like multicollinearity, endogeneity, or nonlinear relationships not captured by this simple model.\nA natural next step would be to explore: - Interaction effects (e.g. cleanliness × room type) - Nonlinear terms (e.g. log(price)) - Alternative models like Negative Binomial to handle overdispersion\n\n\nConclusion\nPoisson regression provides a valuable framework for modeling Airbnb review counts as a function of listing characteristics. This analysis offers actionable insights for hosts seeking to increase visibility and bookings, while also highlighting areas where further modeling could improve interpretability and accuracy."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "href": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nTo model the number of patents awarded per firm, we assume the data follow a Poisson distribution with parameter λ. This is appropriate for count data observed over a fixed time period. In this section, we define the Poisson log-likelihood, visualize it, and estimate the maximum likelihood value of λ both analytically and numerically.\n\n\nStep 1: Load and Inspect the Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Load patent data\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\nWe load the blueprinty.csv dataset and extract the number of patents for each firm, stored in the variable Y.\n\n\n\nStep 2: Define the Log-Likelihood Function\n\n# Negative log-likelihood function (for minimization)\ndef poisson_neg_log_likelihood(lmbda, Y):\n    return -np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nThis is the log-likelihood of the Poisson model (negated for use with optimization). We use gammaln(Y + 1) for numerical stability in place of log(Y!).\n\n\n\nStep 3: Visualize the Log-Likelihood Curve\n\nlambdas = np.linspace(0.1, 10, 200)\nlog_liks = [-poisson_neg_log_likelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks, label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label=\"Sample Mean (Ȳ)\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis curve shows how the log-likelihood varies with different values of λ. The red line indicates the sample mean, which aligns closely with the peak of the curve.\n\n\n\nStep 4: Analytical MLE for Poisson\n\nlambda_mle_analytical = np.mean(Y)\nlambda_mle_analytical\n\n3.6846666666666668\n\n\n\nThis result comes from solving the first derivative of the log-likelihood with respect to λ, setting it to zero:\n\\[ \\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_i \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = 0 \\] Solving gives: \\[ \\hat{\\lambda}_{MLE} = \\bar{Y} \\]\n\nThis derivation confirms that the maximum likelihood estimator for λ is simply the mean of the observed data.\n\n\n\nStep 5: Estimate λ Numerically\n\n# Find lambda using numerical optimization\nopt_result = minimize(lambda l: poisson_neg_log_likelihood(l, Y), x0=[1.0], bounds=[(0.001, None)])\nlambda_mle_numerical = opt_result.x[0]\nlambda_mle_numerical\n\n3.684666485763343\n\n\nUsing numerical optimization, we minimize the negative log-likelihood to obtain λ̂. The result closely matches the sample mean, confirming our analytical solution.\n\n\n\nConclusion\nThis process demonstrates how to derive and estimate the Poisson MLE both mathematically and computationally. We verified that: - The log-likelihood peaks at the sample mean - The analytical and numerical MLEs are identical This lays a strong foundation for moving into more complex models like Poisson regression.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model",
    "href": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load and prepare the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create design matrix X with intercept, age, age squared, region dummies, and customer status\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n# Display summary\npoisson_results.summary()\n\n\nGeneralized Linear Model Regression Results\n\n\nDep. Variable:\npatents\nNo. Observations:\n1500\n\n\nModel:\nGLM\nDf Residuals:\n1492\n\n\nModel Family:\nPoisson\nDf Model:\n7\n\n\nLink Function:\nLog\nScale:\n1.0000\n\n\nMethod:\nIRLS\nLog-Likelihood:\n-3258.1\n\n\nDate:\nSun, 04 May 2025\nDeviance:\n2143.3\n\n\nTime:\n13:48:09\nPearson chi2:\n2.07e+03\n\n\nNo. Iterations:\n5\nPseudo R-squ. (CS):\n0.1360\n\n\nCovariance Type:\nnonrobust\n\n\n\n\n\n\n\n\n\n\n\ncoef\nstd err\nz\nP&gt;|z|\n[0.025\n0.975]\n\n\nIntercept\n-0.5089\n0.183\n-2.778\n0.005\n-0.868\n-0.150\n\n\nC(region)[T.Northeast]\n0.0292\n0.044\n0.669\n0.504\n-0.056\n0.115\n\n\nC(region)[T.Northwest]\n-0.0176\n0.054\n-0.327\n0.744\n-0.123\n0.088\n\n\nC(region)[T.South]\n0.0566\n0.053\n1.074\n0.283\n-0.047\n0.160\n\n\nC(region)[T.Southwest]\n0.0506\n0.047\n1.072\n0.284\n-0.042\n0.143\n\n\nage\n0.1486\n0.014\n10.716\n0.000\n0.121\n0.176\n\n\nage2\n-0.0030\n0.000\n-11.513\n0.000\n-0.003\n-0.002\n\n\niscustomer\n0.2076\n0.031\n6.719\n0.000\n0.147\n0.268\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "href": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-1",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load and prepare the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create design matrix X with intercept, age, age squared, region dummies, and customer status\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\n# Extract summary data\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    'index': 'Variable',\n    'Coef.': 'Coefficient',\n    'Std.Err.': 'Std. Error',\n    'z': 'z-value',\n    'P&gt;|z|': 'p-value',\n    '[0.025': '95% CI Lower',\n    '0.975]': '95% CI Upper'\n}, inplace=True)\n\n# Keep only selected columns and round values\nformatted_summary = summary_table[[\n    'Variable', 'Coefficient', 'Std. Error', 'z-value', 'p-value', '95% CI Lower', '95% CI Upper'\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nC(region)[T.Northeast]\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n2\nC(region)[T.Northwest]\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n3\nC(region)[T.South]\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n4\nC(region)[T.Southwest]\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n5\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n6\nage2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n7\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study-1",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study-1",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study",
    "text": "AirBnB Case Study\n\nIntroduction\nWe assume that the number of reviews is a reasonable proxy for the number of bookings. While not every booking results in a review, listings with more bookings are generally expected to accumulate more reviews over time. Therefore, analyzing review counts can help us understand what drives customer engagement."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-2",
    "href": "blog/project2/hw2_questions.html#estimation-of-poisson-regression-model-2",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Poisson Regression Model",
    "text": "Estimation of Poisson Regression Model\n\nimport numpy as np\nimport pandas as pd\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load and prepare the data\ndf = pd.read_csv(\"blueprinty.csv\")\ndf[\"age2\"] = df[\"age\"] ** 2\n\n# Create design matrix X with intercept, age, age squared, region dummies, and customer status\nX = dmatrix(\"1 + age + age2 + C(region) + iscustomer\", data=df, return_type='dataframe')\nY = df[\"patents\"]\n\n# Fit Poisson regression\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\nPoisson Regression Results (Formatted)\n\n# Extract summary data\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    'index': 'Variable',\n    'Coef.': 'Coefficient',\n    'Std.Err.': 'Std. Error',\n    'z': 'z-value',\n    'P&gt;|z|': 'p-value',\n    '[0.025': '95% CI Lower',\n    '0.975]': '95% CI Upper'\n}, inplace=True)\n\n# Keep only selected columns and round values\nformatted_summary = summary_table[[\n    'Variable', 'Coefficient', 'Std. Error', 'z-value', 'p-value', '95% CI Lower', '95% CI Upper'\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n-0.5089\n0.1832\n-2.7783\n0.0055\n-0.8679\n-0.1499\n\n\n1\nC(region)[T.Northeast]\n0.0292\n0.0436\n0.6686\n0.5037\n-0.0563\n0.1147\n\n\n2\nC(region)[T.Northwest]\n-0.0176\n0.0538\n-0.3268\n0.7438\n-0.1230\n0.0878\n\n\n3\nC(region)[T.South]\n0.0566\n0.0527\n1.0740\n0.2828\n-0.0467\n0.1598\n\n\n4\nC(region)[T.Southwest]\n0.0506\n0.0472\n1.0716\n0.2839\n-0.0419\n0.1431\n\n\n5\nage\n0.1486\n0.0139\n10.7162\n0.0000\n0.1214\n0.1758\n\n\n6\nage2\n-0.0030\n0.0003\n-11.5132\n0.0000\n-0.0035\n-0.0025\n\n\n7\niscustomer\n0.2076\n0.0309\n6.7192\n0.0000\n0.1470\n0.2681\n\n\n\n\n\n\n\n\n\nSimulate Counterfactual Scenarios (Effect of Blueprinty)\n\n# Create two copies of the X matrix:\n# One where everyone is NOT a customer\nX_0 = X.copy()\nX_0[\"iscustomer\"] = 0\n\n# One where everyone IS a customer\nX_1 = X.copy()\nX_1[\"iscustomer\"] = 1\n\n# Predicted number of patents under both scenarios\ny_pred_0 = poisson_results.predict(X_0)\ny_pred_1 = poisson_results.predict(X_1)\n\n# Average difference in predicted patents\neffect_estimate = np.mean(y_pred_1 - y_pred_0)\neffect_estimate\n\n0.7927680710452927\n\n\n\n\n\nInterpretation:\nThe coefficient on iscustomer is statistically significant (p &lt; 0.001) and positive. This suggests that firms using Blueprinty’s software are associated with more patents awarded, even after controlling for age, age squared, and region.\nThe average predicted difference in patent counts when simulating Blueprinty usage versus non-usage across all firms is r round(effect_estimate, 2) additional patents per firm over 5 years."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#airbnb-case-study-poisson-regression",
    "href": "blog/project2/hw2_questions.html#airbnb-case-study-poisson-regression",
    "title": "Poisson Regression Examples",
    "section": "AirBnB Case Study: Poisson Regression",
    "text": "AirBnB Case Study: Poisson Regression\n\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\n# Load the dataset\ndf = pd.read_csv(\"airbnb.csv\")\n\n# Keep relevant variables and drop rows with missing data\ncols = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"number_of_reviews\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[cols].dropna()\n\n# Convert categorical variable to numeric\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"f\": 0, \"t\": 1})\n\n# Create design matrix for Poisson regression\nX = dmatrix(\n    \"1 + price + bedrooms + bathrooms + review_scores_cleanliness + \"\n    \"review_scores_location + review_scores_value + C(room_type) + instant_bookable\",\n    data=df_clean,\n    return_type=\"dataframe\"\n)\nY = df_clean[\"number_of_reviews\"]\n\n# Fit Poisson model\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\n\nPoisson Regression Results (Formatted)\n\n# Extract and clean summary table\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"z\": \"z-value\",\n    \"P&gt;|z|\": \"p-value\",\n    \"[0.025\": \"95% CI Lower\",\n    \"0.975]\": \"95% CI Upper\"\n}, inplace=True)\n\n# Format output\nformatted_summary = summary_table[[\n    \"Variable\", \"Coefficient\", \"Std. Error\", \"z-value\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n3.5725\n0.0160\n223.2145\n0.0000\n3.5411\n3.6039\n\n\n1\nC(room_type)[T.Private room]\n-0.0145\n0.0027\n-5.3104\n0.0000\n-0.0199\n-0.0092\n\n\n2\nC(room_type)[T.Shared room]\n-0.2519\n0.0086\n-29.2286\n0.0000\n-0.2688\n-0.2350\n\n\n3\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n-0.0000\n0.0000\n\n\n4\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n0.0710\n0.0788\n\n\n5\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n-0.1313\n-0.1167\n\n\n6\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n0.1103\n0.1161\n\n\n7\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n-0.0799\n-0.0736\n\n\n8\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n-0.0951\n-0.0880\n\n\n9\ninstant_bookable\n0.3344\n0.0029\n115.7477\n0.0000\n0.3287\n0.3401\n\n\n\n\n\n\n\n\n\n\nInterpretation\n\nroom_type (Shared room) has a large negative coefficient — these listings receive fewer reviews than entire homes.\ninstant_bookable has a strong positive effect — likely increasing bookings.\ncleanliness score positively predicts reviews.\nvalue/location scores unexpectedly have negative coefficients, possibly due to multicollinearity.\nbedrooms is positively associated with reviews, while bathrooms has a small negative effect."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-2",
    "href": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model-2",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nTo analyze the number of patents awarded to engineering firms, we assume the data follows a Poisson distribution. This is appropriate since patent counts are non-negative integers and often modeled using count distributions. We define the Poisson log-likelihood and use both analytical and numerical methods to estimate the parameter λ, which represents the average rate of events (patents per firm).\n\n\nStep 1: Load Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Load patent data\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\nWe begin by loading the dataset. Our outcome of interest is patents, the number of patents awarded over five years, stored as the array Y.\n\n\n\nStep 2: Define the Poisson Log-Likelihood\n\n# Define negative log-likelihood function for optimization\ndef poisson_neg_log_likelihood(lmbda, Y):\n    return -np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nThis function defines the negative log-likelihood of the Poisson model. We use gammaln(Y + 1) instead of log(Y!) for numerical stability. The log-likelihood is a function of λ and the observed data Y.\n\n\n\nStep 3: Plot the Log-Likelihood Curve\n\nlambdas = np.linspace(0.1, 10, 200)\nlog_liks = [-poisson_neg_log_likelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks, label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label=\"Sample Mean (Ȳ)\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThe plot above shows the log-likelihood as a function of λ. The peak of this curve corresponds to the maximum likelihood estimate (MLE) of λ. We also mark the sample mean of Y, which visually aligns with the MLE—a known property of the Poisson distribution.\n\n\n\nStep 4: Derive the Analytical MLE\n\nlambda_mle_analytical = np.mean(Y)\nlambda_mle_analytical\n\n3.6846666666666668\n\n\nFrom calculus, we know that the MLE for λ in a Poisson distribution is:\n\\[\n\\hat{\\lambda}_{MLE} = \\bar{Y}\n\\]\nThis makes intuitive sense because λ is the expected number of events, and the sample mean is our best estimate.\n\n\n\nStep 5: Estimate λ Numerically Using Optimization\n\nopt_result = minimize(lambda l: poisson_neg_log_likelihood(l, Y), x0=[1.0], bounds=[(0.001, None)])\nlambda_mle_numerical = opt_result.x[0]\nlambda_mle_numerical\n\n3.684666485763343\n\n\nWe also compute the MLE numerically by minimizing the negative log-likelihood using scipy.optimize.minimize. This confirms that the MLE from optimization closely matches the sample mean, providing validation for both methods.\n\n\n\nConclusion\nThis exercise demonstrates how to estimate a Poisson model using maximum likelihood, both analytically and numerically. The results confirm that the mean of the observed data is the MLE for λ in a Poisson distribution, consistent with theoretical expectations. This provides a strong foundation for moving toward more complex models, such as Poisson regression with covariates.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#exploratory-data-analysis-eda",
    "href": "blog/project2/hw2_questions.html#exploratory-data-analysis-eda",
    "title": "Poisson Regression Examples",
    "section": "Exploratory Data Analysis (EDA)",
    "text": "Exploratory Data Analysis (EDA)\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Load and filter dataset\ndf = pd.read_csv(\"airbnb.csv\")\ncols = [\n    \"room_type\", \"bathrooms\", \"bedrooms\", \"price\", \"number_of_reviews\",\n    \"review_scores_cleanliness\", \"review_scores_location\", \"review_scores_value\", \"instant_bookable\"\n]\ndf_clean = df[cols].dropna()\ndf_clean[\"instant_bookable\"] = df_clean[\"instant_bookable\"].map({\"f\": 0, \"t\": 1})\n\n\nReview Count Distribution\n\nsns.histplot(df_clean[\"number_of_reviews\"], bins=50)\nplt.title(\"Distribution of Number of Reviews\")\nplt.xlabel(\"Number of Reviews\")\nplt.ylabel(\"Count\")\nplt.xlim(0, 200)\nplt.show()\n\n/Users/jnishyu/anaconda3/lib/python3.10/site-packages/seaborn/_oldcore.py:1119: FutureWarning:\n\nuse_inf_as_na option is deprecated and will be removed in a future version. Convert inf values to NaN before operating instead.\n\n\n\n\n\n\n\n\n\n\nReview counts are highly skewed, with many listings having fewer than 50 reviews and a long right tail — a classic case for using a Poisson model.\n\n\n\nBoxplot: Reviews by Room Type\n\nsns.boxplot(data=df_clean, x=\"room_type\", y=\"number_of_reviews\")\nplt.ylim(0, 100)\nplt.title(\"Number of Reviews by Room Type\")\nplt.show()\n\n\n\n\n\n\n\n\nEntire homes tend to receive the most reviews, suggesting higher demand compared to shared and private rooms.\n\n\n\nCorrelation Heatmap\n\nnumeric_cols = [\"price\", \"bedrooms\", \"bathrooms\", \n                \"review_scores_cleanliness\", \"review_scores_location\", \n                \"review_scores_value\", \"number_of_reviews\"]\nsns.heatmap(df_clean[numeric_cols].corr(), annot=True, cmap=\"coolwarm\")\nplt.title(\"Correlation Between Variables\")\nplt.show()\n\n\n\n\n\n\n\n\nThis helps us check for multicollinearity and informs variable selection in our regression model.\n\n\nSummary Statistics\n\ndf_clean.describe().T\n\n\n\n\n\n\n\n\ncount\nmean\nstd\nmin\n25%\n50%\n75%\nmax\n\n\n\n\nbathrooms\n30160.0\n1.122132\n0.384916\n0.0\n1.0\n1.0\n1.0\n6.0\n\n\nbedrooms\n30160.0\n1.151459\n0.699010\n0.0\n1.0\n1.0\n1.0\n10.0\n\n\nprice\n30160.0\n140.206863\n188.392314\n10.0\n70.0\n103.0\n169.0\n10000.0\n\n\nnumber_of_reviews\n30160.0\n21.170889\n32.007541\n1.0\n3.0\n8.0\n26.0\n421.0\n\n\nreview_scores_cleanliness\n30160.0\n9.201724\n1.114261\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_location\n30160.0\n9.415351\n0.843185\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\nreview_scores_value\n30160.0\n9.333952\n0.900472\n2.0\n9.0\n10.0\n10.0\n10.0\n\n\ninstant_bookable\n30160.0\n0.196187\n0.397118\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n\n\n\n\n\nThis table gives a statistical overview of the cleaned dataset. For example, we can see the median number of reviews, typical price levels, and average review scores. It also helps confirm the need for count modeling, given the range and skew of the number_of_reviews variable."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#model-specification-and-fitting-1",
    "href": "blog/project2/hw2_questions.html#model-specification-and-fitting-1",
    "title": "Poisson Regression Examples",
    "section": "Model Specification and Fitting",
    "text": "Model Specification and Fitting\n\nimport statsmodels.api as sm\nfrom patsy import dmatrix\n\nX = dmatrix(\n    \"1 + price + bedrooms + bathrooms + review_scores_cleanliness + \"\n    \"review_scores_location + review_scores_value + C(room_type) + instant_bookable\",\n    data=df_clean,\n    return_type=\"dataframe\"\n)\nY = df_clean[\"number_of_reviews\"]\n\npoisson_model = sm.GLM(Y, X, family=sm.families.Poisson())\npoisson_results = poisson_model.fit()\n\nThe Poisson model is appropriate for count data, using a log link to model the expected number of reviews as an exponential function of listing characteristics."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#poisson-regression-results-formatted-1",
    "href": "blog/project2/hw2_questions.html#poisson-regression-results-formatted-1",
    "title": "Poisson Regression Examples",
    "section": "Poisson Regression Results (Formatted)",
    "text": "Poisson Regression Results (Formatted)\n\nsummary_table = poisson_results.summary2().tables[1].reset_index()\nsummary_table.rename(columns={\n    \"index\": \"Variable\",\n    \"Coef.\": \"Coefficient\",\n    \"Std.Err.\": \"Std. Error\",\n    \"z\": \"z-value\",\n    \"P&gt;|z|\": \"p-value\",\n    \"[0.025\": \"95% CI Lower\",\n    \"0.975]\": \"95% CI Upper\"\n}, inplace=True)\n\nformatted_summary = summary_table[[\n    \"Variable\", \"Coefficient\", \"Std. Error\", \"z-value\", \"p-value\", \"95% CI Lower\", \"95% CI Upper\"\n]].round(4)\n\nformatted_summary\n\n\n\n\n\n\n\n\nVariable\nCoefficient\nStd. Error\nz-value\np-value\n95% CI Lower\n95% CI Upper\n\n\n\n\n0\nIntercept\n3.5725\n0.0160\n223.2145\n0.0000\n3.5411\n3.6039\n\n\n1\nC(room_type)[T.Private room]\n-0.0145\n0.0027\n-5.3104\n0.0000\n-0.0199\n-0.0092\n\n\n2\nC(room_type)[T.Shared room]\n-0.2519\n0.0086\n-29.2286\n0.0000\n-0.2688\n-0.2350\n\n\n3\nprice\n-0.0000\n0.0000\n-1.7288\n0.0838\n-0.0000\n0.0000\n\n\n4\nbedrooms\n0.0749\n0.0020\n37.6977\n0.0000\n0.0710\n0.0788\n\n\n5\nbathrooms\n-0.1240\n0.0037\n-33.0908\n0.0000\n-0.1313\n-0.1167\n\n\n6\nreview_scores_cleanliness\n0.1132\n0.0015\n75.8205\n0.0000\n0.1103\n0.1161\n\n\n7\nreview_scores_location\n-0.0768\n0.0016\n-47.7956\n0.0000\n-0.0799\n-0.0736\n\n\n8\nreview_scores_value\n-0.0915\n0.0018\n-50.9020\n0.0000\n-0.0951\n-0.0880\n\n\n9\ninstant_bookable\n0.3344\n0.0029\n115.7477\n0.0000\n0.3287\n0.3401"
  },
  {
    "objectID": "blog/project2/hw2_questions.html#interpretation-2",
    "href": "blog/project2/hw2_questions.html#interpretation-2",
    "title": "Poisson Regression Examples",
    "section": "Interpretation",
    "text": "Interpretation\nThe Poisson regression results provide insights into what drives bookings (proxied by reviews):\n\nShared rooms receive significantly fewer reviews than entire homes (exp(-0.25) ≈ 0.78).\nInstant bookable listings receive ~40% more reviews (exp(0.33) ≈ 1.39).\nCleanliness score has a strong positive effect on reviews.\nPrice is not statistically significant at the 5% level.\nBedrooms positively affect review count, while bathrooms show a small negative effect.\nLocation/value scores surprisingly show negative associations, possibly due to multicollinearity or reverse causality."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#conclusion-2",
    "href": "blog/project2/hw2_questions.html#conclusion-2",
    "title": "Poisson Regression Examples",
    "section": "Conclusion",
    "text": "Conclusion\nThis analysis fulfills the goal of modeling what factors drive review counts on Airbnb. EDA helped guide variable selection, and Poisson regression offered interpretable, statistically significant insights. The model shows that room type, convenience (instant booking), and cleanliness are key predictors of customer engagement."
  },
  {
    "objectID": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model",
    "href": "blog/project2/hw2_questions.html#estimation-of-simple-poisson-model",
    "title": "Poisson Regression Examples",
    "section": "Estimation of Simple Poisson Model",
    "text": "Estimation of Simple Poisson Model\nTo model the number of patents awarded per firm, we assume the data follow a Poisson distribution with parameter λ. This is appropriate for count data observed over a fixed time period. In this section, we define the Poisson log-likelihood, visualize it, and estimate the maximum likelihood value of λ both analytically and numerically.\n\n\nStep 1: Load and Inspect the Data\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom scipy.special import gammaln\nfrom scipy.optimize import minimize\n\n# Load patent data\ndf = pd.read_csv(\"blueprinty.csv\")\nY = df[\"patents\"].values\n\nWe load the blueprinty.csv dataset and extract the number of patents for each firm, stored in the variable Y.\n\n\n\nStep 2: Define the Log-Likelihood Function\n\n# Negative log-likelihood function (for minimization)\ndef poisson_neg_log_likelihood(lmbda, Y):\n    return -np.sum(Y * np.log(lmbda) - lmbda - gammaln(Y + 1))\n\nThis is the log-likelihood of the Poisson model (negated for use with optimization). We use gammaln(Y + 1) for numerical stability in place of log(Y!).\n\n\n\nStep 3: Visualize the Log-Likelihood Curve\n\nlambdas = np.linspace(0.1, 10, 200)\nlog_liks = [-poisson_neg_log_likelihood(lmbda, Y) for lmbda in lambdas]\n\nplt.figure(figsize=(8, 5))\nplt.plot(lambdas, log_liks, label=\"Log-Likelihood\")\nplt.axvline(np.mean(Y), color='red', linestyle='--', label=\"Sample Mean (Ȳ)\")\nplt.title(\"Log-Likelihood Curve for Poisson Model\")\nplt.xlabel(\"Lambda (λ)\")\nplt.ylabel(\"Log-Likelihood\")\nplt.legend()\nplt.grid(True)\nplt.show()\n\n\n\n\n\n\n\n\nThis curve shows how the log-likelihood varies with different values of λ. The red line indicates the sample mean, which aligns closely with the peak of the curve.\n\n\n\nStep 4: Analytical MLE for Poisson\n\nlambda_mle_analytical = np.mean(Y)\nlambda_mle_analytical\n\n3.6846666666666668\n\n\n\nThis result comes from solving the first derivative of the log-likelihood with respect to λ, setting it to zero:\n\\[ \\frac{d}{d\\lambda} \\log L(\\lambda) = \\sum_i \\left( \\frac{Y_i}{\\lambda} - 1 \\right) = 0 \\] Solving gives: \\[ \\hat{\\lambda}_{MLE} = \\bar{Y} \\]\n\nThis derivation confirms that the maximum likelihood estimator for λ is simply the mean of the observed data.\n\n\n\nStep 5: Estimate λ Numerically\n\n# Find lambda using numerical optimization\nopt_result = minimize(lambda l: poisson_neg_log_likelihood(l, Y), x0=[1.0], bounds=[(0.001, None)])\nlambda_mle_numerical = opt_result.x[0]\nlambda_mle_numerical\n\n3.684666485763343\n\n\nUsing numerical optimization, we minimize the negative log-likelihood to obtain λ̂. The result closely matches the sample mean, confirming our analytical solution.\n\n\n\nConclusion\nThis process demonstrates how to derive and estimate the Poisson MLE both mathematically and computationally. We verified that: - The log-likelihood peaks at the sample mean - The analytical and numerical MLEs are identical This lays a strong foundation for moving into more complex models like Poisson regression.\n\n\nEstimation of Poisson Regression Model\nNext, we extend our simple Poisson model to a Poisson Regression Model such that \\(Y_i = \\text{Poisson}(\\lambda_i)\\) where \\(\\lambda_i = \\exp(X_i'\\beta)\\). The interpretation is that the success rate of patent awards is not constant across all firms (\\(\\lambda\\)) but rather is a function of firm characteristics \\(X_i\\). Specifically, we will use the covariates age, age squared, region, and whether the firm is a customer of Blueprinty.\ntodo: Update your likelihood or log-likelihood function with an additional argument to take in a covariate matrix X. Also change the parameter of the model from lambda to the beta vector. In this model, lambda must be a positive number, so we choose the inverse link function g_inv() to be exp() so that \\(\\lambda_i = e^{X_i'\\beta}\\). For example:\npoisson_regression_likelihood &lt;- function(beta, Y, X){\n   ...\n}\ntodo: Use your function along with R’s optim() or Python’s sp.optimize() to find the MLE vector and the Hessian of the Poisson model with covariates. Specifically, the first column of X should be all 1’s to enable a constant term in the model, and the subsequent columns should be age, age squared, binary variables for all but one of the regions, and the binary customer variable. Use the Hessian to find standard errors of the beta parameter estimates and present a table of coefficients and standard errors.\ntodo: Check your results using R’s glm() function or Python sm.GLM() function.\ntodo: Interpret the results.\ntodo: What do you conclude about the effect of Blueprinty’s software on patent success? Because the beta coefficients are not directly interpretable, it may help to create two fake datasets: X_0 and X_1 where X_0 is the X data but with iscustomer=0 for every observation and X_1 is the X data but with iscustomer=1 for every observation. Then, use X_0 and your fitted model to get the vector of predicted number of patents (y_pred_0) for every firm in the dataset, and use X_1 to get Y_pred_1 for every firm. Then subtract y_pred_1 minus y_pred_0 and take the average of that vector of differences."
  }
]